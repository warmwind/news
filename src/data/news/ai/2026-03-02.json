[
  {
    "title": "HumanMCP: A Human-Like Query Dataset for Evaluating MCP Tool Retrieval Performance",
    "slug": "humanmcp-human-like-query-dataset-mcp-tool-retrieval",
    "url": "https://arxiv.org/abs/2602.23367",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "This paper introduces the first large-scale dataset of human-like user queries for Model Context Protocol (MCP) servers, covering 2,800 tools across 308 servers with diverse user personas. The dataset addresses a critical gap in evaluating tool-use systems by providing realistic, varied queries that better represent real-world interaction patterns beyond existing benchmarks.",
    "content": "arXiv:2602.23367v1 Announce Type: new \nAbstract: Model Context Protocol (MCP) servers contain a collection of thousands of open-source standardized tools, linking LLMs to external systems; however, existing datasets and benchmarks lack realistic, human-like user queries, remaining a critical gap in evaluating the tool usage and ecosystems of MCP servers. Existing datasets often do contain tool descriptions but fail to represent how different users portray their requests, leading to poor generalization and inflated reliability of certain benchmarks. This paper introduces the first large-scale MCP dataset featuring diverse, high-quality diverse user queries generated specifically to match 2800 tools across 308 MCP servers, developing on the MCP Zero dataset. Each tool is paired with multiple unique user personas that we have generated, to capture varying levels of user intent ranging from precise task requests, and ambiguous, exploratory commands, reflecting the complexity of real-world interaction patterns.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "HumanMCP：用于评估MCP工具检索性能的类人查询数据集",
        "summary": "本文介绍了第一个大规模类人用户查询数据集，适用于模型上下文协议（MCP）服务器，涵盖308个服务器中的2,800个工具和多样化的用户角色。该数据集通过提供真实的、多样化的查询，更好地代表真实世界的交互模式，超越现有基准，填补了评估工具使用系统的关键空白。"
      },
      "fr": {
        "title": "HumanMCP : Un ensemble de données de requêtes similaires à l'homme pour évaluer les performances de récupération d'outils MCP",
        "summary": "Cet article introduit le premier ensemble de données à grande échelle de requêtes utilisateur similaires à l'homme pour les serveurs du protocole de contexte de modèle (MCP), couvrant 2 800 outils dans 308 serveurs avec des personas d'utilisateurs diversifiés. L'ensemble de données comble un écart critique dans l'évaluation des systèmes d'utilisation d'outils en fournissant des requêtes réalistes et variées qui représentent mieux les modèles d'interaction du monde réel au-delà des benchmarks existants."
      },
      "de": {
        "title": "HumanMCP: Ein Human-ähnlicher Query-Datensatz zur Bewertung der MCP-Toolabruf-Leistung",
        "summary": "Dieses Papier stellt den ersten großmaßstäblichen Datensatz menschenähnlicher Benutzerabfragen für Model Context Protocol (MCP)-Server vor und deckt 2.800 Tools auf 308 Servern mit vielfältigen Benutzerpersonas ab. Der Datensatz schließt eine kritische Lücke bei der Bewertung von Tool-Nutzungssystemen, indem er realistische, vielfältige Abfragen bereitstellt, die reale Interaktionsmuster besser darstellen als bestehende Benchmarks."
      },
      "es": {
        "title": "HumanMCP: Un conjunto de datos de consultas similares a humanos para evaluar el desempeño de la recuperación de herramientas MCP",
        "summary": "Este artículo presenta el primer conjunto de datos a gran escala de consultas de usuario similares a humanos para servidores del Protocolo de Contexto de Modelo (MCP), cubriendo 2.800 herramientas en 308 servidores con diversos perfiles de usuario. El conjunto de datos aborda una brecha crítica en la evaluación de sistemas de uso de herramientas al proporcionar consultas realistas y variadas que representan mejor los patrones de interacción del mundo real más allá de los puntos de referencia existentes."
      }
    }
  },
  {
    "title": "An Agentic LLM Framework for Adverse Media Screening in AML Compliance",
    "slug": "agentic-llm-framework-adverse-media-screening-aml-compliance",
    "url": "https://arxiv.org/abs/2602.23373",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "This work presents an LLM-based agentic system using retrieval-augmented generation to automate adverse media screening for financial compliance. By reducing reliance on keyword searches and manual review, the system demonstrates improved accuracy in distinguishing high-risk from low-risk individuals in AML/KYC processes.",
    "content": "arXiv:2602.23373v1 Announce Type: new \nAbstract: Adverse media screening is a critical component of anti-money laundering (AML) and know-your-customer (KYC) compliance processes in financial institutions. Traditional approaches rely on keyword-based searches that generate high false-positive rates or require extensive manual review. We present an agentic system that leverages Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to automate adverse media screening. Our system implements a multi-step approach where an LLM agent searches the web, retrieves and processes relevant documents, and computes an Adverse Media Index (AMI) score for each subject. We evaluate our approach using multiple LLM backends on a dataset comprising Politically Exposed Persons (PEPs), persons from regulatory watchlists, and sanctioned persons from OpenSanctions and clean names from academic sources, demonstrating the system's ability to distinguish between high-risk and low-risk individuals.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "用于AML合规的不良媒体筛查的代理LLM框架",
        "summary": "这项工作提出了一个基于LLM的代理系统，使用检索增强生成来自动化不良媒体筛查以实现财务合规。通过减少对关键词搜索和人工审查的依赖，该系统在AML/KYC流程中区分高风险和低风险个人的准确性得到了改善。"
      },
      "fr": {
        "title": "Un cadre LLM agentique pour la détection des médias adverses dans la conformité AML",
        "summary": "Ce travail présente un système agentique basé sur LLM utilisant la génération augmentée par récupération pour automatiser la détection des médias adverses pour la conformité financière. En réduisant la dépendance aux recherches par mots-clés et à l'examen manuel, le système démontre une précision améliorée dans la distinction entre les individus à haut risque et à faible risque dans les processus AML/KYC."
      },
      "de": {
        "title": "Ein agentisches LLM-Framework für das Screening von negativen Medien bei der AML-Compliance",
        "summary": "Diese Arbeit präsentiert ein agentisches LLM-System, das generative Retrievals zur Automatisierung des Screenings von negativen Medien für finanzielle Compliance nutzt. Durch die Verringerung der Abhängigkeit von Schlüsselwortsuchen und manueller Überprüfung zeigt das System verbesserte Genauigkeit bei der Unterscheidung zwischen Personen mit hohem und niedrigem Risiko in AML/KYC-Prozessen."
      },
      "es": {
        "title": "Un marco LLM agéntico para la detección de medios adversos en la conformidad AML",
        "summary": "Este trabajo presenta un sistema agéntico basado en LLM utilizando generación aumentada por recuperación para automatizar la detección de medios adversos para el cumplimiento financiero. Al reducir la dependencia de búsquedas por palabras clave y revisión manual, el sistema demuestra una precisión mejorada en la distinción entre individuos de alto riesgo y bajo riesgo en los procesos AML/KYC."
      }
    }
  },
  {
    "title": "Causal Identification from Counterfactual Data: Completeness and Bounding Results",
    "slug": "causal-identification-counterfactual-data-completeness-bounds",
    "url": "https://arxiv.org/abs/2602.23541",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "This paper develops the CTFIDU+ algorithm for identifying counterfactual quantities from experimentally realizable distributions and establishes theoretical limits on exact causal inference. The work provides both completeness guarantees and novel analytical bounds for non-identifiable counterfactuals in non-parametric settings.",
    "content": "arXiv:2602.23541v1 Announce Type: new \nAbstract: Previous work establishing completeness results for $\\textit{counterfactual identification}$ has been circumscribed to the setting where the input data belongs to observational or interventional distributions (Layers 1 and 2 of Pearl's Causal Hierarchy), since it was generally presumed impossible to obtain data from counterfactual distributions, which belong to Layer 3. However, recent work (Raghavan & Bareinboim, 2025) has formally characterized a family of counterfactual distributions which can be directly estimated via experimental methods - a notion they call $\\textit{counterfactual realizabilty}$. This leaves open the question of what $\\textit{additional}$ counterfactual quantities now become identifiable, given this new access to (some) Layer 3 data. To answer this question, we develop the CTFIDU+ algorithm for identifying counterfactual queries from an arbitrary set of Layer 3 distributions, and prove that it is complete for this task. Building on this, we establish the theoretical limit of which counterfactuals can be identified from physically realizable distributions, thus implying the $\\textit{fundamental limit to exact causal inference in the non-parametric setting}$. Finally, given the impossibility of identifying certain critical types of counterfactuals, we derive novel analytic bounds for such quantities using realizable counterfactual data, and corroborate using simulations that counterfactual data helps tighten the bounds for non-identifiable quantities in practice.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "反事实数据中的因果识别：完全性和界限结果",
        "summary": "本文开发了CTFIDU+算法，用于从实验可实现的分布中识别反事实量，并建立了精确因果推理的理论限制。该工作在非参数设置中为不可识别的反事实提供了完全性保证和新颖的分析界限。"
      },
      "fr": {
        "title": "Identification causale à partir de données contrefactuelles : résultats de complétude et de limitation",
        "summary": "Cet article développe l'algorithme CTFIDU+ pour identifier les quantités contrefactuelles à partir de distributions réalisables expérimentalement et établit des limites théoriques sur l'inférence causale exacte. Le travail fournit à la fois des garanties de complétude et des limites analytiques nouvelles pour les contrefactuels non identifiables dans des paramètres non paramétriques."
      },
      "de": {
        "title": "Kausale Identifikation aus kontrafaktischen Daten: Vollständigkeit und Begrenzungsergebnisse",
        "summary": "Dieses Papier entwickelt den CTFIDU+-Algorithmus zur Identifikation kontrafaktischer Größen aus experimentell realisierbaren Verteilungen und legt theoretische Grenzen für exakte kausale Inferenz fest. Die Arbeit bietet sowohl Vollständigkeitsgarantien als auch neuartige analytische Grenzen für nicht identifizierbare Kontrafaktika in nicht-parametrischen Einstellungen."
      },
      "es": {
        "title": "Identificación causal a partir de datos contrafácticos: resultados de completitud y limitaciones",
        "summary": "Este artículo desarrolla el algoritmo CTFIDU+ para identificar cantidades contrafácticas a partir de distribuciones realizables experimentalmente y establece límites teóricos en la inferencia causal exacta. El trabajo proporciona tanto garantías de completitud como límites analíticos novedosos para contrafácticos no identificables en configuraciones no paramétricas."
      }
    }
  },
  {
    "title": "Planning under Distribution Shifts with Causal POMDPs",
    "slug": "planning-distribution-shifts-causal-pomdps",
    "url": "https://arxiv.org/abs/2602.23545",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "The paper proposes a causal POMDP framework enabling planning under partial observability when environment distributions shift. By maintaining belief over both latent state and domain changes while preserving computational tractability, the approach addresses real-world challenges where learned models become invalid due to environmental changes.",
    "content": "arXiv:2602.23545v1 Announce Type: new \nAbstract: In the real world, planning is often challenged by distribution shifts. As such, a model of the environment obtained under one set of conditions may no longer remain valid as the distribution of states or the environment dynamics change, which in turn causes previously learned strategies to fail. In this work, we propose a theoretical framework for planning under partial observability using Partially Observable Markov Decision Processes (POMDPs) formulated using causal knowledge. By representing shifts in the environment as interventions on this causal POMDP, the framework enables evaluating plans under hypothesized changes and actively identifying which components of the environment have been altered. We show how to maintain and update a belief over both the latent state and the underlying domain, and we prove that the value function remains piecewise linear and convex (PWLC) in this augmented belief space. Preservation of PWLC under distribution shifts has the advantage of maintaining the tractability of planning via $\\alpha$-vector-based POMDP methods.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "因果POMDP框架下的分布偏移规划",
        "summary": "该论文提出了一个因果POMDP框架，在环境分布发生偏移时实现部分可观测性下的规划。通过维护对潜在状态和域变化的信念，同时保持计算可行性，该方法解决了真实世界中由于环境变化导致学习模型失效的挑战。"
      },
      "fr": {
        "title": "Planification sous des changements de distribution avec les POMDPs causaux",
        "summary": "L'article propose un cadre causal POMDP permettant la planification sous observabilité partielle lorsque les distributions d'environnement changent. En maintenant une croyance sur l'état latent et les changements de domaine tout en préservant la tractabilité informatique, l'approche aborde les défis du monde réel où les modèles appris deviennent invalides en raison des changements environnementaux."
      },
      "de": {
        "title": "Planung unter Verteilungsverschiebungen mit kausalen POMDPs",
        "summary": "Das Papier schlägt einen kausalen POMDP-Rahmen vor, der Planung unter partieller Beobachtbarkeit ermöglicht, wenn sich Umgebungsverteilungen verschieben. Durch die Aufrechterhaltung von Überzeugungen über latente Zustände und Domänenänderungen bei Beibehaltung der Rechentraktabilität adressiert der Ansatz reale Herausforderungen, bei denen gelernte Modelle aufgrund von Umweltveränderungen ungültig werden."
      },
      "es": {
        "title": "Planificación bajo cambios de distribución con POMDPs causales",
        "summary": "El artículo propone un marco causal POMDP que permite la planificación bajo observabilidad parcial cuando las distribuciones del entorno cambian. Al mantener creencias sobre el estado latente y los cambios de dominio mientras se preserva la tractabilidad computacional, el enfoque aborda desafíos del mundo real donde los modelos aprendidos se vuelven inválidos debido a cambios ambientales."
      }
    }
  },
  {
    "title": "Construct, Merge, Solve & Adapt with Reinforcement Learning for the min-max Multiple Traveling Salesman Problem",
    "slug": "construct-merge-solve-adapt-reinforcement-learning-mtsp",
    "url": "https://arxiv.org/abs/2602.23579",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "RL-CMSA combines reinforcement learning with exact optimization for the min-max Multiple Traveling Salesman Problem, outperforming genetic algorithms on complex instances. The hybrid approach balances exploration and exploitation through learned pairwise q-values and restricted set-covering optimization.",
    "content": "arXiv:2602.23579v1 Announce Type: new \nAbstract: The Multiple Traveling Salesman Problem (mTSP) extends the Traveling Salesman Problem to m tours that start and end at a common depot and jointly visit all customers exactly once. In the min-max variant, the objective is to minimize the longest tour, reflecting workload balance. We propose a hybrid approach, Construct, Merge, Solve & Adapt with Reinforcement Learning (RL-CMSA), for the symmetric single-depot min-max mTSP. The method iteratively constructs diverse solutions using probabilistic clustering guided by learned pairwise q-values, merges routes into a compact pool, solves a restricted set-covering MILP, and refines solutions via inter-route remove, shift, and swap moves. The q-values are updated by reinforcing city-pair co-occurrences in high-quality solutions, while the pool is adapted through ageing and pruning. This combination of exact optimization and reinforcement-guided construction balances exploration and exploitation. Computational results on random and TSPLIB instances show that RL-CMSA consistently finds (near-)best solutions and outperforms a state-of-the-art hybrid genetic algorithm under comparable time limits, especially as instance size and the number of salesmen increase.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "用强化学习进行最小最大多旅行商问题的构造、合并、求解与自适应",
        "summary": "RL-CMSA将强化学习与精确优化相结合来解决最小最大多旅行商问题，在复杂实例上的表现优于遗传算法。该混合方法通过学习的成对Q值和受限的集合覆盖优化来平衡探索和利用。"
      },
      "fr": {
        "title": "Construction, fusion, résolution et adaptation avec apprentissage par renforcement pour le problème de voyageur de commerce multiple min-max",
        "summary": "RL-CMSA combine l'apprentissage par renforcement avec l'optimisation exacte pour le problème de voyageur de commerce multiple min-max, surpassant les algorithmes génétiques sur les instances complexes. L'approche hybride équilibre l'exploration et l'exploitation grâce aux valeurs q par paire apprises et à l'optimisation restreinte du set-covering."
      },
      "de": {
        "title": "Konstruieren, Zusammenführen, Lösen und Anpassen mit verstärktem Lernen für das min-max Multiple Traveling Salesman Problem",
        "summary": "RL-CMSA kombiniert verstärktes Lernen mit exakter Optimierung für das min-max Multiple Traveling Salesman Problem und übertrifft genetische Algorithmen bei komplexen Instanzen. Der hybride Ansatz balanciert Erkundung und Ausnutzung durch gelernte paarweise Q-Werte und eingeschränkte Set-Cover-Optimierung."
      },
      "es": {
        "title": "Construcción, fusión, resolución y adaptación con aprendizaje por refuerzo para el problema min-max de múltiples viajeros de comercio",
        "summary": "RL-CMSA combina el aprendizaje por refuerzo con la optimización exacta para el problema min-max de múltiples viajeros de comercio, superando a los algoritmos genéticos en instancias complejas. El enfoque híbrido equilibra la exploración y explotación a través de valores q por pares aprendidos y optimización de cobertura de conjuntos restringida."
      }
    }
  },
  {
    "title": "SleepLM: Natural-Language Intelligence for Human Sleep",
    "slug": "sleeplm-natural-language-intelligence-human-sleep",
    "url": "https://arxiv.org/abs/2602.23605",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "SleepLM is a foundation model family enabling natural language understanding of human sleep through multimodal alignment on a dataset of 100,000+ hours from 10,000+ individuals. The model supports zero-shot sleep event localization and insight generation, moving beyond traditional closed-label sleep classification approaches.",
    "content": "arXiv:2602.23605v1 Announce Type: new \nAbstract: We present SleepLM, a family of sleep-language foundation models that enable human sleep alignment, interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to describe, query, or generalize to novel sleep phenomena. SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the curation of the first large-scale sleep-text dataset, comprising over 100K hours of data from more than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines contrastive alignment, caption generation, and signal reconstruction to better capture physiological fidelity and cross-modal interactions. Extensive experiments on real-world sleep understanding tasks verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including language-guided event localization, targeted insight generation, and zero-shot generalization to unseen tasks. All code and data will be open-sourced.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "SleepLM: 人类睡眠的自然语言智能",
        "summary": "SleepLM是一个基础模型系列，通过在包含来自10,000多个个体的100,000多小时数据的多模态对齐数据集上进行训练，实现对人类睡眠的自然语言理解。该模型支持零样本睡眠事件定位和洞察生成，超越了传统的闭标签睡眠分类方法。"
      },
      "fr": {
        "title": "SleepLM: Intelligence en langage naturel pour le sommeil humain",
        "summary": "SleepLM est une famille de modèles fondamentaux permettant la compréhension du langage naturel du sommeil humain grâce à l'alignement multimodal sur un ensemble de données de 100 000+ heures de 10 000+ individus. Le modèle prend en charge la localisation d'événements de sommeil sans exemple et la génération d'insights, allant au-delà des approches traditionnelles de classification du sommeil avec étiquettes fermées."
      },
      "de": {
        "title": "SleepLM: Natürlichsprachige Intelligenz für menschlichen Schlaf",
        "summary": "SleepLM ist eine Grundmodell-Familie, die natürliches Sprachverständnis des menschlichen Schlafs durch multimodale Ausrichtung auf einem Datensatz von über 100.000 Stunden von über 10.000 Personen ermöglicht. Das Modell unterstützt schlaffreie Schlaf-Event-Lokalisierung und Insight-Generierung und geht über traditionelle geschlossene Schlafklassifizierungsansätze hinaus."
      },
      "es": {
        "title": "SleepLM: Inteligencia en lenguaje natural para el sueño humano",
        "summary": "SleepLM es una familia de modelos fundamentales que permite la comprensión del lenguaje natural del sueño humano a través de la alineación multimodal en un conjunto de datos de más de 100.000 horas de más de 10.000 individuos. El modelo admite localización de eventos de sueño sin ejemplos y generación de perspectivas, yendo más allá de los enfoques tradicionales de clasificación de sueño con etiquetas cerradas."
      }
    }
  },
  {
    "title": "MMKG-RDS: Reasoning Data Synthesis via Deep Mining of Multimodal Knowledge Graphs",
    "slug": "mmkg-rds-reasoning-data-synthesis-multimodal-knowledge-graphs",
    "url": "https://arxiv.org/abs/2602.23632",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "MMKG-RDS presents a framework for synthesizing high-quality training data through multimodal knowledge graphs with fine-grained extraction and multidimensional quality scoring. Fine-tuning models on synthesized data improves reasoning accuracy by 9.2%, demonstrating effectiveness across multiple domains.",
    "content": "arXiv:2602.23632v1 Announce Type: new \nAbstract: Synthesizing high-quality training data is crucial for enhancing domain models' reasoning abilities. Existing methods face limitations in long-tail knowledge coverage, effectiveness verification, and interpretability. Knowledge-graph-based approaches still fall short in functionality, granularity, customizability, and evaluation. To address these issues, we propose MMKG-RDS, a flexible framework for reasoning data synthesis that leverages multimodal knowledge graphs. It supports fine-grained knowledge extraction, customizable path sampling, and multidimensional data quality scoring. We validate MMKG-RDS with the MMKG-RDS-Bench dataset, covering five domains, 17 task types, and 14,950 samples. Experimental results show fine-tuning Qwen3 models (0.6B/8B/32B) on a small number of synthesized samples improves reasoning accuracy by 9.2%. The framework also generates distinct data, challenging existing models on tasks involving tables and formulas, useful for complex benchmark construction. The dataset and code are available at https://github.com/360AILAB-NLP/MMKG-RDS",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "MMKG-RDS：通过多模态知识图谱深度挖掘进行推理数据合成",
        "summary": "MMKG-RDS提出了一个框架，通过具有细粒度提取和多维质量评分的多模态知识图谱来合成高质量的训练数据。在合成数据上微调模型可将推理准确率提高9.2%，展示了其在多个领域的有效性。"
      },
      "fr": {
        "title": "MMKG-RDS : Synthèse de données de raisonnement par exploration approfondie des graphes de connaissances multimodaux",
        "summary": "MMKG-RDS présente un cadre pour synthétiser des données d'entraînement de haute qualité via des graphes de connaissances multimodaux avec extraction fine et notation de qualité multidimensionnelle. L'ajustement fin des modèles sur les données synthétisées améliore la précision du raisonnement de 9,2%, démontrant l'efficacité dans plusieurs domaines."
      },
      "de": {
        "title": "MMKG-RDS: Synthesisierung von Reasoning-Daten durch tiefe Analyse von Multimodalen Wissensgraphen",
        "summary": "MMKG-RDS präsentiert einen Rahmen zur Synthesisierung von hochwertigen Trainingsdaten durch Multimodale Wissensgraphen mit feingranularer Extraktion und mehrdimensionaler Qualitätsbewertung. Das Feintuning von Modellen auf synthetisierten Daten verbessert die Reasoning-Genauigkeit um 9,2% und demonstriert Effektivität über mehrere Domänen."
      },
      "es": {
        "title": "MMKG-RDS: Síntesis de Datos de Razonamiento mediante Minería Profunda de Gráficos de Conocimiento Multimodales",
        "summary": "MMKG-RDS presenta un marco para sintetizar datos de entrenamiento de alta calidad mediante gráficos de conocimiento multimodales con extracción de grano fino y puntuación de calidad multidimensional. El ajuste fino de modelos en datos sintetizados mejora la precisión del razonamiento en un 9,2%, demostrando efectividad en múltiples dominios."
      }
    }
  },
  {
    "title": "AI Must Embrace Specialization via Superhuman Adaptable Intelligence",
    "slug": "ai-specialization-superhuman-adaptable-intelligence",
    "url": "https://arxiv.org/abs/2602.23643",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "This paper critiques AGI as a flawed concept and proposes Superhuman Adaptable Intelligence (SAI) as a more practical framework emphasizing specialization and superhuman performance. The shift from generality to specialization provides clearer guidance for AI development and more realistic expectations.",
    "content": "arXiv:2602.23643v1 Announce Type: new \nAbstract: Everyone from AI executives and researchers to doomsayers, politicians, and activists is talking about Artificial General Intelligence (AGI). Yet, they often don't seem to agree on its exact definition. One common definition of AGI is an AI that can do everything a human can do, but are humans truly general? In this paper, we address what's wrong with our conception of AGI, and why, even in its most coherent formulation, it is a flawed concept to describe the future of AI. We explore whether the most widely accepted definitions are plausible, useful, and truly general. We argue that AI must embrace specialization, rather than strive for generality, and in its specialization strive for superhuman performance, and introduce Superhuman Adaptable Intelligence (SAI). SAI is defined as intelligence that can learn to exceed humans at anything important that we can do, and that can fill in the skill gaps where humans are incapable. We then lay out how SAI can help hone a discussion around AI that was blurred by an overloaded definition of AGI, and extrapolate the implications of using it as a guide for the future.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "AI必须通过超人类适应性智能拥抱专业化",
        "summary": "本文批评AGI是一个有缺陷的概念，提议超人类适应性智能(SAI)作为更实用的框架，强调专业化和超人类性能。从通用性到专业化的转变为AI发展提供了更清晰的指导和更现实的期望。"
      },
      "fr": {
        "title": "L'IA doit adopter la spécialisation via l'Intelligence Adaptable Surhumaine",
        "summary": "Cet article critique l'AGI comme un concept défectueux et propose l'Intelligence Adaptable Surhumaine (IAS) comme un cadre plus pratique mettant l'accent sur la spécialisation et les performances surhumaines. Le passage de la généralité à la spécialisation fournit une orientation plus claire pour le développement de l'IA et des attentes plus réalistes."
      },
      "de": {
        "title": "KI muss Spezialisierung durch Superhuman Adaptive Intelligence annehmen",
        "summary": "Dieses Papier kritisiert AGI als fehlerhaftes Konzept und schlägt Superhuman Adaptive Intelligence (SAI) als praktischeres Rahmenwerk vor, das Spezialisierung und übermenschliche Leistung betont. Der Wandel von Allgemeinheit zu Spezialisierung bietet klarere Anleitung für KI-Entwicklung und realistischere Erwartungen."
      },
      "es": {
        "title": "La IA debe abrazar la especialización a través de la Inteligencia Adaptable Sobrehumana",
        "summary": "Este artículo critica la AGI como un concepto defectuoso y propone la Inteligencia Adaptable Sobrehumana (SAI) como un marco más práctico que enfatiza la especialización y el desempeño sobrehumano. El cambio de la generalidad a la especialización proporciona orientación más clara para el desarrollo de la IA y expectativas más realistas."
      }
    }
  },
  {
    "title": "PseudoAct: Leveraging Pseudocode Synthesis for Flexible Planning and Action Control in Large Language Model Agents",
    "slug": "pseudoact-pseudocode-synthesis-llm-agent-planning-control",
    "url": "https://arxiv.org/abs/2602.23668",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "PseudoAct enables LLM agents to synthesize explicit pseudocode plans that structure tasks with clear control flows before execution. This approach significantly reduces redundant actions and token consumption while achieving state-of-the-art results on complex multi-step reasoning benchmarks.",
    "content": "arXiv:2602.23668v1 Announce Type: new \nAbstract: Large language model (LLM) agents typically rely on reactive decision-making paradigms such as ReAct, selecting actions conditioned on growing execution histories. While effective for short tasks, these approaches often lead to redundant tool usage, unstable reasoning, and high token consumption in complex long-horizon tasks involving branching, iteration, or multi-tool coordination. To address these limitations, this paper introduces PseudoAct, a novel framework for flexible planning and action control in LLM agents through pseudocode synthesis. Leveraging the ability of LLMs to express task-solving strategies as code, PseudoAct synthesizes a structured pseudocode plan that decomposes a task into subtasks and explicitly encodes control flow, including sequencing, conditionals, loops, parallel composition, and combinations of these logic primitives. Actions are then executed by following this global plan, making the decision logic explicit and temporally coherent. This design reduces redundant actions, prevents infinite loops, and avoids uninformative alternative exploration, enabling consistent and efficient long-horizon decision-making. Experiments on benchmark datasets show that our method significantly outperforms existing reactive agent approaches, achieving a 20.93% absolute gain in success rate on FEVER and setting a new state-of-the-art on HotpotQA.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "PseudoAct：利用伪代码合成实现大语言模型代理的灵活规划和动作控制",
        "summary": "PseudoAct使LLM代理能够合成明确的伪代码计划，在执行前使用清晰的控制流结构化任务。这种方法显著减少了冗余动作和令牌消耗，同时在复杂多步骤推理基准上达到了最先进的结果。"
      },
      "fr": {
        "title": "PseudoAct : Exploitation de la synthèse de pseudocode pour une planification flexible et un contrôle d'action dans les agents de modèles de langage volumineux",
        "summary": "PseudoAct permet aux agents LLM de synthétiser des plans pseudocode explicites qui structurent les tâches avec des flux de contrôle clairs avant l'exécution. Cette approche réduit considérablement les actions redondantes et la consommation de jetons tout en obtenant des résultats de pointe sur les benchmarks complexes de raisonnement multi-étapes."
      },
      "de": {
        "title": "PseudoAct: Nutzung von Pseudocode-Synthese für flexible Planung und Aktionssteuerung in Large Language Model Agenten",
        "summary": "PseudoAct ermöglicht es LLM-Agenten, explizite Pseudocode-Pläne zu synthetisieren, die Aufgaben mit klaren Kontrollflüssen vor der Ausführung strukturieren. Dieser Ansatz reduziert erheblich redundante Aktionen und Token-Verbrauch, während gleichzeitig hochmoderne Ergebnisse bei komplexen mehrstufigen Reasoning-Benchmarks erreicht werden."
      },
      "es": {
        "title": "PseudoAct: Aprovechamiento de la síntesis de pseudocódigo para planificación flexible y control de acciones en agentes de modelos de lenguaje grandes",
        "summary": "PseudoAct permite que los agentes LLM sintetizen planes de pseudocódigo explícitos que estructuran tareas con flujos de control claros antes de la ejecución. Este enfoque reduce significativamente las acciones redundantes y el consumo de tokens mientras logra resultados de vanguardia en benchmarks complejos de razonamiento de múltiples pasos."
      }
    }
  },
  {
    "title": "ODAR: Principled Adaptive Routing for LLM Reasoning via Active Inference",
    "slug": "odar-adaptive-routing-llm-reasoning-active-inference",
    "url": "https://arxiv.org/abs/2602.23681",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "ODAR-Expert optimizes the accuracy-efficiency trade-off in LLM reasoning through adaptive routing between fast and slow paths grounded in active inference. The framework achieves strong benchmark performance while reducing computational costs by 82% through principled free-energy-based decision-making.",
    "content": "arXiv:2602.23681v1 Announce Type: new \nAbstract: The paradigm of large language model (LLM) reasoning is shifting from parameter scaling to test-time compute scaling, yet many existing approaches still rely on uniform brute-force sampling (for example, fixed best-of-N or self-consistency) that is costly, hard to attribute, and can trigger overthinking with diminishing returns. We propose ODAR-Expert, an adaptive routing framework that optimizes the accuracy-efficiency trade-off via principled resource allocation. ODAR uses a difficulty estimator grounded in amortized active inference to dynamically route queries between a heuristic Fast Agent and a deliberative Slow Agent. We further introduce a free-energy-principled, risk-sensitive fusion mechanism that selects answers by minimizing a variational free energy objective, balancing log-likelihood with epistemic uncertainty (varentropy) as a principled alternative to ad hoc voting over heterogeneous candidates. Extensive evaluation across 23 benchmarks shows strong and consistent gains, including 98.2% accuracy on MATH and 54.8% on Humanity's Last Exam (HLE), while improving the compute-accuracy frontier under compute-matched settings. We also validate reproducibility on a fully open-source stack (Llama 4 + DeepSeek), where ODAR surpasses homogeneous sampling strategies while reducing computational costs by 82%. Overall, our results suggest that thinking-optimal scaling requires adaptive resource allocation with free-energy-based decision-making rather than simply increasing test-time compute.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "ODAR：通过主动推理为LLM推理的有原则自适应路由",
        "summary": "ODAR-Expert通过以主动推理为基础的快速和慢速路径之间的自适应路由来优化LLM推理中的准确性-效率权衡。该框架通过有原则的自由能决策，实现了强大的基准性能，同时将计算成本降低了82%。"
      },
      "fr": {
        "title": "ODAR : Routage Adaptatif Principié pour le Raisonnement des LLM via l'Inférence Active",
        "summary": "ODAR-Expert optimise le compromis précision-efficacité dans le raisonnement des LLM par le routage adaptatif entre des chemins rapides et lents fondés sur l'inférence active. Le framework atteint des performances de référence solides tout en réduisant les coûts informatiques de 82% grâce à une prise de décision basée sur l'énergie libre principée."
      },
      "de": {
        "title": "ODAR: Prinzipiertes adaptives Routing für LLM-Reasoning durch Active Inference",
        "summary": "ODAR-Expert optimiert den Genauigkeit-Effizienz-Kompromiss beim LLM-Reasoning durch adaptives Routing zwischen schnellen und langsamen Pfaden, die auf Active Inference basieren. Das Framework erreicht starke Benchmark-Leistungen und reduziert gleichzeitig Rechenkosten um 82% durch prinzipierte Free-Energy-basierte Entscheidungsfindung."
      },
      "es": {
        "title": "ODAR: Enrutamiento Adaptativo Fundamentado para Razonamiento de LLM mediante Inferencia Activa",
        "summary": "ODAR-Expert optimiza el equilibrio precisión-eficiencia en el razonamiento de LLM mediante el enrutamiento adaptativo entre rutas rápidas y lentas fundamentadas en inferencia activa. El framework logra un desempeño sólido en los puntos de referencia mientras reduce los costos computacionales en un 82% mediante la toma de decisiones basada en energía libre fundamentada."
      }
    }
  },
  {
    "title": "From Flat Logs to Causal Graphs: Hierarchical Failure Attribution for LLM-based Multi-Agent Systems",
    "slug": "hierarchical-failure-attribution-llm-multi-agent-systems-chief",
    "url": "https://arxiv.org/abs/2602.23701",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "CHIEF transforms flat execution logs into hierarchical causal graphs to attribute failures in LLM-based multi-agent systems. Through oracle-guided backtracking and counterfactual screening, it reliably distinguishes root causes from propagated symptoms with higher accuracy than existing methods.",
    "content": "arXiv:2602.23701v1 Announce Type: new \nAbstract: LLM-powered Multi-Agent Systems (MAS) have demonstrated remarkable capabilities in complex domains but suffer from inherent fragility and opaque failure mechanisms. Existing failure attribution methods, whether relying on direct prompting, costly replays, or supervised fine-tuning, typically treat execution logs as flat sequences. This linear perspective fails to disentangle the intricate causal links inherent to MAS, leading to weak observability and ambiguous responsibility boundaries. To address these challenges, we propose CHIEF, a novel framework that transforms chaotic trajectories into a structured hierarchical causal graph. It then employs hierarchical oracle-guided backtracking to efficiently prune the search space via sybthesized virtual oracles. Finally, it implements counterfactual attribution via a progressive causal screening strategy to rigorously distinguish true root causes from propagated symptoms. Experiments on Who&When benchmark show that CHIEF outperforms eight strong and state-of-the-art baselines on both agent- and step-level accuracy. Ablation studies further confirm the critical role of each proposed module.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "从平坦日志到因果图：基于LLM的多智能体系统的分层故障归因",
        "summary": "CHIEF将平坦执行日志转换为分层因果图，以归因基于LLM的多智能体系统中的故障。通过预言机指导的回溯和反事实筛选，它能可靠地区分根本原因和传播症状，准确性高于现有方法。"
      },
      "fr": {
        "title": "Des Journaux Plats aux Graphiques de Causalité : Attribution Hiérarchique des Défaillances pour les Systèmes Multi-Agents basés sur LLM",
        "summary": "CHIEF transforme les journaux d'exécution plats en graphiques de causalité hiérarchiques pour attribuer les défaillances dans les systèmes multi-agents basés sur LLM. Grâce au rétrotraçage guidé par oracle et au criblage contrefactuel, il distingue de manière fiable les causes racines des symptômes propagés avec une précision plus élevée que les méthodes existantes."
      },
      "de": {
        "title": "Von flachen Protokollen zu Kausalitätsgraphen: Hierarchische Fehlerattribution für LLM-basierte Multi-Agent-Systeme",
        "summary": "CHIEF wandelt flache Ausführungsprotokolle in hierarchische Kausalitätsgraphen um, um Fehler in LLM-basierten Multi-Agent-Systemen zuzuordnen. Durch Oracle-gesteuertes Backtracking und kontrafaktisches Screening unterscheidet es zuverlässig Grundursachen von propagierten Symptomen mit höherer Genauigkeit als bestehende Methoden."
      },
      "es": {
        "title": "De Registros Planos a Gráficos Causales: Atribución Jerárquica de Fallos para Sistemas Multi-Agente basados en LLM",
        "summary": "CHIEF transforma registros de ejecución planos en gráficos causales jerárquicos para atribuir fallos en sistemas multi-agente basados en LLM. A través de rastreo guiado por oráculo y filtrado contrafáctico, distingue de manera confiable las causas raíz de los síntomas propagados con mayor precisión que los métodos existentes."
      }
    }
  },
  {
    "title": "ProductResearch: Training E-Commerce Deep Research Agents via Multi-Agent Synthetic Trajectory Distillation",
    "slug": "productresearch-ecommerce-deep-research-agents-trajectory",
    "url": "https://arxiv.org/abs/2602.23716",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "ProductResearch synthesizes high-fidelity tool-use trajectories for training e-commerce shopping agents through multi-agent collaboration and reflective distillation. The framework enables compact models to achieve substantial improvements in research depth and user-perceived utility for complex shopping inquiries.",
    "content": "arXiv:2602.23716v1 Announce Type: new \nAbstract: Large Language Model (LLM)-based agents show promise for e-commerce conversational shopping, yet existing implementations lack the interaction depth and contextual breadth required for complex product research. Meanwhile, the Deep Research paradigm, despite advancing information synthesis in web search, suffers from domain gaps when transferred to e-commerce. We propose ProductResearch, a multi-agent framework that synthesizes high-fidelity, long-horizon tool-use trajectories for training robust e-commerce shopping agents. The framework employs a User Agent to infer nuanced shopping intents from behavioral histories, and a Supervisor Agent that orchestrates iterative collaboration with a Research Agent to generate synthetic trajectories culminating in comprehensive, insightful product research reports. These trajectories are rigorously filtered and distilled through a reflective internalization process that consolidates multi-agent supervisory interactions into coherent single-role training examples, enabling effective fine-tuning of LLM agents for complex shopping inquiries. Extensive experiments show that a compact MoE model fine-tuned on our synthetic data achieves substantial improvements over its base model in response comprehensiveness, research depth, and user-perceived utility, approaching the performance of frontier proprietary deep research systems and establishing multi-agent synthetic trajectory training as an effective and scalable paradigm for enhancing LLM-based shopping assistance.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "ProductResearch：通过多智能体合成轨迹蒸馏训练电子商务深度研究智能体",
        "summary": "ProductResearch通过多智能体协作和反思蒸馏为电子商务购物智能体的训练合成高保真工具使用轨迹。该框架使紧凑模型能够在研究深度和用户感知的复杂购物查询效用方面实现实质性改进。"
      },
      "fr": {
        "title": "ProductResearch : Formation d'Agents de Recherche Profonde pour le Commerce Électronique via Distillation de Trajectoires Synthétiques Multi-Agents",
        "summary": "ProductResearch synthétise des trajectoires d'utilisation d'outils haute fidélité pour l'entraînement d'agents de shopping de commerce électronique grâce à la collaboration multi-agents et à la distillation réflexive. Le framework permet aux modèles compacts d'atteindre des améliorations substantielles dans la profondeur de la recherche et l'utilité perçue par l'utilisateur pour les recherches commerciales complexes."
      },
      "de": {
        "title": "ProductResearch: Training von E-Commerce-Tiefenforschungs-Agenten durch Multi-Agent-Synthetische-Trajectorie-Destillation",
        "summary": "ProductResearch synthetisiert hochgenaue Tool-Use-Trajektorien für das Training von E-Commerce-Shopping-Agenten durch Multi-Agent-Zusammenarbeit und reflektive Destillation. Das Framework ermöglicht es kompakten Modellen, substantielle Verbesserungen in der Forschungstiefe und benutzergenommener Nützlichkeit für komplexe Shopping-Anfragen zu erreichen."
      },
      "es": {
        "title": "ProductResearch: Entrenamiento de Agentes de Investigación Profunda de Comercio Electrónico mediante Destilación de Trayectorias Sintéticas Multiagente",
        "summary": "ProductResearch sintetiza trayectorias de uso de herramientas de alta fidelidad para entrenar agentes de compra de comercio electrónico a través de colaboración multiagente y destilación reflexiva. El framework permite que modelos compactos logren mejoras sustanciales en profundidad de investigación y utilidad percibida por el usuario para búsquedas de compra complejas."
      }
    }
  },
  {
    "title": "The Auton Agentic AI Framework",
    "slug": "auton-agentic-ai-framework-architecture-agents",
    "url": "https://arxiv.org/abs/2602.23720",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "The Auton Framework provides a principled architecture for autonomous agents separating declarative Cognitive Blueprints from Runtime Engines, addressing the mismatch between stochastic LLM outputs and deterministic backend systems. The design includes hierarchical memory, formal safety constraints, and runtime optimizations for efficient multi-step execution.",
    "content": "arXiv:2602.23720v1 Announce Type: new \nAbstract: The field of Artificial Intelligence is undergoing a transition from Generative AI -- probabilistic generation of text and images -- to Agentic AI, in which autonomous systems execute actions within external environments on behalf of users. This transition exposes a fundamental architectural mismatch: Large Language Models (LLMs) produce stochastic, unstructured outputs, whereas the backend infrastructure they must control -- databases, APIs, cloud services -- requires deterministic, schema-conformant inputs. The present paper describes the Auton Agentic AI Framework, a principled architecture for standardizing the creation, execution, and governance of autonomous agent systems. The framework is organized around a strict separation between the Cognitive Blueprint, a declarative, language-agnostic specification of agent identity and capabilities, and the Runtime Engine, the platform-specific execution substrate that instantiates and runs the agent. This separation enables cross-language portability, formal auditability, and modular tool integration via the Model Context Protocol (MCP). The paper formalizes the agent execution model as an augmented Partially Observable Markov Decision Process (POMDP) with a latent reasoning space, introduces a hierarchical memory consolidation architecture inspired by biological episodic memory systems, defines a constraint manifold formalism for safety enforcement via policy projection rather than post-hoc filtering, presents a three-level self-evolution framework spanning in-context adaptation through reinforcement learning, and describes runtime optimizations -- including parallel graph execution, speculative inference, and dynamic context pruning -- that reduce end-to-end latency for multi-step agent workflows.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "Auton智能AI框架",
        "summary": "Auton框架为自主智能体提供了有原则的架构，将声明式认知蓝图与运行时引擎分离，解决了随机LLM输出与确定性后端系统之间的不匹配。该设计包括分层内存、正式安全约束和运行时优化，用于高效的多步执行。"
      },
      "fr": {
        "title": "Le Framework d'IA Agentique Auton",
        "summary": "Le framework Auton fournit une architecture principiée pour les agents autonomes, séparant les Blueprints Cognitifs déclaratifs des moteurs d'exécution, résolvant l'inadéquation entre les résultats stochastiques des LLM et les systèmes backend déterministes. La conception comprend une mémoire hiérarchique, des contraintes de sécurité formelles et des optimisations à l'exécution pour une exécution multi-étapes efficace."
      },
      "de": {
        "title": "Das Auton Agentic-AI-Framework",
        "summary": "Das Auton-Framework bietet eine prinzipienbasierte Architektur für autonome Agenten und trennt deklarative Cognitive Blueprints von Runtime Engines, um die Diskrepanz zwischen stochastischen LLM-Ausgaben und deterministischen Backend-Systemen zu beheben. Das Design umfasst hierarchisches Speicher, formale Sicherheitsbeschränkungen und Laufzeitoptimierungen für effiziente mehrstufige Ausführung."
      },
      "es": {
        "title": "El Framework de IA Agentivo Auton",
        "summary": "El framework Auton proporciona una arquitectura principiada para agentes autónomos, separando Planos Cognitivos declarativos de motores de ejecución, abordando la discrepancia entre salidas estocásticas de LLM y sistemas backend deterministas. El diseño incluye memoria jerárquica, restricciones de seguridad formales y optimizaciones en tiempo de ejecución para una ejecución eficiente de múltiples pasos."
      }
    }
  },
  {
    "title": "Unlocking Cognitive Capabilities and Analyzing the Perception-Logic Trade-off",
    "slug": "unlocking-cognitive-capabilities-perception-logic-tradeoff-mlm",
    "url": "https://arxiv.org/abs/2602.23730",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "MERaLiON2-Omni (Alpha) is a multilingual omni-perception model for Southeast Asia that decouples perception and reasoning to address their efficiency-stability trade-off. The research reveals how extended reasoning can destabilize low-level sensory processing through temporal drift and visual over-interpretation.",
    "content": "arXiv:2602.23730v1 Announce Type: new \nAbstract: Recent advancements in Multimodal Large Language Models (MLLMs) pursue omni-perception capabilities, yet integrating robust sensory grounding with complex reasoning remains a challenge, particularly for underrepresented regions. In this report, we introduce the research preview of MERaLiON2-Omni (Alpha), a 10B-parameter multilingual omni-perception tailored for Southeast Asia (SEA). We present a progressive training pipeline that explicitly decouples and then integrates \"System 1\" (Perception) and \"System 2\" (Reasoning) capabilities. First, we establish a robust Perception Backbone by aligning region-specific audio-visual cues (e.g., Singlish code-switching, local cultural landmarks) with a multilingual LLM through orthogonal modality adaptation. Second, to inject cognitive capabilities without large-scale supervision, we propose a cost-effective Generate-Judge-Refine pipeline. By utilizing a Super-LLM to filter hallucinations and resolve conflicts via a consensus mechanism, we synthesize high-quality silver data that transfers textual Chain-of-Thought reasoning to multimodal scenarios.\n  Comprehensive evaluation on our newly introduced SEA-Omni Benchmark Suite reveals an Efficiency-Stability Paradox: while reasoning acts as a non-linear amplifier for abstract tasks (boosting mathematical and instruction-following performance significantly), it introduces instability in low-level sensory processing. Specifically, we identify Temporal Drift in long-context audio, where extended reasoning desynchronizes the model from acoustic timestamps, and Visual Over-interpretation, where logic overrides pixel-level reality. This report details the architecture, the data-efficient training recipe, and a diagnostic analysis of the trade-offs between robust perception and structured reasoning.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "解锁认知能力和分析感知-逻辑权衡",
        "summary": "MERaLiON2-Omni（Alpha）是为东南亚设计的多语言全感知模型，将感知和推理分离以解决效率-稳定性权衡。研究揭示了扩展推理如何通过时间漂移和视觉过度解释来破坏低级感官处理。"
      },
      "fr": {
        "title": "Débloquer les Capacités Cognitives et Analyser le Compromis Perception-Logique",
        "summary": "MERaLiON2-Omni (Alpha) est un modèle omni-perception multilingue pour l'Asie du Sud-Est qui découple la perception et le raisonnement pour résoudre leur compromis efficacité-stabilité. La recherche révèle comment le raisonnement étendu peut déstabiliser le traitement sensoriel de bas niveau à travers la dérive temporelle et la sur-interprétation visuelle."
      },
      "de": {
        "title": "Entsperren kognitiver Fähigkeiten und Analyse des Wahrnehmungs-Logik-Kompromisses",
        "summary": "MERaLiON2-Omni (Alpha) ist ein mehrsprachiges Omni-Perception-Modell für Südostasien, das Wahrnehmung und Reasoning entkoppelt, um ihren Effizienz-Stabilitäts-Kompromiss zu bewältigen. Die Forschung zeigt, wie erweitertes Reasoning die Verarbeitung sensorischer Signale auf niedriger Ebene durch zeitliche Verschiebung und visuelle Überinterpretation destabilisieren kann."
      },
      "es": {
        "title": "Desbloquear Capacidades Cognitivas y Analizar el Compromiso Percepción-Lógica",
        "summary": "MERaLiON2-Omni (Alpha) es un modelo omni-percepción multilingüe para el Sudeste Asiático que desacopla la percepción y el razonamiento para abordar su compensación eficiencia-estabilidad. La investigación revela cómo el razonamiento extendido puede desestabilizar el procesamiento sensorial de bajo nivel a través de la deriva temporal y la sobreinterpretación visual."
      }
    }
  },
  {
    "title": "Reasoning-Driven Multimodal LLM for Domain Generalization",
    "slug": "reasoning-driven-multimodal-llm-domain-generalization",
    "url": "https://arxiv.org/abs/2602.23777",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "RD-MLDG leverages reasoning chains in multimodal LLMs for domain generalization by addressing the trade-off between semantic richness and optimization efficiency. The framework achieves state-of-the-art results through multi-task cross-training and self-aligned reasoning regularization on standard benchmarks.",
    "content": "arXiv:2602.23777v1 Announce Type: new \nAbstract: This paper addresses the domain generalization (DG) problem in deep learning. While most DG methods focus on enforcing visual feature invariance, we leverage the reasoning capability of multimodal large language models (MLLMs) and explore the potential of constructing reasoning chains that derives image categories to achieve more robust predictions under domain shift. To this end, we systematically study the role of reasoning in DG using DomainBed-Reasoning, a newly constructed extension of DomainBed dataset, in which each sample is paired with class-relevant reasoning chains. Our analysis reveals two key challenges: (i) fine-tuning MLLMs with reasoning chains for classification is more challenging than direct label supervision, since the model must optimize complex reasoning sequences before label prediction; and (ii) mismatches in reasoning patterns between supervision signals and fine-tuned MLLMs lead to a trade-off between semantic richness (informative but harder to optimize) and optimization efficiency (easier to optimize but less informative). To address these issues, we propose RD-MLDG (Reasoning-Driven Multimodal LLM for Domain Generalization), a framework with two components: (i) MTCT (Multi-Task Cross-Training), which introduces an additional direct classification pathway to guide reasoning supervision; and (ii) SARR (Self-Aligned Reasoning Regularization), which preserves the semantic richness of reasoning chains while mitigating reasoning-pattern mismatches via iterative self-labeling. Experiments on standard DomainBed datasets (PACS, VLCS, OfficeHome, TerraInc) demonstrate that RD-MLDG achieves state-of-the-art performances, highlighting reasoning as a promising complementary signal for robust out-of-domain generalization.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "用于域泛化的推理驱动型多模态LLM",
        "summary": "RD-MLDG通过在多模态LLM中利用推理链来实现域泛化，解决语义丰富性和优化效率之间的权衡。该框架通过多任务交叉训练和自对齐推理正则化在标准基准上实现了最先进的结果。"
      },
      "fr": {
        "title": "LLM Multimodal Piloté par le Raisonnement pour la Généralisation de Domaine",
        "summary": "RD-MLDG exploite les chaînes de raisonnement dans les LLM multimodaux pour la généralisation de domaine en abordant le compromis entre la richesse sémantique et l'efficacité de l'optimisation. Le framework atteint des résultats à la pointe de la technologie grâce à l'entraînement croisé multi-tâches et à la régularisation du raisonnement auto-aligné sur les benchmarks standard."
      },
      "de": {
        "title": "Reasoning-gesteuertes Multimodales LLM zur Domänengeneralisierung",
        "summary": "RD-MLDG nutzt Reasoning-Ketten in multimodalen LLMs für die Domänengeneralisierung, indem es den Kompromiss zwischen semantischer Reichhaltigkeit und Optimierungseffizienz anspricht. Das Framework erreicht modernste Ergebnisse durch Multi-Task-Cross-Training und selbstausgerichtete Reasoning-Regularisierung auf Standard-Benchmarks."
      },
      "es": {
        "title": "LLM Multimodal Impulsado por Razonamiento para Generalización de Dominio",
        "summary": "RD-MLDG aprovecha las cadenas de razonamiento en LLM multimodales para la generalización de dominio abordando el compromiso entre riqueza semántica y eficiencia de optimización. El framework logra resultados de última generación a través del entrenamiento cruzado multi-tarea y regularización de razonamiento auto-alineado en referencias estándar."
      }
    }
  },
  {
    "title": "EMO-R3: Reflective Reinforcement Learning for Emotional Reasoning in Multimodal Large Language Models",
    "slug": "emo-r3-reflective-reinforcement-learning-emotional-reasoning",
    "url": "https://arxiv.org/abs/2602.23802",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "EMO-R3 proposes a reinforcement learning framework that enhances multimodal LLMs' emotional reasoning through structured thinking and reflective rewards based on visual-text consistency. The approach significantly improves both interpretability and emotional intelligence across multiple benchmarks, addressing a critical gap in how AI systems understand human emotions.",
    "content": "arXiv:2602.23802v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual reasoning and understanding tasks but still struggle to capture the complexity and subjectivity of human emotions. Existing approaches based on supervised fine-tuning often suffer from limited generalization and poor interpretability, while reinforcement learning methods such as Group Relative Policy Optimization fail to align with the intrinsic characteristics of emotional cognition. To address these challenges, we propose Reflective Reinforcement Learning for Emotional Reasoning (EMO-R3), a framework designed to enhance the emotional reasoning ability of MLLMs. Specifically, we introduce Structured Emotional Thinking to guide the model to perform step-by-step emotional reasoning in a structured and interpretable manner, and design a Reflective Emotional Reward that enables the model to re-evaluate its reasoning based on visual-text consistency and emotional coherence. Extensive experiments demonstrate that EMO-R3 significantly improves both the interpretability and emotional intelligence of MLLMs, achieving superior performance across multiple visual emotional understanding benchmarks.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "EMO-R3：多模态大型语言模型中情感推理的反思性强化学习",
        "summary": "EMO-R3提出了一个强化学习框架，通过结构化思维和基于视觉-文本一致性的反思性奖励来增强多模态LLM的情感推理能力。该方法在多个基准上显著提高了可解释性和情感智能，解决了人工智能系统理解人类情感的关键缺陷。"
      },
      "fr": {
        "title": "EMO-R3 : Apprentissage par renforcement réfléchi pour le raisonnement émotionnel dans les grands modèles de langage multimodaux",
        "summary": "EMO-R3 propose un cadre d'apprentissage par renforcement qui améliore le raisonnement émotionnel des LLM multimodaux grâce à la pensée structurée et aux récompenses réfléchies basées sur la cohérence visuelle-textuelle. L'approche améliore considérablement l'interprétabilité et l'intelligence émotionnelle sur plusieurs benchmarks, comblant un écart critique dans la manière dont les systèmes d'IA comprennent les émotions humaines."
      },
      "de": {
        "title": "EMO-R3: Reflexives Reinforcement Learning für emotionales Denken in multimodalen großen Sprachmodellen",
        "summary": "EMO-R3 schlägt ein Reinforcement-Learning-Framework vor, das das emotionale Denken von multimodalen LLMs durch strukturiertes Denken und reflektierende Belohnungen basierend auf visueller Textkonsistenz verbessert. Der Ansatz verbessert die Interpretierbarkeit und emotionale Intelligenz erheblich über mehrere Benchmarks hinweg und behebt eine kritische Lücke bei der Frage, wie KI-Systeme menschliche Emotionen verstehen."
      },
      "es": {
        "title": "EMO-R3: Aprendizaje por refuerzo reflexivo para razonamiento emocional en modelos de lenguaje grandes multimodales",
        "summary": "EMO-R3 propone un marco de aprendizaje por refuerzo que mejora el razonamiento emocional de los LLM multimodales a través del pensamiento estructurado y recompensas reflexivas basadas en la consistencia visual-textual. El enfoque mejora significativamente la interpretabilidad e inteligencia emocional en múltiples puntos de referencia, abordando una brecha crítica en cómo los sistemas de IA comprenden las emociones humanas."
      }
    }
  },
  {
    "title": "RUMAD: Reinforcement-Unifying Multi-Agent Debate",
    "slug": "rumad-reinforcement-unifying-multi-agent-debate",
    "url": "https://arxiv.org/abs/2602.23864",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "RUMAD presents a reinforcement learning framework for multi-agent debate that dynamically optimizes communication topology, reducing computational costs by over 80% while improving reasoning accuracy. The approach uses content-agnostic observations and multi-objective rewards to balance solution quality, cohesion, and efficiency.",
    "content": "arXiv:2602.23864v1 Announce Type: new \nAbstract: Multi-agent debate (MAD) systems leverage collective intelligence to enhance reasoning capabilities, yet existing approaches struggle to simultaneously optimize accuracy, consensus formation, and computational efficiency. Static topology methods lack adaptability to task complexity variations, while external LLM-based coordination risks introducing privileged knowledge that compromises debate neutrality. This work presents RUMAD (Reinforcement-Unifying Multi-Agent Debate), a novel framework that formulates dynamic communication topology control in MAD as a reinforcement learning (RL) problem.\n  RUMAD employs a content-agnostic observation scheme that captures high-level debate dynamics avoiding access to raw agent reasoning content. RUMAD uses a multi-objective reward to model solution quality, cohesion and efficiency. A PPO-trained controller dynamically adjusts edge weights in the communication graph, while a dual-threshold mechanism enables fine-grained control over both agent activation and information visibility.\n  Experimental evaluation across MMLU, GSM8K, and GPQA benchmarks demonstrates that RUMAD achieves substantial efficiency gains, reducing token costs by over 80\\%, while still improving reasoning accuracy compared to single LLM model and multiple MAD baselines. Notably, RUMAD trained exclusively on MMLU exhibits robust zero-shot generalization to out-of-domain (OOD) tasks, indicating that the learned communication strategies capture task-independent principles of effective multi-agent coordination. These results establish RUMAD as a efficient and robust approach for deploying multi-agent reasoning application with practical resource constraints.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "RUMAD：强化统一的多智能体辩论",
        "summary": "RUMAD提出了一个多智能体辩论的强化学习框架，动态优化通信拓扑，将计算成本降低80%以上，同时提高推理准确性。该方法使用内容无关的观察和多目标奖励来平衡解决方案质量、凝聚力和效率。"
      },
      "fr": {
        "title": "RUMAD : Débat multi-agents unifié par renforcement",
        "summary": "RUMAD présente un cadre d'apprentissage par renforcement pour le débat multi-agents qui optimise dynamiquement la topologie de communication, réduisant les coûts informatiques de plus de 80% tout en améliorant la précision du raisonnement. L'approche utilise des observations agnostiques au contenu et des récompenses multi-objectifs pour équilibrer la qualité de la solution, la cohésion et l'efficacité."
      },
      "de": {
        "title": "RUMAD: Durch Verstärkung einheitliche Multi-Agent-Debatte",
        "summary": "RUMAD präsentiert ein Reinforcement-Learning-Framework für Multi-Agent-Debatten, das die Kommunikationstopologie dynamisch optimiert, die Rechenkosten um über 80% senkt und gleichzeitig die Genauigkeit des Denkens verbessert. Der Ansatz verwendet inhaltsunabhängige Beobachtungen und mehrzielige Belohnungen, um die Lösungsqualität, Kohäsion und Effizienz auszugleichen."
      },
      "es": {
        "title": "RUMAD: Debate Multi-Agente Unificado por Refuerzo",
        "summary": "RUMAD presenta un marco de aprendizaje por refuerzo para debatir entre múltiples agentes que optimiza dinámicamente la topología de comunicación, reduciendo los costos computacionales en más del 80% mientras mejora la precisión del razonamiento. El enfoque utiliza observaciones agnósticas al contenido y recompensas multi-objetivo para equilibrar la calidad de la solución, la cohesión y la eficiencia."
      }
    }
  },
  {
    "title": "RF-Agent: Automated Reward Function Design via Language Agent Tree Search",
    "slug": "rf-agent-automated-reward-function-design",
    "url": "https://arxiv.org/abs/2602.23876",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "RF-Agent frames reward function design as a sequential decision-making process using language agents and Monte Carlo Tree Search, improving search efficiency and historical feedback utilization. The method demonstrates effectiveness across 17 diverse control tasks, reducing reliance on expert-designed reward functions.",
    "content": "arXiv:2602.23876v1 Announce Type: new \nAbstract: Designing efficient reward functions for low-level control tasks is a challenging problem. Recent research aims to reduce reliance on expert experience by using Large Language Models (LLMs) with task information to generate dense reward functions. These methods typically rely on training results as feedback, iteratively generating new reward functions with greedy or evolutionary algorithms. However, they suffer from poor utilization of historical feedback and inefficient search, resulting in limited improvements in complex control tasks. To address this challenge, we propose RF-Agent, a framework that treats LLMs as language agents and frames reward function design as a sequential decision-making process, enhancing optimization through better contextual reasoning. RF-Agent integrates Monte Carlo Tree Search (MCTS) to manage the reward design and optimization process, leveraging the multi-stage contextual reasoning ability of LLMs. This approach better utilizes historical information and improves search efficiency to identify promising reward functions. Outstanding experimental results in 17 diverse low-level control tasks demonstrate the effectiveness of our method. The source code is available at https://github.com/deng-ai-lab/RF-Agent.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "RF-Agent：通过语言智能体树搜索的自动奖励函数设计",
        "summary": "RF-Agent将奖励函数设计框架化为使用语言智能体和蒙特卡洛树搜索的顺序决策过程，提高了搜索效率和历史反馈利用率。该方法在17个不同的控制任务中展示了有效性，减少了对专家设计的奖励函数的依赖。"
      },
      "fr": {
        "title": "RF-Agent : Conception automatisée de fonction de récompense via recherche d'arbre d'agent linguistique",
        "summary": "RF-Agent cadre la conception de fonction de récompense comme un processus décisionnel séquentiel utilisant des agents linguistiques et la recherche d'arbre de Monte Carlo, améliorant l'efficacité de la recherche et l'utilisation des commentaires historiques. La méthode démontre son efficacité sur 17 tâches de contrôle diverses, réduisant la dépendance à l'égard des fonctions de récompense conçues par les experts."
      },
      "de": {
        "title": "RF-Agent: Automatisiertes Reward-Function-Design über Language Agent Tree Search",
        "summary": "RF-Agent rahmt das Design der Belohnungsfunktion als sequenziellen Entscheidungsprozess unter Verwendung von Sprachagenten und Monte-Carlo-Baumsuche ein, was die Sucheffizienz und die Nutzung historischer Rückmeldungen verbessert. Die Methode zeigt Effektivität bei 17 verschiedenen Steuerungsaufgaben und reduziert die Abhängigkeit von von Experten entworfenen Belohnungsfunktionen."
      },
      "es": {
        "title": "RF-Agent: Diseño Automatizado de Función de Recompensa mediante Búsqueda de Árbol de Agente de Lenguaje",
        "summary": "RF-Agent enmarca el diseño de la función de recompensa como un proceso de toma de decisiones secuencial utilizando agentes de lenguaje y búsqueda de árbol de Monte Carlo, mejorando la eficiencia de búsqueda y la utilización de retroalimentación histórica. El método demuestra efectividad en 17 tareas de control diversas, reduciendo la dependencia de funciones de recompensa diseñadas por expertos."
      }
    }
  },
  {
    "title": "Pessimistic Auxiliary Policy for Offline Reinforcement Learning",
    "slug": "pessimistic-auxiliary-policy-offline-rl",
    "url": "https://arxiv.org/abs/2602.23974",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "This paper introduces a pessimistic auxiliary strategy for offline reinforcement learning that maximizes the lower confidence bound of Q-functions to sample reliable actions and reduce approximation errors. The approach demonstrably improves efficacy across multiple offline RL benchmarks.",
    "content": "arXiv:2602.23974v1 Announce Type: new \nAbstract: Offline reinforcement learning aims to learn an agent from pre-collected datasets, avoiding unsafe and inefficient real-time interaction. However, inevitable access to out-ofdistribution actions during the learning process introduces approximation errors, causing the error accumulation and considerable overestimation. In this paper, we construct a new pessimistic auxiliary policy for sampling reliable actions. Specifically, we develop a pessimistic auxiliary strategy by maximizing the lower confidence bound of the Q-function. The pessimistic auxiliary strategy exhibits a relatively high value and low uncertainty in the vicinity of the learned policy, avoiding the learned policy sampling high-value actions with potentially high errors during the learning process. Less approximation error introduced by sampled action from pessimistic auxiliary strategy leads to the alleviation of error accumulation. Extensive experiments on offline reinforcement learning benchmarks reveal that utilizing the pessimistic auxiliary strategy can effectively improve the efficacy of other offline RL approaches.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "离线强化学习的悲观辅助策略",
        "summary": "本文为离线强化学习引入了一种悲观辅助策略，通过最大化Q函数的下置信界来采样可靠的动作并减少近似误差。该方法在多个离线RL基准测试中显著提高了有效性。"
      },
      "fr": {
        "title": "Politique Auxiliaire Pessimiste pour l'Apprentissage par Renforcement Hors Ligne",
        "summary": "Cet article introduit une stratégie auxiliaire pessimiste pour l'apprentissage par renforcement hors ligne qui maximise la limite de confiance inférieure des fonctions Q pour échantillonner des actions fiables et réduire les erreurs d'approximation. L'approche améliore démontrablement l'efficacité sur plusieurs benchmarks RL hors ligne."
      },
      "de": {
        "title": "Pessimistische Hilfspolitik für Offline-Reinforcement-Learning",
        "summary": "Das Papier führt eine pessimistische Hilfsstrategie für Offline-Reinforcement-Learning ein, die die untere Konfidenzgrenze von Q-Funktionen maximiert, um zuverlässige Maßnahmen zu sampeln und Approximationsfehler zu reduzieren. Der Ansatz verbessert nachweislich die Effizienz über mehrere Offline-RL-Benchmarks."
      },
      "es": {
        "title": "Política Auxiliar Pesimista para Aprendizaje por Refuerzo Fuera de Línea",
        "summary": "El artículo introduce una estrategia auxiliar pesimista para el aprendizaje por refuerzo fuera de línea que maximiza el límite inferior de confianza de las funciones Q para muestrear acciones confiables y reducir errores de aproximación. El enfoque mejora demostrablemente la eficacia en varios puntos de referencia de RL fuera de línea."
      }
    }
  },
  {
    "title": "Portfolio Reinforcement Learning with Scenario-Context Rollout",
    "slug": "portfolio-reinforcement-learning-scenario-context",
    "url": "https://arxiv.org/abs/2602.24037",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "The paper proposes scenario-context rollout for portfolio rebalancing that generates plausible market scenarios and addresses reward-transition mismatch through counterfactual next states. The method achieves up to 76% Sharpe ratio improvement and 53% maximum drawdown reduction compared to baseline approaches.",
    "content": "arXiv:2602.24037v1 Announce Type: new \nAbstract: Market regime shifts induce distribution shifts that can degrade the performance of portfolio rebalancing policies. We propose macro-conditioned scenario-context rollout (SCR) that generates plausible next-day multivariate return scenarios under stress events. However, doing so faces new challenges, as history will never tell what would have happened differently. As a result, incorporating scenario-based rewards from rollouts introduces a reward--transition mismatch in temporal-difference learning, destabilizing RL critic training.\n  We analyze this inconsistency and show it leads to a mixed evaluation target. Guided by this analysis, we construct a counterfactual next state using the rollout-implied continuations and augment the critic agent's bootstrap target. Doing so stabilizes the learning and provides a viable bias-variance tradeoff.\n  In out-of-sample evaluations across 31 distinct universes of U.S. equity and ETF portfolios, our method improves Sharpe ratio by up to 76% and reduces maximum drawdown by up to 53% compared with classic and RL-based portfolio rebalancing baselines.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "具有场景上下文展开的投资组合强化学习",
        "summary": "本文提出了用于投资组合重新平衡的场景上下文展开方法，通过生成合理的市场场景并通过反事实下一状态来解决奖励转换不匹配问题。该方法与基准方法相比，夏普比率提高了76%，最大回撤减少了53%。"
      },
      "fr": {
        "title": "Apprentissage par Renforcement de Portefeuille avec Déroulement de Contexte de Scénario",
        "summary": "L'article propose un déroulement de contexte de scénario pour le rééquilibrage du portefeuille qui génère des scénarios de marché plausibles et résout l'inadéquation récompense-transition par des états suivants contrefactuels. La méthode réalise jusqu'à 76 % d'amélioration du ratio de Sharpe et 53 % de réduction du drawdown maximal par rapport aux approches de base."
      },
      "de": {
        "title": "Portfolio-Reinforcement-Learning mit Szenario-Kontext-Rollout",
        "summary": "Das Papier schlägt ein Szenario-Kontext-Rollout für das Rebalancing von Portfolios vor, das plausible Marktszenarien generiert und die Reward-Übergangs-Unstimmigkeit durch kontrafaktische nächste Zustände adressiert. Die Methode erreicht bis zu 76% Verbesserung der Sharpe-Ratio und 53% Reduktion des maximalen Drawdowns im Vergleich zu Baseline-Ansätzen."
      },
      "es": {
        "title": "Aprendizaje por Refuerzo de Cartera con Despliegue de Contexto de Escenario",
        "summary": "El artículo propone un despliegue de contexto de escenario para el reequilibrio de cartera que genera escenarios de mercado plausibles y aborda el desajuste de recompensa-transición mediante estados siguientes contrafácticos. El método logra hasta un 76% de mejora en la relación de Sharpe y una reducción del 53% en la pérdida máxima en comparación con los enfoques de referencia."
      }
    }
  },
  {
    "title": "CIRCLE: A Framework for Evaluating AI from a Real-World Lens",
    "slug": "circle-framework-evaluating-ai-real-world-lens",
    "url": "https://arxiv.org/abs/2602.24055",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "CIRCLE presents a six-stage lifecycle-based framework that bridges the gap between model-centric metrics and real-world AI deployment outcomes by systematically translating stakeholder concerns into measurable signals. The framework integrates field testing, red teaming, and longitudinal studies for comprehensive evaluation.",
    "content": "arXiv:2602.24055v1 Announce Type: new \nAbstract: This paper proposes CIRCLE, a six-stage, lifecycle-based framework to bridge the reality gap between model-centric performance metrics and AI's materialized outcomes in deployment. While existing frameworks like MLOps focus on system stability and benchmarks measure abstract capabilities, decision-makers outside the AI stack lack systematic evidence about the behavior of AI technologies under real-world user variability and constraints. CIRCLE operationalizes the Validation phase of TEVV (Test, Evaluation, Verification, and Validation) by formalizing the translation of stakeholder concerns outside the stack into measurable signals. Unlike participatory design, which often remains localized, or algorithmic audits, which are often retrospective, CIRCLE provides a structured, prospective protocol for linking context-sensitive qualitative insights to scalable quantitative metrics. By integrating methods such as field testing, red teaming, and longitudinal studies into a coordinated pipeline, CIRCLE produces systematic knowledge: evidence that is comparable across sites yet sensitive to local context. This can enable governance based on materialized downstream effects rather than theoretical capabilities.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "CIRCLE：从现实视角评估人工智能的框架",
        "summary": "CIRCLE提出了一个基于生命周期的六阶段框架，通过系统地将利益相关者的关切转化为可测量的信号，来弥合以模型为中心的指标与实际人工智能部署结果之间的差距。该框架整合了现场测试、红队测试和纵向研究，用于全面评估。"
      },
      "fr": {
        "title": "CIRCLE : Un Cadre pour Évaluer l'IA à partir d'une Perspective du Monde Réel",
        "summary": "CIRCLE présente un cadre basé sur le cycle de vie en six étapes qui comble le fossé entre les métriques centrées sur le modèle et les résultats du déploiement réel de l'IA en traduisant systématiquement les préoccupations des parties prenantes en signaux mesurables. Le cadre intègre les tests sur le terrain, les tests adversariels et les études longitudinales pour une évaluation complète."
      },
      "de": {
        "title": "CIRCLE: Ein Rahmenwerk zur Bewertung von KI aus einer Real-World-Perspektive",
        "summary": "CIRCLE präsentiert ein lebenszyklusbasiertes sechsstufiges Rahmenwerk, das die Lücke zwischen modellzentrierten Metriken und Ergebnissen der realen KI-Bereitstellung schließt, indem es systematisch die Bedenken der Stakeholder in messbare Signale übersetzt. Das Rahmenwerk integriert Feldtests, Red Teaming und Längsschnittstudien für eine umfassende Bewertung."
      },
      "es": {
        "title": "CIRCLE: Un Marco para Evaluar la IA desde una Perspectiva del Mundo Real",
        "summary": "CIRCLE presenta un marco basado en el ciclo de vida de seis etapas que cierra la brecha entre las métricas centradas en el modelo y los resultados reales de la implementación de IA traduciendo sistemáticamente las preocupaciones de los interesados en señales medibles. El marco integra pruebas de campo, pruebas adversarias y estudios longitudinales para una evaluación integral."
      }
    }
  },
  {
    "title": "Human or Machine? A Preliminary Turing Test for Speech-to-Speech Interaction",
    "slug": "turing-test-speech-to-speech-interaction",
    "url": "https://arxiv.org/abs/2602.24080",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "Researchers conducted the first Turing test for speech-to-speech systems, finding that no current state-of-the-art system passes, with failures stemming from paralinguistic features and emotional expressivity rather than semantic understanding. They developed a taxonomy of human-likeness dimensions and an interpretable discrimination model.",
    "content": "arXiv:2602.24080v1 Announce Type: new \nAbstract: The pursuit of human-like conversational agents has long been guided by the Turing test. For modern speech-to-speech (S2S) systems, a critical yet unanswered question is whether they can converse like humans. To tackle this, we conduct the first Turing test for S2S systems, collecting 2,968 human judgments on dialogues between 9 state-of-the-art S2S systems and 28 human participants. Our results deliver a clear finding: no existing evaluated S2S system passes the test, revealing a significant gap in human-likeness. To diagnose this failure, we develop a fine-grained taxonomy of 18 human-likeness dimensions and crowd-annotate our collected dialogues accordingly. Our analysis shows that the bottleneck is not semantic understanding but stems from paralinguistic features, emotional expressivity, and conversational persona. Furthermore, we find that off-the-shelf AI models perform unreliably as Turing test judges. In response, we propose an interpretable model that leverages the fine-grained human-likeness ratings and delivers accurate and transparent human-vs-machine discrimination, offering a powerful tool for automatic human-likeness evaluation. Our work establishes the first human-likeness evaluation for S2S systems and moves beyond binary outcomes to enable detailed diagnostic insights, paving the way for human-like improvements in conversational AI systems.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "人类还是机器？语音交互初步图灵测试",
        "summary": "研究人员进行了首次语音交互系统图灵测试，发现当前没有最先进的系统能通过，失败原因来自副语言特征和情感表达性，而非语义理解。他们开发了人类相似度维度的分类法和可解释的判别模型。"
      },
      "fr": {
        "title": "Humain ou machine ? Un test de Turing préliminaire pour l'interaction vocale",
        "summary": "Les chercheurs ont mené le premier test de Turing pour les systèmes vocaux, constatant qu'aucun système de pointe actuel ne réussit, les échecs provenant de caractéristiques paralinguistiques et d'expressivité émotionnelle plutôt que de compréhension sémantique. Ils ont développé une taxonomie des dimensions de ressemblance humaine et un modèle de discrimination interprétable."
      },
      "de": {
        "title": "Mensch oder Maschine? Ein vorläufiger Turing-Test für Sprachinteraktion",
        "summary": "Forscher führten den ersten Turing-Test für Sprachübertragungssysteme durch und stellten fest, dass kein aktuelles State-of-the-Art-System besteht, mit Fehlern aufgrund von paralinguistischen Merkmalen und emotionaler Ausdrucksfähigkeit statt semantischen Verständnis. Sie entwickelten eine Taxonomie von Dimensionen der Menschenähnlichkeit und ein interpretierbares Diskriminanzmodell."
      },
      "es": {
        "title": "¿Humano o máquina? Una prueba de Turing preliminar para la interacción de voz a voz",
        "summary": "Los investigadores realizaron la primera prueba de Turing para sistemas de voz a voz, encontrando que ningún sistema de vanguardia actual aprueba, con fallos derivados de características paralingüísticas y expresividad emocional en lugar de comprensión semántica. Desarrollaron una taxonomía de dimensiones de semejanza humana y un modelo de discriminación interpretable."
      }
    }
  },
  {
    "title": "Bi-level RL-Heuristic Optimization for Real-world Winter Road Maintenance",
    "slug": "bi-level-rl-optimization-winter-road-maintenance",
    "url": "https://arxiv.org/abs/2602.24097",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "A bi-level optimization framework combines reinforcement learning for network partitioning with multi-objective vehicle routing for winter road maintenance on UK strategic networks. Results demonstrate balanced workloads, travel times below target thresholds, reduced emissions, and substantial cost savings.",
    "content": "arXiv:2602.24097v1 Announce Type: new \nAbstract: Winter road maintenance is critical for ensuring public safety and reducing environmental impacts, yet existing methods struggle to manage large-scale routing problems effectively and mostly reply on human decision. This study presents a novel, scalable bi-level optimization framework, validated on real operational data on UK strategic road networks (M25, M6, A1), including interconnected local road networks in surrounding areas for vehicle traversing, as part of the highway operator's efforts to solve existing planning challenges. At the upper level, a reinforcement learning (RL) agent strategically partitions the road network into manageable clusters and optimally allocates resources from multiple depots. At the lower level, a multi-objective vehicle routing problem (VRP) is solved within each cluster, minimizing the maximum vehicle travel time and total carbon emissions. Unlike existing approaches, our method handles large-scale, real-world networks efficiently, explicitly incorporating vehicle-specific constraints, depot capacities, and road segment requirements. Results demonstrate significant improvements, including balanced workloads, reduced maximum travel times below the targeted two-hour threshold, lower emissions, and substantial cost savings. This study illustrates how advanced AI-driven bi-level optimization can directly enhance operational decision-making in real-world transportation and logistics.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "双层强化学习启发式优化在真实冬季道路维护中的应用",
        "summary": "双层优化框架结合强化学习用于网络分区和多目标车辆路由以进行英国战略网络上的冬季道路维护。结果显示工作负载均衡、行驶时间低于目标阈值、排放减少和显著的成本节省。"
      },
      "fr": {
        "title": "Optimisation à deux niveaux RL-Heuristique pour l'entretien routier hivernal du monde réel",
        "summary": "Un cadre d'optimisation à deux niveaux combine l'apprentissage par renforcement pour le partitionnement de réseau avec le routage des véhicules multi-objectifs pour l'entretien routier hivernal sur les réseaux stratégiques britanniques. Les résultats démontrent des charges de travail équilibrées, des temps de parcours inférieurs aux seuils cibles, des émissions réduites et des économies de coûts substantielles."
      },
      "de": {
        "title": "Zweiebenen-RL-Heuristische Optimierung für die reale Winterstraßenunterhaltung",
        "summary": "Ein zweistufiges Optimierungsrahmenwerk kombiniert Reinforcement Learning für Netzwerkpartitionierung mit mehrzielgerichteter Fahrzeugroutierung für Winterstraßenunterhaltung auf britischen strategischen Netzwerken. Die Ergebnisse zeigen ausgewogene Arbeitslasten, Fahrtzeiten unter Schwellenwerten, reduzierte Emissionen und erhebliche Kostenersparnisse."
      },
      "es": {
        "title": "Optimización bicapa RL-Heurística para el mantenimiento real de carreteras en invierno",
        "summary": "Un marco de optimización bicapa combina aprendizaje por refuerzo para la partición de red con enrutamiento de vehículos multiobjetivo para el mantenimiento de carreteras invernales en redes estratégicas del Reino Unido. Los resultados demuestran cargas de trabajo equilibradas, tiempos de viaje por debajo de umbrales objetivo, emisiones reducidas y ahorros de costos sustanciales."
      }
    }
  },
  {
    "title": "Artificial Agency Program: Curiosity, compression, and communication in agents",
    "slug": "artificial-agency-program-curiosity-compression",
    "url": "https://arxiv.org/abs/2602.24100",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "The Artificial Agency Program proposes treating AI systems as resource-bounded agents driven by curiosity-as-learning-progress within computational constraints. The framework unifies predictive compression, intrinsic motivation, and information theory with practical multimodal experimentation.",
    "content": "arXiv:2602.24100v1 Announce Type: new \nAbstract: This paper presents the Artificial Agency Program (AAP), a position and research agenda for building AI systems as reality embedded, resource-bounded agents whose development is driven by curiosity-as-learning-progress under physical and computational constraints. The central thesis is that AI is most useful when treated as part of an extended human--tool system that increases sensing, understanding, and actuation capability while reducing friction at the interface between people, tools, and environments. The agenda unifies predictive compression, intrinsic motivation, empowerment and control, interface quality (unification), and language/self-communication as selective information bottlenecks. We formulate these ideas as a falsifiable program with explicit costs, staged experiments, and a concrete multimodal tokenized testbed in which an agent allocates limited budget among observation, action, and deliberation. The aim is to provide a conceptual and experimental framework that connects intrinsic motivation, information theory, thermodynamics, bounded rationality, and modern reasoning systems",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "人工智能体计划：代理中的好奇心、压缩和通信",
        "summary": "人工智能体计划提议将人工智能系统视为资源受限的智能体，由计算约束内学习进步的好奇心驱动。该框架统一了预测压缩、内在动机和信息论与实际多模态实验。"
      },
      "fr": {
        "title": "Programme d'Agentivité Artificielle : Curiosité, compression et communication chez les agents",
        "summary": "Le Programme d'Agentivité Artificielle propose de traiter les systèmes d'IA comme des agents limités en ressources motivés par la curiosité en tant que progrès d'apprentissage dans des contraintes de calcul. Le cadre unifie la compression prédictive, la motivation intrinsèque et la théorie de l'information avec l'expérimentation multimodale pratique."
      },
      "de": {
        "title": "Künstliches Agentenschaftsprogramm: Neugier, Kompression und Kommunikation bei Agenten",
        "summary": "Das Künstliche-Agentenschafts-Programm schlägt vor, KI-Systeme als ressourcenbegrenzte Agenten zu behandeln, die von Neugier als Lernfortschritt innerhalb von Rechenbeschränkungen angetrieben werden. Das Framework vereinheitlicht prädiktive Kompression, intrinsische Motivation und Informationstheorie mit praktischer multimodaler Experimentierung."
      },
      "es": {
        "title": "Programa de Agencia Artificial: Curiosidad, compresión y comunicación en agentes",
        "summary": "El Programa de Agencia Artificial propone tratar los sistemas de IA como agentes limitados en recursos impulsados por la curiosidad como progreso de aprendizaje dentro de restricciones computacionales. El marco unifica la compresión predictiva, la motivación intrínseca y la teoría de la información con experimentación multimodal práctica."
      }
    }
  },
  {
    "title": "Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance",
    "slug": "scope-salvaging-exploration-fine-grained-correction",
    "url": "https://arxiv.org/abs/2602.24110",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "SCOPE uses Process Reward Models to identify erroneous steps in suboptimal rollouts and applies fine-grained corrections, salvaging largely correct trajectories that would otherwise be discarded. The method achieves 46.6% accuracy on math reasoning and 53.4% on out-of-distribution tasks with increased diversity.",
    "content": "arXiv:2602.24110v1 Announce Type: new \nAbstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "回收失败：通过细粒度非策略指导在RLVR中拯救探索",
        "summary": "SCOPE使用过程奖励模型来识别次优汇总中的错误步骤，并应用细粒度更正，拯救那些本来会被丢弃的基本正确的轨迹。该方法在数学推理上达到46.6%的准确率，在分布外任务上达到53.4%，具有增加的多样性。"
      },
      "fr": {
        "title": "Recyclage des Échecs : Sauvetage de l'Exploration dans RLVR via Guidance Hors-Politique à Granularité Fine",
        "summary": "SCOPE utilise des modèles de récompense de processus pour identifier les étapes erronées dans les récapitulatifs sous-optimaux et applique des corrections à granularité fine, sauvegardant les trajectoires largement correctes qui seraient autrement rejetées. La méthode atteint 46,6% de précision en raisonnement mathématique et 53,4% sur les tâches hors distribution avec une diversité accrue."
      },
      "de": {
        "title": "Wiederverwertung von Fehlschlägen: Rettung der Exploration in RLVR durch feinkörnige Off-Policy-Anleitung",
        "summary": "SCOPE verwendet Prozessbelohnungsmodelle, um fehlerhafte Schritte in suboptimalen Durchläufen zu identifizieren und wendet feinkörnige Korrektionen an, um weitgehend korrekte Trajektorien zu retten, die sonst verworfen würden. Das Verfahren erreicht 46,6% Genauigkeit beim mathematischen Denken und 53,4% bei verteilungsfernen Aufgaben mit erhöhter Vielfalt."
      },
      "es": {
        "title": "Reciclaje de Fallos: Salvando la Exploración en RLVR mediante Orientación Fuera de Política de Grano Fino",
        "summary": "SCOPE utiliza modelos de recompensa de proceso para identificar pasos erróneos en ejecuciones subóptimas y aplica correcciones de grano fino, salvando trayectorias en gran medida correctas que de otro modo serían descartadas. El método logra una precisión del 46,6% en razonamiento matemático y del 53,4% en tareas fuera de distribución con mayor diversidad."
      }
    }
  },
  {
    "title": "LemmaBench: A Live, Research-Level Benchmark to Evaluate LLM Capabilities in Mathematics",
    "slug": "lemmabench-live-research-level-llm-mathematics",
    "url": "https://arxiv.org/abs/2602.24173",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "LemmaBench introduces an updatable benchmark for evaluating LLMs on research-level mathematics by automatically extracting and rewriting lemmas from arXiv into self-contained problems. Current state-of-the-art LLMs achieve only 10-15% accuracy in theorem proving, revealing a significant capability gap.",
    "content": "arXiv:2602.24173v1 Announce Type: new \nAbstract: We present a new approach for benchmarking Large Language Model (LLM) capabilities on research-level mathematics. Existing benchmarks largely rely on static, hand-curated sets of contest or textbook-style problems as proxies for mathematical research. Instead, we establish an updatable benchmark evaluating models directly on the latest research results in mathematics. This consists of an automatic pipeline that extracts lemmas from arXiv and rewrites them into self-contained statements by making all assumptions and required definitions explicit. It results in a benchmark that can be updated regularly with new problems taken directly from human mathematical research, while previous instances can be used for training without compromising future evaluations. We benchmark current state-of-the-art LLMs, which obtain around 10-15$\\%$ accuracy in theorem proving (pass@1) depending on the model, showing that there is currently a large margin of progression for LLMs to reach human-level proving capabilities in a research context.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "LemmaBench：用于评估LLM数学能力的实时研究级基准",
        "summary": "LemmaBench引入了一个可更新的基准，用于通过自动从arXiv中提取和重写引理为自成一体的问题来评估LLM在研究级数学上的性能。目前最先进的LLM在定理证明中只能达到10-15%的准确率，揭示了显著的能力差距。"
      },
      "fr": {
        "title": "LemmaBench : Un Benchmark en Direct de Niveau Recherche pour Évaluer les Capacités des LLM en Mathématiques",
        "summary": "LemmaBench introduit un benchmark mise à jour pour évaluer les LLMs en mathématiques au niveau de la recherche en extrayant automatiquement et en réécrivant les lemmes d'arXiv en problèmes autonomes. Les LLMs les plus avancés n'atteignent actuellement que 10-15% de précision en preuve de théorème, révélant un écart de capacité significatif."
      },
      "de": {
        "title": "LemmaBench: Ein Live-Benchmark auf Forschungsniveau zur Bewertung der Fähigkeiten von LLMs in Mathematik",
        "summary": "LemmaBench führt einen aktualisierbaren Benchmark zur Bewertung von LLMs in Mathematik auf Forschungsniveau ein, indem Lemmas automatisch aus arXiv extrahiert und in eigenständige Probleme umgeschrieben werden. Die derzeit modernsten LLMs erreichen nur 10-15% Genauigkeit beim Theorembeweisen, was auf eine signifikante Fähigkeitslücke hindeutet."
      },
      "es": {
        "title": "LemmaBench: Un Benchmark en Directo de Nivel de Investigación para Evaluar las Capacidades de LLM en Matemáticas",
        "summary": "LemmaBench presenta un benchmark actualizable para evaluar los LLM en matemáticas de nivel de investigación mediante la extracción automática y reescritura de lemas de arXiv en problemas autónomos. Los LLM más avanzados actualmente logran solo 10-15% de precisión en la demostración de teoremas, revelando una brecha de capacidad significativa."
      }
    }
  },
  {
    "title": "Learning Flexible Job Shop Scheduling under Limited Buffers and Material Kitting Constraints",
    "slug": "learning-flexible-job-shop-scheduling-constraints",
    "url": "https://arxiv.org/abs/2602.24180",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "This paper applies deep reinforcement learning with heterogeneous graph networks to flexible job shop scheduling that incorporates realistic constraints like limited buffers and material kitting. The approach outperforms traditional heuristics in makespan and pallet utilization while maintaining computational efficiency.",
    "content": "arXiv:2602.24180v1 Announce Type: new \nAbstract: The Flexible Job Shop Scheduling Problem (FJSP) originates from real production lines, while some practical constraints are often ignored or idealized in current FJSP studies, among which the limited buffer problem has a particular impact on production efficiency. To this end, we study an extended problem that is closer to practical scenarios--the Flexible Job Shop Scheduling Problem with Limited Buffers and Material Kitting. In recent years, deep reinforcement learning (DRL) has demonstrated considerable potential in scheduling tasks. However, its capacity for state modeling remains limited when handling complex dependencies and long-term constraints. To address this, we leverage a heterogeneous graph network within the DRL framework to model the global state. By constructing efficient message passing among machines, operations, and buffers, the network focuses on avoiding decisions that may cause frequent pallet changes during long-sequence scheduling, thereby helping improve buffer utilization and overall decision quality. Experimental results on both synthetic and real production line datasets show that the proposed method outperforms traditional heuristics and advanced DRL methods in terms of makespan and pallet changes, and also achieves a good balance between solution quality and computational cost. Furthermore, a supplementary video is provided to showcase a simulation system that effectively visualizes the progression of the production line.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "学习在有限缓冲区和物料组装约束下的灵活作业车间调度",
        "summary": "本文将具有异构图网络的深度强化学习应用于灵活作业车间调度，该调度包含有限缓冲区和物料组装等现实约束。该方法在跨度和托盘利用方面优于传统启发式算法，同时保持计算效率。"
      },
      "fr": {
        "title": "Apprentissage de l'Ordonnancement Flexible des Ateliers sous Contraintes de Tampons Limités et de Kitting de Matériaux",
        "summary": "Cet article applique l'apprentissage par renforcement profond avec des réseaux de graphes hétérogènes à l'ordonnancement flexible des ateliers qui intègre des contraintes réalistes telles que les tampons limités et le kitting des matériaux. L'approche surpasse les heuristiques traditionnelles en temps d'exécution et en utilisation des palettes tout en maintenant l'efficacité informatique."
      },
      "de": {
        "title": "Lernen von flexiblem Jobshop-Scheduling unter Einschränkungen durch begrenzte Puffer und Materialkitting",
        "summary": "Dieses Papier wendet tiefes Reinforcement Learning mit heterogenen Graphennetzen auf flexibles Jobshop-Scheduling an, das realistische Einschränkungen wie begrenzte Puffer und Materialkitting berücksichtigt. Das Verfahren übertrifft traditionelle Heuristiken bei Durchlaufzeit und Palettenauslastung und behält gleichzeitig die Recheneffizienz bei."
      },
      "es": {
        "title": "Aprendizaje de la Programación Flexible del Taller de Trabajo bajo Restricciones de Búferes Limitados y Kitting de Material",
        "summary": "Este documento aplica el aprendizaje por refuerzo profundo con redes de grafos heterogéneos a la programación flexible del taller de trabajo que incorpora restricciones realistas como búferes limitados y kitting de material. El enfoque supera a las heurísticas tradicionales en tiempo de ejecución y utilización de palets mientras se mantiene la eficiencia computacional."
      }
    }
  },
  {
    "title": "Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume",
    "slug": "umpire-uncertainty-quantification-multimodal-llms",
    "url": "https://arxiv.org/abs/2602.24195",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "UMPIRE is a training-free uncertainty quantification framework for multimodal LLMs that measures incoherence-adjusted semantic volume to detect errors across image, audio, and video modalities without external tools. The method consistently outperforms baselines in error detection and uncertainty calibration.",
    "content": "arXiv:2602.24195v1 Announce Type: new \nAbstract: Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "多模态大语言模型不确定性量化与不一致性调整语义体积",
        "summary": "UMPIRE是一个无需训练的多模态大语言模型不确定性量化框架,通过测量不一致性调整的语义体积来检测图像、音频和视频模态中的错误,无需外部工具。该方法在错误检测和不确定性校准中始终优于基准方法。"
      },
      "fr": {
        "title": "Quantification de l'incertitude pour les grands modèles de langage multimodaux avec volume sémantique ajusté pour l'incohérence",
        "summary": "UMPIRE est un cadre de quantification de l'incertitude sans formation pour les LLM multimodaux qui mesure le volume sémantique ajusté pour l'incohérence afin de détecter les erreurs dans les modalités d'image, d'audio et de vidéo sans outils externes. La méthode surpasse constamment les lignes de base en détection d'erreurs et calibrage de l'incertitude."
      },
      "de": {
        "title": "Unsicherheitsquantifizierung für multimodale große Sprachmodelle mit inkohärenzangepasstem semantischem Volumen",
        "summary": "UMPIRE ist ein trainingsfreies Unsicherheitsquantifizierungsframework für multimodale LLMs, das das inkohärenzangepasste semantische Volumen misst, um Fehler in Bild-, Audio- und Video-Modalitäten ohne externe Tools zu erkennen. Die Methode übertrifft ständig Baselines bei der Fehlererkennung und Unsicherheitskalibrierung."
      },
      "es": {
        "title": "Cuantificación de incertidumbre para modelos de lenguaje grandes multimodales con volumen semántico ajustado por incoherencia",
        "summary": "UMPIRE es un marco de cuantificación de incertidumbre sin entrenamiento para LLMs multimodales que mide el volumen semántico ajustado por incoherencia para detectar errores en modalidades de imagen, audio y video sin herramientas externas. El método supera constantemente a los baselines en detección de errores y calibración de incertidumbre."
      }
    }
  },
  {
    "title": "A Minimal Agent for Automated Theorem Proving",
    "slug": "minimal-agent-automated-theorem-proving",
    "url": "https://arxiv.org/abs/2602.24273",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "This paper proposes a minimal yet competitive baseline for AI-based theorem proving that implements iterative proof refinement, library search, and context management with significantly simpler architecture than state-of-the-art systems. The work demonstrates consistent advantages of iterative approaches and provides open-source reference.",
    "content": "arXiv:2602.24273v1 Announce Type: new \nAbstract: We propose a minimal agentic baseline that enables systematic comparison across different AI-based theorem prover architectures. This design implements the core features shared among state-of-the-art systems: iterative proof refinement, library search and context management. We evaluate our baseline using qualitatively different benchmarks and compare various popular models and design choices, and demonstrate competitive performance compared to state-of-the-art approaches, while using a significantly simpler architecture. Our results demonstrate consistent advantages of an iterative approach over multiple single-shot generations, especially in terms of sample efficiency and cost effectiveness. The implementation is released open-source as a candidate reference for future research and as an accessible prover for the community.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "自动定理证明的最小代理",
        "summary": "本文提出了一个针对基于人工智能的定理证明的最小但具有竞争力的基准,实现了迭代证明优化、库搜索和上下文管理,其架构明显比最先进的系统更简单。该工作展示了迭代方法的一致优势,并提供了开源参考。"
      },
      "fr": {
        "title": "Un agent minimal pour la preuve automatisée de théorèmes",
        "summary": "Cet article propose une ligne de base minimale mais compétitive pour la preuve de théorèmes basée sur l'IA qui implémente le raffinement itératif des preuves, la recherche de bibliothèque et la gestion du contexte avec une architecture considérablement plus simple que les systèmes de pointe. Le travail démontre les avantages constants des approches itératives et fournit une référence open-source."
      },
      "de": {
        "title": "Ein minimales Agentensystem für automatisierte Theorembeweise",
        "summary": "Dieses Papier schlägt eine minimale, aber wettbewerbsfähige Baseline für KI-basierte Theorembeweise vor, die iterative Beweisverbesserung, Bibliothekssuche und Kontextmanagement mit deutlich einfacherer Architektur als hochmoderne Systeme implementiert. Die Arbeit demonstriert konsistente Vorteile iterativer Ansätze und bietet eine Open-Source-Referenz."
      },
      "es": {
        "title": "Un agente mínimo para demostración automática de teoremas",
        "summary": "Este documento propone una línea base mínima pero competitiva para la demostración de teoremas basada en IA que implementa refinamiento iterativo de pruebas, búsqueda de bibliotecas y gestión de contexto con una arquitectura significativamente más simple que los sistemas de última generación. El trabajo demuestra ventajas consistentes de enfoques iterativos y proporciona una referencia de código abierto."
      }
    }
  },
  {
    "title": "DARE-bench: Evaluating Modeling and Instruction Fidelity of LLMs in Data Science",
    "slug": "dare-bench-llm-data-science-modeling-evaluation",
    "url": "https://arxiv.org/abs/2602.24288",
    "source": "arXiv cs.AI",
    "date": "2026-03-02T05:00:00.000Z",
    "summary": "DARE-bench introduces a 6,300-task benchmark with verifiable ground truth for evaluating LLMs on data science instruction following and machine learning modeling tasks. Fine-tuning on DARE-bench achieves 1.83x to 8x accuracy improvements, demonstrating its value for both evaluation and training.",
    "content": "arXiv:2602.24288v1 Announce Type: new \nAbstract: The fast-growing demands in using Large Language Models (LLMs) to tackle complex multi-step data science tasks create an emergent need for accurate benchmarking. There are two major gaps in existing benchmarks: (i) the lack of standardized, process-aware evaluation that captures instruction adherence and process fidelity, and (ii) the scarcity of accurately labeled training data. To bridge these gaps, we introduce DARE-bench, a benchmark designed for machine learning modeling and data science instruction following. Unlike many existing benchmarks that rely on human- or model-based judges, all tasks in DARE-bench have verifiable ground truth, ensuring objective and reproducible evaluation. To cover a broad range of tasks and support agentic tools, DARE-bench consists of 6,300 Kaggle-derived tasks and provides both large-scale training data and evaluation sets. Extensive evaluations show that even highly capable models such as gpt-o4-mini struggle to achieve good performance, especially in machine learning modeling tasks. Using DARE-bench training tasks for fine-tuning can substantially improve model performance. For example, supervised fine-tuning boosts Qwen3-32B's accuracy by 1.83x and reinforcement learning boosts Qwen3-4B's accuracy by more than 8x. These significant improvements verify the importance of DARE-bench both as an accurate evaluation benchmark and critical training data.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "DARE-bench:评估数据科学中LLMs的建模和指令保真度",
        "summary": "DARE-bench引入了一个包含6300个任务的基准,具有可验证的真实标注,用于评估LLMs在数据科学指令跟踪和机器学习建模任务中的表现。在DARE-bench上进行微调可实现1.83倍至8倍的准确性提升,展示了其在评估和训练中的价值。"
      },
      "fr": {
        "title": "DARE-bench: Évaluation de la fidélité de la modélisation et des instructions des LLM en science des données",
        "summary": "DARE-bench introduit un benchmark de 6 300 tâches avec une vérité fondamentale vérifiable pour évaluer les LLM sur les tâches de suivi d'instructions en science des données et de modélisation d'apprentissage automatique. Le fine-tuning sur DARE-bench réalise des améliorations de précision de 1,83x à 8x, démontrant sa valeur pour l'évaluation et la formation."
      },
      "de": {
        "title": "DARE-bench: Bewertung der Modellierungs- und Anweisungstreue von LLMs in Data Science",
        "summary": "DARE-bench führt ein Benchmark mit 6.300 Aufgaben mit überprüfbarer Grundwahrheit ein, um LLMs bei Aufgaben zur Befolgung von Anweisungen in der Datenwissenschaft und bei Aufgaben zur Modellierung des maschinellen Lernens zu bewerten. Fine-Tuning auf DARE-bench erreicht Genauigkeitsverbesserungen von 1,83x bis 8x und demonstriert seinen Wert für Bewertung und Training."
      },
      "es": {
        "title": "DARE-bench: Evaluación de la fidelidad de modelado e instrucciones de LLMs en ciencia de datos",
        "summary": "DARE-bench introduce un benchmark de 6,300 tareas con verdad fundamental verificable para evaluar LLMs en tareas de seguimiento de instrucciones en ciencia de datos y modelado de aprendizaje automático. El ajuste fino en DARE-bench logra mejoras de precisión de 1,83x a 8x, demostrando su valor tanto para evaluación como para entrenamiento."
      }
    }
  }
]