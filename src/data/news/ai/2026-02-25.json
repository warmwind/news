[
  {
    "title": "Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health",
    "slug": "childhood-obesity-prediction-ml-comparison",
    "url": "https://arxiv.org/abs/2602.20303",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "This study compares statistical and machine learning approaches to predict childhood obesity using data from 18,792 U.S. children, finding that logistic regression performs comparably to more complex models while persistent disparities across racial and socioeconomic groups remain. The research demonstrates that increased algorithmic complexity provides limited benefits over simpler approaches, emphasizing the need for improved data quality and equity-focused surveillance rather than more sophisticated models.",
    "content": "arXiv:2602.20303v1 Announce Type: new \nAbstract: Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "美国10-17岁儿童超重和肥胖的多层次决定因素：使用2021年全国儿童健康调查的统计和机器学习方法比较评估",
        "summary": "本研究比较统计和机器学习方法预测儿童肥胖症，使用来自18,792名美国儿童的数据，发现逻辑回归的性能与更复杂的模型相当，但种族和社会经济群体之间的持久差异依然存在。研究表明增加算法复杂性相比更简单的方法收益有限，强调需要改进数据质量和以公平性为重点的监测，而不是更复杂的模型。"
      },
      "ja": {
        "title": "米国10-17歳児童における過体重と肥満の多層的決定因子：2021年全国児童保健調査を使用した統計学的および機械学習アプローチの比較評価",
        "summary": "本研究は18,792人の米国児童のデータを用いた小児肥満症予測における統計的および機械学習アプローチを比較し、ロジスティック回帰がより複雑なモデルと同等の性能を発揮する一方、人種および社会経済的グループ間の持続的な格差が存在することを発見した。研究は、アルゴリズム複雑性の増加がより単純なアプローチに対して限定的な利益を提供することを示し、より洗練されたモデルではなく、データ品質の向上と公平性に焦点を当てた監視の必要性を強調している。"
      },
      "ko": {
        "title": "미국 10-17세 아동의 과체중 및 비만의 다층적 결정요인: 2021년 전국 아동 건강 조사를 사용한 통계 및 기계학습 방법의 비교 평가",
        "summary": "본 연구는 18,792명의 미국 아동 데이터를 사용하여 소아 비만 예측을 위한 통계적 및 기계학습 방법을 비교하였으며, 로지스틱 회귀가 더 복잡한 모델과 비슷한 성능을 보이면서 인종 및 사회경제적 집단 간 지속적인 격차가 존재함을 발견했다. 연구는 알고리즘 복잡성 증가가 더 간단한 방법에 비해 제한된 이점을 제공하며, 더 정교한 모델보다는 데이터 품질 개선 및 공평성 중심 모니터링의 필요성을 강조한다."
      },
      "fr": {
        "title": "Déterminants multiples du surpoids et de l'obésité chez les enfants américains âgés de 10 à 17 ans : évaluation comparative des approches statistiques et d'apprentissage automatique utilisant l'Enquête nationale sur la santé des enfants 2021",
        "summary": "Cette étude compare les approches statistiques et d'apprentissage automatique pour prédire l'obésité infantile en utilisant des données de 18 792 enfants américains, constatant que la régression logistique fonctionne de manière comparable aux modèles plus complexes tandis que les disparités persistantes entre les groupes raciaux et socioéconomiques demeurent. La recherche démontre que l'augmentation de la complexité algorithmique offre des bénéfices limités par rapport aux approches plus simples, soulignant le besoin d'améliorer la qualité des données et la surveillance axée sur l'équité plutôt que des modèles plus sophistiqués."
      },
      "de": {
        "title": "Mehrstufige Determinanten von Übergewicht und Adipositas bei US-Kindern im Alter von 10-17 Jahren: Vergleichende Bewertung statistischer und maschineller Lernansätze anhand der National Survey of Children's Health 2021",
        "summary": "Diese Studie vergleicht statistische und maschinelle Lernansätze zur Vorhersage von Adipositas bei Kindern anhand von Daten von 18.792 US-amerikanischen Kindern und stellt fest, dass die logistische Regression vergleichbar mit komplexeren Modellen funktioniert, während anhaltende Disparitäten zwischen rassischen und sozioökonomischen Gruppen bestehen bleiben. Die Forschung zeigt, dass eine erhöhte algorithmische Komplexität nur begrenzte Vorteile gegenüber einfacheren Ansätzen bietet und unterstreicht die Notwendigkeit verbesserter Datenqualität und auf Gerechtigkeit ausgerichteter Überwachung statt anspruchsvollerer Modelle."
      },
      "es": {
        "title": "Determinantes multiniveles del sobrepeso y la obesidad entre niños estadounidenses de 10-17 años: Evaluación comparativa de enfoques estadísticos y de aprendizaje automático utilizando la Encuesta Nacional de Salud Infantil de 2021",
        "summary": "Este estudio compara enfoques estadísticos y de aprendizaje automático para predecir la obesidad infantil utilizando datos de 18.792 niños estadounidenses, encontrando que la regresión logística funciona de manera comparable a modelos más complejos mientras persisten disparidades entre grupos raciales y socioeconómicos. La investigación demuestra que el aumento de la complejidad algorítmica proporciona beneficios limitados sobre enfoques más simples, enfatizando la necesidad de mejorar la calidad de los datos y la vigilancia centrada en la equidad en lugar de modelos más sofisticados."
      }
    }
  },
  {
    "title": "An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models",
    "slug": "rare-disease-phenotyping-llm-framework",
    "url": "https://arxiv.org/abs/2602.20324",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "Researchers developed RARE-PHENIX, an AI framework that extracts disease phenotypes from clinical notes and standardizes them to medical ontology terms for rare disease diagnosis. The framework outperformed existing methods by combining natural language processing with supervised ranking, showing potential to support clinician-assisted diagnosis in real-world settings.",
    "content": "arXiv:2602.20324v1 Announce Type: new \nAbstract: Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "使用大型语言模型从临床笔记进行端到端罕见病表型鉴定的人工智能框架",
        "summary": "研究人员开发了RARE-PHENIX，一个从临床笔记中提取疾病表型并将其标准化为医学本体术语用于罕见病诊断的人工智能框架。该框架通过结合自然语言处理与监督排序优于现有方法，展示了在真实环境中支持临床医生辅助诊断的潜力。"
      },
      "ja": {
        "title": "大規模言語モデルを使用した臨床ノートからのエンドツーエンド罕少疾患表現型鑑別のための人工知能フレームワーク",
        "summary": "研究者らはRAREPHENIXを開発し、臨床ノートから疾患表現型を抽出し、罕少疾患診断のための医学オントロジー用語に標準化する人工知能フレームワークである。このフレームワークは自然言語処理と教師ありランキングを組み合わせることで既存の方法を上回り、実際の設定で臨床医支援診断をサポートする可能性を示した。"
      },
      "ko": {
        "title": "대규모 언어 모델을 사용한 임상 기록에서 희귀 질병 표현형의 종단 간 식별을 위한 인공지능 프레임워크",
        "summary": "연구자들은 임상 기록에서 질병 표현형을 추출하고 희귀 질병 진단을 위해 의학 온톨로지 용어로 표준화하는 인공지능 프레임워크인 RARE-PHENIX를 개발했다. 이 프레임워크는 자연어 처리와 지도학습 순위를 결합하여 기존 방법을 능가했으며, 실제 환경에서 임상의 보조 진단을 지원할 잠재력을 보여주었다."
      },
      "fr": {
        "title": "Un cadre d'intelligence artificielle pour l'identification phénotypique des maladies rares de bout en bout à partir de notes cliniques utilisant des modèles de langage de grande taille",
        "summary": "Les chercheurs ont développé RARE-PHENIX, un cadre d'IA qui extrait les phénotypes de maladies des notes cliniques et les standardise en termes d'ontologie médicale pour le diagnostic des maladies rares. Le cadre a surpassé les méthodes existantes en combinant le traitement du langage naturel avec le classement supervisé, montrant le potentiel de soutenir le diagnostic assisté par clinicien dans les paramètres du monde réel."
      },
      "de": {
        "title": "Ein Rahmen für künstliche Intelligenz zur End-to-End-Phänotypisierung seltener Krankheiten aus klinischen Notizen unter Verwendung großer Sprachmodelle",
        "summary": "Forscher entwickelten RARE-PHENIX, einen KI-Rahmen, der Krankheitsphänotypen aus klinischen Notizen extrahiert und sie auf medizinische Ontologie-Begriffe standardisiert für die Diagnose seltener Krankheiten. Der Rahmen übertraf bestehende Methoden durch die Kombination von Verarbeitung natürlicher Sprache mit überwachtem Ranking und zeigte das Potenzial, die kliniker-unterstützte Diagnose in realen Szenarien zu unterstützen."
      },
      "es": {
        "title": "Un marco de inteligencia artificial para la identificación fenotípica de enfermedades raras de extremo a extremo a partir de notas clínicas utilizando modelos de lenguaje grande",
        "summary": "Los investigadores desarrollaron RARE-PHENIX, un marco de IA que extrae fenotipos de enfermedades de notas clínicas y los estandariza en términos de ontología médica para el diagnóstico de enfermedades raras. El marco superó los métodos existentes al combinar procesamiento de lenguaje natural con clasificación supervisada, mostrando potencial para apoyar el diagnóstico asistido por clínicos en entornos del mundo real."
      }
    }
  },
  {
    "title": "DMCD: Semantic-Statistical Framework for Causal Discovery",
    "slug": "dmcd-causal-discovery-framework",
    "url": "https://arxiv.org/abs/2602.20333",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "DMCD combines large language models' semantic reasoning with statistical validation to discover causal relationships in data, using AI to propose initial causal structures that are then refined through statistical testing. The approach achieved competitive performance on real-world datasets spanning industrial and environmental domains, demonstrating the value of combining semantic priors with statistical verification.",
    "content": "arXiv:2602.20333v1 Announce Type: new \nAbstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.\n  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "DMCD：用于因果发现的语义-统计框架",
        "summary": "DMCD结合大型语言模型的语义推理和统计验证来发现数据中的因果关系，使用人工智能提出初始因果结构，然后通过统计测试进行优化。该方法在跨越工业和环境领域的真实数据集上取得竞争性性能，展示了结合语义先验与统计验证的价值。"
      },
      "ja": {
        "title": "DMCD：因果発見のための意味論-統計的フレームワーク",
        "summary": "DMCDは大規模言語モデルの意味論的推論と統計的検証を組み合わせてデータ内の因果関係を発見し、AIを使用して初期因果構造を提案し、その後統計的テストを通じて洗練される。このアプローチは産業および環境領域にまたがる実世界のデータセットで競争力のあるパフォーマンスを達成し、意味論的先験と統計的検証の組み合わせの価値を実証した。"
      },
      "ko": {
        "title": "DMCD: 인과 발견을 위한 의미-통계 프레임워크",
        "summary": "DMCD는 대규모 언어 모델의 의미론적 추론과 통계적 검증을 결합하여 데이터의 인과관계를 발견하며, AI를 사용하여 초기 인과구조를 제안한 후 통계적 검증을 통해 정제한다. 이 접근법은 산업 및 환경 분야에 걸친 실제 데이터셋에서 경쟁력 있는 성능을 달성했으며, 의미론적 선행정보와 통계적 검증 결합의 가치를 입증했다."
      },
      "fr": {
        "title": "DMCD : cadre sémantique-statistique pour la découverte causale",
        "summary": "DMCD combine le raisonnement sémantique des modèles de langage de grande taille avec la validation statistique pour découvrir les relations causales dans les données, en utilisant l'IA pour proposer des structures causales initiales qui sont ensuite affinées par des tests statistiques. L'approche a obtenu des performances compétitives sur des ensembles de données réelles couvrant les domaines industriel et environnemental, démontrant la valeur de la combinaison de priors sémantiques avec la vérification statistique."
      },
      "de": {
        "title": "DMCD: Semantisch-statistisches Rahmenwerk zur Ursachenentdeckung",
        "summary": "DMCD kombiniert semantisches Reasoning großer Sprachmodelle mit statistischer Validierung, um kausale Beziehungen in Daten zu entdecken. Es nutzt KI, um anfängliche kausale Strukturen vorzuschlagen, die dann durch statistisches Testen verfeinert werden. Der Ansatz erzielte wettbewerbsfähige Leistungen auf echten Datensätzen in industriellen und Umweltbereichen und demonstriert den Wert der Kombination semantischer Vorwissen mit statistischer Verifikation."
      },
      "es": {
        "title": "DMCD: Marco semántico-estadístico para el descubrimiento causal",
        "summary": "DMCD combina el razonamiento semántico de modelos de lenguaje grande con validación estadística para descubrir relaciones causales en datos, utilizando IA para proponer estructuras causales iniciales que luego se refinan mediante pruebas estadísticas. El enfoque logró un desempeño competitivo en conjuntos de datos del mundo real que abarcan dominios industriales y ambientales, demostrando el valor de combinar priors semánticos con verificación estadística."
      }
    }
  },
  {
    "title": "Diffusion Modulation via Environment Mechanism Modeling for Planning",
    "slug": "diffusion-modulation-environment-planning",
    "url": "https://arxiv.org/abs/2602.20422",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "Researchers propose DMEMM, a novel method for offline reinforcement learning that improves diffusion-based planning by incorporating environment mechanics like transition dynamics and reward functions. The approach achieves state-of-the-art performance by ensuring consistency between generated trajectories and real-world environment behavior.",
    "content": "arXiv:2602.20422v1 Announce Type: new \nAbstract: Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "通过环境机制建模的扩散调节用于规划",
        "summary": "研究人员提出DMEMM，一种新颖的离线强化学习方法，通过融合转移动力学和奖励函数等环境力学来改进扩散规划。该方法通过确保生成的轨迹与真实环境行为的一致性来达到最先进的性能。"
      },
      "ja": {
        "title": "環境メカニズムモデリングによる拡散変調計画",
        "summary": "研究者らがDMEMMを提案し、遷移ダイナミクスと報酬関数などの環境メカニズムを組み込むことで拡散ベースの計画を改善する、オフライン強化学習の新しい方法。生成された軌跡と実環境の動作の一貫性を確保することで、最先端のパフォーマンスを達成。"
      },
      "ko": {
        "title": "환경 메커니즘 모델링을 통한 확산 조절 계획",
        "summary": "연구자들이 전환 역학 및 보상 함수와 같은 환경 메커니즘을 통합하여 확산 기반 계획을 개선하는 오프라인 강화학습의 새로운 방법 DMEMM을 제안합니다. 생성된 궤적과 실제 환경 행동 간의 일관성을 보장하여 최첨단 성능을 달성합니다."
      },
      "fr": {
        "title": "Modulation de diffusion via modélisation des mécanismes environnementaux pour la planification",
        "summary": "Les chercheurs proposent DMEMM, une nouvelle méthode d'apprentissage par renforcement hors ligne qui améliore la planification basée sur la diffusion en intégrant la mécanique de l'environnement comme la dynamique de transition et les fonctions de récompense. L'approche atteint des performances de pointe en assurant la cohérence entre les trajectoires générées et le comportement de l'environnement réel."
      },
      "de": {
        "title": "Diffusionsmodulation durch Umweltmechanismus-Modellierung für Planung",
        "summary": "Forscher schlagen DMEMM vor, eine neuartige Methode des Offline-Reinforcement-Learning, die die diffusionsbasierte Planung durch Einbeziehung von Umweltmechaniken wie Übergangsdynamiken und Belohnungsfunktionen verbessert. Der Ansatz erreicht hochmoderne Leistung durch Gewährleistung der Konsistenz zwischen generierten Trajektorien und realem Umweltverhalten."
      },
      "es": {
        "title": "Modulación de difusión mediante modelado de mecanismos ambientales para la planificación",
        "summary": "Los investigadores proponen DMEMM, un nuevo método de aprendizaje por refuerzo fuera de línea que mejora la planificación basada en difusión incorporando mecánicas del ambiente como dinámicas de transición y funciones de recompensa. El enfoque logra rendimiento de vanguardia al garantizar la consistencia entre las trayectorias generadas y el comportamiento real del ambiente."
      }
    }
  },
  {
    "title": "Implicit Intelligence -- Evaluating Agents on What Users Don't Say",
    "slug": "implicit-intelligence-agent-evaluation",
    "url": "https://arxiv.org/abs/2602.20424",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "A new evaluation framework tests whether AI agents can reason about implicit requirements beyond literal instruction-following, such as accessibility needs, privacy concerns, and contextual constraints. Testing 16 frontier models revealed the best performance achieved only 48.3% scenario pass rate, indicating substantial gaps in agents' contextual reasoning abilities.",
    "content": "arXiv:2602.20424v1 Announce Type: new \nAbstract: Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "隐性智能——评估代理对用户未说明内容的理解",
        "summary": "一个新的评估框架测试AI代理是否能够推理超越字面指令的隐性需求，例如无障碍需求、隐私问题和上下文约束。对16个前沿模型的测试显示最佳性能仅达到48.3%的场景通过率，表明代理在上下文推理能力上存在实质性差距。"
      },
      "ja": {
        "title": "暗黙的インテリジェンス——ユーザーが言わないことに関するエージェント評価",
        "summary": "新しい評価フレームワークは、アクセシビリティの必要性、プライバシーの懸念、文脈的制約など、字面的な指示に従うことを超えた暗黙的要件について推論できるかどうかをテストします。16の最先端モデルのテストでは、最高のパフォーマンスは48.3%のシナリオパスレートにすぎず、エージェントの文脈推理能力に実質的なギャップがあることを示しています。"
      },
      "ko": {
        "title": "암묵적 지능 -- 사용자가 말하지 않은 것에 대한 에이전트 평가",
        "summary": "새로운 평가 프레임워크는 접근성 필요, 개인정보 보호 우려, 상황 제약 등 문자 그대로의 지시 따르기를 넘어 암묵적 요구 사항을 추론할 수 있는지 테스트합니다. 16개의 최첨단 모델 테스트 결과 최고 성능은 48.3% 시나리오 통과율만 달성했으며, 에이전트의 상황 추론 능력에 실질적인 차이가 있음을 나타냅니다."
      },
      "fr": {
        "title": "Intelligence implicite -- Évaluation des agents sur ce que les utilisateurs ne disent pas",
        "summary": "Un nouveau cadre d'évaluation teste si les agents d'IA peuvent raisonner sur les exigences implicites au-delà du suivi littéral des instructions, telles que les besoins d'accessibilité, les préoccupations relatives à la confidentialité et les contraintes contextuelles. Les tests sur 16 modèles de pointe ont révélé que la meilleure performance ne réalisait que 48,3% de taux de réussite de scénario, indiquant des lacunes substantielles dans les capacités de raisonnement contextuel des agents."
      },
      "de": {
        "title": "Implizite Intelligenz -- Bewertung von Agenten anhand dessen, was Benutzer nicht sagen",
        "summary": "Ein neues Bewertungsrahmenwerk testet, ob KI-Agenten über explizite Anforderungen hinaus implizite Anforderungen wie Zugänglichkeitsbedarf, Datenschutzbedenken und kontextuelle Beschränkungen berücksichtigen können. Tests an 16 Frontier-Modellen zeigten, dass die beste Leistung nur 48,3% Szenario-Erfolgsquote erreichte, was auf erhebliche Lücken in den Kontextverständigungsfähigkeiten der Agenten hindeutet."
      },
      "es": {
        "title": "Inteligencia implícita -- Evaluación de agentes sobre lo que los usuarios no dicen",
        "summary": "Un nuevo marco de evaluación prueba si los agentes de IA pueden razonar sobre requisitos implícitos más allá del seguimiento literal de instrucciones, como necesidades de accesibilidad, preocupaciones de privacidad y restricciones contextuales. Las pruebas en 16 modelos fronterizos revelaron que el mejor desempeño logró solo una tasa de éxito de escenario del 48.3%, indicando brechas sustanciales en las capacidades de razonamiento contextual de los agentes."
      }
    }
  },
  {
    "title": "Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use",
    "slug": "tool-description-optimization-llm-agents",
    "url": "https://arxiv.org/abs/2602.20426",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "Trace-Free+ is a curriculum learning framework that optimizes tool interface descriptions to improve how language models select and use tools without requiring execution traces. Experiments showed up to 93% improvement in recommendation accuracy when agents use learned tool descriptions rather than template-based ones, with strong generalization to unseen tools.",
    "content": "arXiv:2602.20426v1 Announce Type: new \nAbstract: The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "学习重写工具描述以实现可靠的LLM代理工具使用",
        "summary": "Trace-Free+是一个课程学习框架，优化工具界面描述以改进语言模型选择和使用工具的方式，无需执行跟踪。实验显示当代理使用学习的工具描述而不是基于模板的描述时，推荐准确性提高了93%，对未见过的工具具有强的泛化能力。"
      },
      "ja": {
        "title": "信頼できるLLMエージェントツール使用のためのツール説明を書き直すことを学習",
        "summary": "Trace-Free+は、実行トレースを必要とせずに言語モデルがツールを選択して使用する方法を改善するためにツールインターフェース説明を最適化するカリキュラム学習フレームワークです。実験により、エージェントが学習したツール説明を使用する場合、テンプレートベースの説明ではなく、推奨精度が最大93%向上し、未知のツールへの強い一般化が示されました。"
      },
      "ko": {
        "title": "신뢰할 수 있는 LLM 에이전트 도구 사용을 위한 도구 설명 다시 쓰기 학습",
        "summary": "Trace-Free+는 실행 추적 없이 언어 모델이 도구를 선택하고 사용하는 방식을 개선하기 위해 도구 인터페이스 설명을 최적화하는 커리큘럼 학습 프레임워크입니다. 실험 결과 에이전트가 템플릿 기반 설명 대신 학습된 도구 설명을 사용할 때 권장 정확도가 최대 93% 향상되었으며, 보지 못한 도구에 강한 일반화 능력을 보였습니다."
      },
      "fr": {
        "title": "Apprendre à réécrire les descriptions d'outils pour l'utilisation fiable des outils LLM-Agent",
        "summary": "Trace-Free+ est un cadre d'apprentissage par curriculum qui optimise les descriptions d'interfaces d'outils pour améliorer la façon dont les modèles de langage sélectionnent et utilisent les outils sans nécessiter de traces d'exécution. Les expériences ont montré jusqu'à 93% d'amélioration de la précision des recommandations lorsque les agents utilisent des descriptions d'outils apprises plutôt que des descriptions basées sur des modèles, avec une forte généralisation aux outils non vus."
      },
      "de": {
        "title": "Lernen, Werkzeugbeschreibungen für zuverlässige LLM-Agent-Werkzeugnutzung umzuschreiben",
        "summary": "Trace-Free+ ist ein Curriculum-Learning-Framework, das Werkzeugschnittstellenbeschreibungen optimiert, um zu verbessern, wie Sprachmodelle Werkzeuge ohne Ausführungsverfolgung auswählen und verwenden. Experimente zeigten bis zu 93% Verbesserung der Empfehlungsgenauigkeit, wenn Agenten gelernte Werkzeugbeschreibungen verwenden, statt vorlagenbasierte, mit starker Verallgemeinerung auf unbekannte Werkzeuge."
      },
      "es": {
        "title": "Aprender a reescribir descripciones de herramientas para un uso confiable de herramientas LLM-Agent",
        "summary": "Trace-Free+ es un marco de aprendizaje por currículum que optimiza descripciones de interfaces de herramientas para mejorar cómo los modelos de lenguaje seleccionan y usan herramientas sin requerir trazas de ejecución. Los experimentos mostraron hasta 93% de mejora en la precisión de recomendaciones cuando los agentes usan descripciones de herramientas aprendidas en lugar de basadas en plantillas, con fuerte generalización a herramientas no vistas."
      }
    }
  },
  {
    "title": "PreScience: A Benchmark for Forecasting Scientific Contributions",
    "slug": "prescience-scientific-contribution-forecasting",
    "url": "https://arxiv.org/abs/2602.20459",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "PreScience is a benchmark dataset of 98K AI research papers designed to test whether AI can forecast scientific advances by predicting collaborators, relevant prior work, research contributions, and impact. The benchmark reveals substantial headroom in forecasting capabilities, with state-of-the-art language models achieving only moderate performance in predicting research contributions.",
    "content": "arXiv:2602.20459v1 Announce Type: new \nAbstract: Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "预科学：科学贡献预测基准",
        "summary": "PreScience是一个包含98K篇AI研究论文的基准数据集，用于测试AI是否能通过预测合作者、相关先前工作、研究贡献和影响来预测科学进步。该基准揭示了预测能力中存在大量空间，最先进的语言模型在预测研究贡献方面只能取得中等水平的性能。"
      },
      "ja": {
        "title": "PreScience：科学的貢献予測ベンチマーク",
        "summary": "PreScience は 98K 個の AI 研究論文から構成されたベンチマークデータセットで、AI が共同研究者、関連する先行研究、研究貢献、および影響を予測することで科学的進歩を予測できるかをテストするために設計されています。このベンチマークは予測能力に大きな余地があることを明らかにし、最先端の言語モデルは研究貢献の予測においてのみ中程度のパフォーマンスを達成しています。"
      },
      "ko": {
        "title": "PreScience: 과학적 기여 예측 벤치마크",
        "summary": "PreScience는 98K개의 AI 연구 논문으로 구성된 벤치마크 데이터세트이며, AI가 협력자, 관련 선행 연구, 연구 기여도, 그리고 영향을 예측하여 과학적 진전을 예측할 수 있는지 테스트하도록 설계되었습니다. 이 벤치마크는 예측 능력에 상당한 여지가 있음을 보여주며, 최첨단 언어 모델은 연구 기여도 예측에서만 중간 정도의 성능을 달성합니다."
      },
      "fr": {
        "title": "PreScience: Un référentiel pour prédire les contributions scientifiques",
        "summary": "PreScience est un ensemble de données de référence contenant 98K articles de recherche en IA conçu pour tester si l'IA peut prévoir les avancées scientifiques en prédisant les collaborateurs, les travaux antérieurs pertinents, les contributions de recherche et l'impact. Le référentiel révèle un potentiel considérable dans les capacités de prévision, les modèles de langage de pointe n'atteignant que des performances modérées dans la prédiction des contributions de recherche."
      },
      "de": {
        "title": "PreScience: Ein Benchmarking-Datensatz zur Vorhersage wissenschaftlicher Beiträge",
        "summary": "PreScience ist ein Benchmark-Datensatz aus 98K KI-Forschungspapieren, der entwickelt wurde, um zu testen, ob KI wissenschaftliche Fortschritte durch die Vorhersage von Mitarbeitern, relevanten Vorgängern, Forschungsbeiträgen und deren Auswirkungen vorhersagen kann. Der Benchmark zeigt erhebliches Potenzial in den Prognosefähigkeiten, wobei hochmoderne Sprachmodelle nur moderate Leistungen bei der Vorhersage von Forschungsbeiträgen erreichen."
      },
      "es": {
        "title": "PreScience: Un Benchmark para Pronosticar Contribuciones Científicas",
        "summary": "PreScience es un conjunto de datos de referencia de 98K artículos de investigación en IA diseñado para probar si la IA puede pronosticar avances científicos prediciendo colaboradores, trabajos previos relevantes, contribuciones de investigación e impacto. El benchmark revela un considerable margen en las capacidades de pronóstico, con los modelos de lenguaje de última generación alcanzando solo un desempeño moderado en la predicción de contribuciones de investigación."
      }
    }
  },
  {
    "title": "KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning",
    "slug": "kairosvl-time-series-semantic-reasoning",
    "url": "https://arxiv.org/abs/2602.20494",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "KairosVL extends time series analysis by incorporating semantic understanding and contextual reasoning alongside numerical modeling, using a two-round reinforcement learning approach to improve temporal reasoning. The model demonstrates competitive performance on complex time series tasks while improving generalization to unseen scenarios.",
    "content": "arXiv:2602.20494v1 Announce Type: new \nAbstract: Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "KairosVL：编排时间序列和语义实现统一推理",
        "summary": "KairosVL通过在数值建模的基础上融合语义理解和上下文推理来扩展时间序列分析，使用两轮强化学习方法改进时间推理。该模型在复杂时间序列任务上展现了具有竞争力的性能，同时改进了对未见情景的泛化能力。"
      },
      "ja": {
        "title": "KairosVL：時系列とセマンティクスの統合推論のための調整",
        "summary": "KairosVL は、セマンティック理解と文脈推論を数値モデリングと統合することにより時系列分析を拡張し、2ラウンドの強化学習アプローチを使用して時間推論を改善します。このモデルは複雑な時系列タスクで競争力のあるパフォーマンスを示し、未見のシナリオへの汎化を向上させます。"
      },
      "ko": {
        "title": "KairosVL: 시계열과 의미론을 위한 통합 추론 조정",
        "summary": "KairosVL은 수치 모델링과 함께 의미론적 이해와 맥락적 추론을 통합하여 시계열 분석을 확장하며, 2라운드 강화학습 접근법을 사용하여 시간적 추론을 개선합니다. 이 모델은 복잡한 시계열 작업에서 경쟁력 있는 성능을 보이면서 보지 못한 시나리오에 대한 일반화를 개선합니다."
      },
      "fr": {
        "title": "KairosVL: Orchestrer les séries temporelles et la sémantique pour un raisonnement unifié",
        "summary": "KairosVL étend l'analyse des séries temporelles en intégrant la compréhension sémantique et le raisonnement contextuel aux côtés de la modélisation numérique, en utilisant une approche d'apprentissage par renforcement en deux étapes pour améliorer le raisonnement temporel. Le modèle démontre des performances compétitives sur les tâches complexes de séries temporelles tout en améliorant la généralisation aux scénarios non vus."
      },
      "de": {
        "title": "KairosVL: Orchestrierung von Zeitreihen und Semantik für einheitliches Denken",
        "summary": "KairosVL erweitert die Zeitreihenanalyse durch die Integration von semantischem Verständnis und kontextuellem Denken neben numerischer Modellierung und verwendet einen zweistufigen Verstärkungslernansatz zur Verbesserung des zeitlichen Denkens. Das Modell demonstriert wettbewerbsfähige Leistung bei komplexen Zeitreihenaufgaben und verbessert gleichzeitig die Verallgemeinerung auf ungesehene Szenarien."
      },
      "es": {
        "title": "KairosVL: Orquestación de series temporales y semántica para razonamiento unificado",
        "summary": "KairosVL extiende el análisis de series temporales incorporando comprensión semántica y razonamiento contextual junto con modelado numérico, utilizando un enfoque de aprendizaje por refuerzo de dos rondas para mejorar el razonamiento temporal. El modelo demuestra un desempeño competitivo en tareas complejas de series temporales mientras mejora la generalización a escenarios no vistos."
      }
    }
  },
  {
    "title": "ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory",
    "slug": "actionengine-gui-agents-state-machine",
    "url": "https://arxiv.org/abs/2602.20502",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "ActionEngine transitions GUI agents from reactive step-by-step operation to programmatic planning by using one agent to map interface state machines and another to generate executable code for tasks. The approach achieved 95% task success on Reddit tasks with a single LLM call, reducing costs by 11.8x compared to vision-only baselines.",
    "content": "arXiv:2602.20502v1 Announce Type: new \nAbstract: Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.\n  We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.\n  To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.\n  This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.\n  Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "ActionEngine：通过状态机内存从响应式转向编程式GUI代理",
        "summary": "ActionEngine通过使用一个代理映射界面状态机，另一个代理生成可执行代码来处理任务，将GUI代理从响应式逐步操作转向编程式规划。该方法在Reddit任务上实现了95%的任务成功率，通过单次LLM调用，相比仅基于视觉的基线降低成本11.8倍。"
      },
      "ja": {
        "title": "ActionEngine：状態機械メモリを介したリアクティブからプログラム的GUI エージェントへの転換",
        "summary": "ActionEngine は、1 つのエージェントがインターフェース状態機械をマップし、別のエージェントがタスク用の実行可能コードを生成することにより、GUI エージェントをリアクティブなステップバイステップ操作からプログラム的計画へと移行させます。このアプローチは Reddit タスクで 95% のタスク成功率を達成し、単一の LLM 呼び出しで、ビジョン専用ベースラインと比較してコストを 11.8 倍削減します。"
      },
      "ko": {
        "title": "ActionEngine: 상태 머신 메모리를 통한 반응형에서 프로그래밍 방식 GUI 에이전트로의 전환",
        "summary": "ActionEngine은 하나의 에이전트가 인터페이스 상태 머신을 매핑하고 다른 에이전트가 작업을 위한 실행 가능한 코드를 생성하여 GUI 에이전트를 반응형 단계별 작업에서 프로그래밍 방식 계획으로 전환합니다. 이 접근법은 Reddit 작업에서 95% 작업 성공률을 달성하며 단일 LLM 호출로 비전 전용 기준선에 비해 비용을 11.8배 절감합니다."
      },
      "fr": {
        "title": "ActionEngine: De la réaction à la programmation des agents GUI via la mémoire des machines d'état",
        "summary": "ActionEngine fait la transition des agents GUI d'une opération réactive étape par étape à une planification programmatique en utilisant un agent pour mapper les machines d'état de l'interface et un autre pour générer du code exécutable pour les tâches. L'approche a atteint un taux de succès de 95% sur les tâches Reddit avec un seul appel LLM, réduisant les coûts de 11,8x par rapport aux lignes de base basées sur la vision uniquement."
      },
      "de": {
        "title": "ActionEngine: Von reaktiven zu programmatischen GUI-Agenten über Zustandsmaschinengedächtnis",
        "summary": "ActionEngine übernimmt GUI-Agenten von reaktiven schrittweisen Operationen zu programmatischer Planung, indem ein Agent Schnittstellenzustandsmaschinen abbildet und ein anderer ausführbaren Code für Aufgaben generiert. Der Ansatz erreichte eine Aufgabenerfolgsquote von 95% bei Reddit-Aufgaben mit einem einzigen LLM-Aufruf und reduzierte die Kosten um das 11,8-fache im Vergleich zu reinen Vision-basierten Baselines."
      },
      "es": {
        "title": "ActionEngine: De agentes GUI reactivos a programáticos a través de la memoria de máquinas de estado",
        "summary": "ActionEngine transiciona los agentes GUI de operación reactiva paso a paso a planificación programática utilizando un agente para mapear máquinas de estado de interfaz y otro para generar código ejecutable para tareas. El enfoque logró una tasa de éxito del 95% en tareas de Reddit con una única llamada LLM, reduciendo costos 11.8x en comparación con líneas base solo de visión."
      }
    }
  },
  {
    "title": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination",
    "slug": "inner-speech-behavior-guides-imitation",
    "url": "https://arxiv.org/abs/2602.20517",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "MIMIC is an imitation learning framework that uses natural language \"inner speech\" as an intermediate representation to enable AI agents to learn diverse human behaviors while allowing fine-grained control at inference time. The approach enhances behavior diversity and fidelity across robotic and human-AI collaboration tasks without requiring additional training data.",
    "content": "arXiv:2602.20517v1 Announce Type: new \nAbstract: Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "内心语言作为行为指南:人类与AI协调的可控多样行为模仿",
        "summary": "MIMIC是一个模仿学习框架,使用自然语言\"内心语言\"作为中间表示,使AI代理能够学习多样的人类行为,同时在推理时允许细粒度控制。该方法在机器人和人类与AI协作任务中增强了行为多样性和保真度,无需额外的训练数据。"
      },
      "ja": {
        "title": "内心の音声を行動ガイドとして:人間-AI協調のための多様な行動の操作可能な模倣",
        "summary": "MIMICは、自然言語の\"内心の音声\"を中間表現として使用する模倣学習フレームワークです。AIエージェントが多様な人間の行動を学習できるようになり、推論時に細粒度の制御が可能になります。このアプローチは、ロボットと人間-AI協調タスク全体で行動の多様性と忠実度を向上させ、追加の訓練データを必要としません。"
      },
      "ko": {
        "title": "행동 가이드로서의 내적 음성: 인간-AI 협조를 위한 다양한 행동의 조종 가능한 모방",
        "summary": "MIMIC은 자연언어의 \"내적 음성\"을 중간 표현으로 사용하는 모방 학습 프레임워크입니다. AI 에이전트가 다양한 인간의 행동을 학습할 수 있게 해주며, 추론 시간에 세밀한 제어를 가능하게 합니다. 이 접근 방식은 로봇과 인간-AI 협력 작업 전반에 걸쳐 행동 다양성과 충실도를 향상시키며, 추가 학습 데이터가 필요하지 않습니다."
      },
      "fr": {
        "title": "Le langage intérieur comme guides de comportement: imitation contrôlable de comportements diversifiés pour la coordination humain-IA",
        "summary": "MIMIC est un cadre d'apprentissage par imitation qui utilise le \"langage intérieur\" en langage naturel comme représentation intermédiaire, permettant aux agents IA d'apprendre des comportements humains diversifiés tout en permettant un contrôle fin-grained au moment de l'inférence. L'approche améliore la diversité des comportements et la fidélité dans les tâches de collaboration robotique et humain-IA sans nécessiter de données d'entraînement supplémentaires."
      },
      "de": {
        "title": "Innere Sprache als Verhaltensleiter: Steuerbare Nachahmung vielfältiger Verhaltensweisen für Mensch-KI-Koordination",
        "summary": "MIMIC ist ein Imitationslernen-Framework, das natürliche Sprache \"innere Sprache\" als Zwischendarstellung verwendet, um KI-Agenten zu befähigen, vielfältige menschliche Verhaltensweisen zu erlernen, während gleichzeitig eine fein abgestimmte Kontrolle zur Inferenzzeit ermöglicht wird. Der Ansatz verbessert die Verhaltensvielfalt und -treue in Robotik- und Mensch-KI-Zusammenarbeitsaufgaben ohne zusätzliche Trainingsdaten."
      },
      "es": {
        "title": "El lenguaje interno como guías de comportamiento: imitación controlable de comportamientos diversos para la coordinación humano-IA",
        "summary": "MIMIC es un marco de aprendizaje por imitación que utiliza el \"lenguaje interno\" en lenguaje natural como representación intermedia, permitiendo que los agentes de IA aprendan comportamientos humanos diversos mientras permiten un control fino en el momento de la inferencia. El enfoque mejora la diversidad de comportamiento y la fidelidad en tareas de colaboración robótica y humano-IA sin requerir datos de entrenamiento adicionales."
      }
    }
  },
  {
    "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production",
    "slug": "logs-to-language-llm-recommendation",
    "url": "https://arxiv.org/abs/2602.20558",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "A reinforcement learning approach learns to optimally convert user interaction logs into natural language contexts for language model-based recommendation systems, achieving up to 93% improvement in recommendation accuracy. The learned verbalization strategies discovered interpretable patterns like user interest summarization and noise removal that improve downstream predictions.",
    "content": "arXiv:2602.20558v1 Announce Type: new \nAbstract: Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "从日志到语言:为生产中基于LLM的推荐学习最优表述",
        "summary": "强化学习方法学会将用户交互日志最优转换为自然语言上下文,用于基于语言模型的推荐系统,推荐准确率提高了93%。学习的表述策略发现了可解释的模式,如用户兴趣总结和噪声删除,改进了下游预测。"
      },
      "ja": {
        "title": "ログから言語へ:本番環境のLLMベースの推奨システムのための最適な言語化学習",
        "summary": "強化学習アプローチは、ユーザーインタラクションログを言語モデルベースの推奨システムの自然言語コンテキストに最適に変換することを学習し、推奨精度で最大93%の改善を実現します。学習された言語化戦略は、ユーザーの関心の要約とノイズ除去などの解釈可能なパターンを発見し、ダウンストリーム予測を改善します。"
      },
      "ko": {
        "title": "로그에서 언어로: 프로덕션에서 LLM 기반 추천을 위한 최적의 언어화 학습",
        "summary": "강화학습 접근 방식은 사용자 상호작용 로그를 언어 모델 기반 추천 시스템의 자연언어 컨텍스트로 최적으로 변환하는 방법을 학습하여, 추천 정확도를 최대 93% 개선합니다. 학습된 언어화 전략은 사용자 관심 요약 및 노이즈 제거와 같은 해석 가능한 패턴을 발견하여 다운스트림 예측을 개선합니다."
      },
      "fr": {
        "title": "Des journaux au langage: apprentissage de la verbalisation optimale pour les systèmes de recommandation basés sur LLM en production",
        "summary": "Une approche par apprentissage par renforcement apprend à convertir de manière optimale les journaux d'interaction utilisateur en contextes en langage naturel pour les systèmes de recommandation basés sur des modèles de langage, réalisant jusqu'à 93% d'amélioration de la précision des recommandations. Les stratégies de verbalisation apprises découvrent des modèles interprétables comme la synthèse des intérêts utilisateurs et la suppression du bruit qui améliorent les prédictions en aval."
      },
      "de": {
        "title": "Von Protokollen zur Sprache: Erlernung optimaler Verbalisierung für LLM-basierte Empfehlung in der Produktion",
        "summary": "Ein Verstärkungslernansatz lernt, Benutzerinteraktionsprotokolle optimal in natürliche Sprachkontexte für sprachmodellgestützte Empfehlungssysteme zu konvertieren und erreicht eine bis zu 93%ige Verbesserung der Empfehlungsgenauigkeit. Die erlernten Verbalisierungsstrategien entdeckten interpretierbare Muster wie Benutzerinteressenszusammenfassung und Rauschentfernung, die nachgelagerte Vorhersagen verbessern."
      },
      "es": {
        "title": "De registros a lenguaje: aprendiendo verbalización óptima para recomendaciones basadas en LLM en producción",
        "summary": "Un enfoque de aprendizaje por refuerzo aprende a convertir óptimamente los registros de interacción del usuario en contextos de lenguaje natural para sistemas de recomendación basados en modelos de lenguaje, logrando una mejora de hasta el 93% en la precisión de las recomendaciones. Las estrategias de verbalización aprendidas descubrieron patrones interpretables como la síntesis de intereses del usuario y la eliminación de ruido que mejoran las predicciones posteriores."
      }
    }
  },
  {
    "title": "CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation",
    "slug": "causal-reasoning-benchmark-identification",
    "url": "https://arxiv.org/abs/2602.20571",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "CausalReasoningBenchmark evaluates automated causal inference systems by separately scoring causal identification (research design formulation) and estimation (numerical implementation) across 173 real-world queries. The benchmark revealed that while state-of-the-art language models correctly identify strategies 84% of the time, full identification-specification correctness drops to only 30%.",
    "content": "arXiv:2602.20571v1 Announce Type: new \nAbstract: Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "因果推理基准:用于因果识别和估计解耦评估的真实基准",
        "summary": "CausalReasoningBenchmark通过在173个真实查询中分别评分因果识别(研究设计制定)和估计(数值实现)来评估自动因果推理系统。该基准揭示,虽然最先进的语言模型在84%的时间内正确识别策略,但完整识别规范正确性仅为30%。"
      },
      "ja": {
        "title": "因果推理ベンチマーク:因果識別と推定の分離評価のためのリアルワールドベンチマーク",
        "summary": "CausalReasoningBenchmarkは、173の実世界クエリ全体で因果識別(研究設計定式化)と推定(数値実装)を個別にスコアリングすることで、自動因果推論システムを評価します。ベンチマークは、最先端の言語モデルが84%の時間で戦略を正しく識別しますが、完全な識別仕様の正確性はわずか30%に低下することを明らかにしました。"
      },
      "ko": {
        "title": "인과추론벤치마크: 인과식별 및 추정의 분리된 평가를 위한 실제 벤치마크",
        "summary": "CausalReasoningBenchmark는 173개의 실제 쿼리에서 인과 식별(연구 설계 공식화)과 추정(수치 구현)을 별도로 점수 매겨 자동화된 인과 추론 시스템을 평가합니다. 벤치마크는 최첨단 언어 모델이 84%의 시간에 전략을 올바르게 식별하지만, 완전한 식별 명세 정확도는 30%에 불과함을 드러냈습니다."
      },
      "fr": {
        "title": "CausalReasoningBenchmark: Un repère du monde réel pour l'évaluation désenchevêtrée de l'identification et l'estimation causales",
        "summary": "CausalReasoningBenchmark évalue les systèmes d'inférence causale automatisés en notant séparément l'identification causale (formulation de conception de recherche) et l'estimation (implémentation numérique) sur 173 requêtes du monde réel. L'étalon a révélé que bien que les modèles de langage de pointe identifient correctement les stratégies 84% du temps, la correction complète de l'identification-spécification chute à seulement 30%."
      },
      "de": {
        "title": "CausalReasoningBenchmark: Eine reale Benchmark zur entwobenen Bewertung kausaler Identifikation und Schätzung",
        "summary": "CausalReasoningBenchmark bewertet automatisierte Kausalinferenzsysteme, indem es kausale Identifikation (Forschungsdesign-Formulierung) und Schätzung (numerische Implementierung) über 173 reale Abfragen separat bewertet. Die Benchmark zeigte, dass hochmoderne Sprachmodelle in 84% der Zeit Strategien korrekt identifizieren, aber die vollständige Identifikations-Spezifikations-Korrektheit auf nur 30% sinkt."
      },
      "es": {
        "title": "CausalReasoningBenchmark: Un punto de referencia del mundo real para la evaluación desacoplada de identificación y estimación causal",
        "summary": "CausalReasoningBenchmark evalúa sistemas de inferencia causal automatizados puntuando por separado la identificación causal (formulación del diseño de investigación) y la estimación (implementación numérica) en 173 consultas del mundo real. El punto de referencia reveló que aunque los modelos de lenguaje de última generación identifican correctamente estrategias el 84% de las veces, la corrección completa de especificación de identificación cae a solo 30%."
      }
    }
  },
  {
    "title": "Physics-based phenomenological characterization of cross-modal bias in multimodal models",
    "slug": "cross-modal-bias-multimodal-models",
    "url": "https://arxiv.org/abs/2602.20624",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "This position paper argues that multimodal language models can exhibit systematic bias arising from complex cross-modal interactions that existing analysis methods fail to capture. Using physics-based surrogate models to analyze transformer dynamics, the authors demonstrate that multimodal inputs can reinforce modality dominance rather than mitigate it.",
    "content": "arXiv:2602.20624v1 Announce Type: new \nAbstract: The term 'algorithmic fairness' is used to evaluate whether AI models operate fairly in both comparative (where fairness is understood as formal equality, such as \"treat like cases as like\") and non-comparative (where unfairness arises from the model's inaccuracy, arbitrariness, or inscrutability) contexts. Recent advances in multimodal large language models (MLLMs) are breaking new ground in multimodal understanding, reasoning, and generation; however, we argue that inconspicuous distortions arising from complex multimodal interaction dynamics can lead to systematic bias. The purpose of this position paper is twofold: first, it is intended to acquaint AI researchers with phenomenological explainable approaches that rely on the physical entities that the machine experiences during training/inference, as opposed to the traditional cognitivist symbolic account or metaphysical approaches; second, it is to state that this phenomenological doctrine will be practically useful for tackling algorithmic fairness issues in MLLMs. We develop a surrogate physics-based model that describes transformer dynamics (i.e., semantic network structure and self-/cross-attention) to analyze the dynamics of cross-modal bias in MLLM, which are not fully captured by conventional embedding- or representation-level analyses. We support this position through multi-input diagnostic experiments: 1) perturbation-based analyses of emotion classification using Qwen2.5-Omni and Gemma 3n, and 2) dynamical analysis of Lorenz chaotic time-series prediction through the physical surrogate. Across two architecturally distinct MLLMs, we show that multimodal inputs can reinforce modality dominance rather than mitigate it, as revealed by structured error-attractor patterns under systematic label perturbation, complemented by dynamical analysis.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "多模态模型中跨模态偏差的基于物理的现象学表征",
        "summary": "这篇立场文章论证，多模态语言模型可能会因复杂的跨模态相互作用而产生系统性偏差，现有分析方法无法捕捉这一点。利用基于物理的代理模型分析变换器动态，作者证明多模态输入可能强化模态主导地位，而不是减轻它。"
      },
      "ja": {
        "title": "マルチモーダルモデルにおけるクロスモーダルバイアスの物理ベース現象学的特性化",
        "summary": "この立場論文は、マルチモーダル言語モデルが既存の分析方法では捉えられない複雑なクロスモーダル相互作用に由来する体系的なバイアスを示す可能性があると主張している。物理ベースの代理モデルを使用してトランスフォーマーの力学を分析することで、マルチモーダル入力がモダリティの優位性を軽減するのではなく強化する可能性があることを実証している。"
      },
      "ko": {
        "title": "다중모달 모델에서 크로스모달 편향의 물리 기반 현상학적 특성화",
        "summary": "이 입장 논문은 다중모달 언어 모델이 기존 분석 방법으로 포착할 수 없는 복잡한 크로스모달 상호작용에서 비롯된 체계적 편향을 나타낼 수 있음을 주장한다. 물리 기반 대체 모델을 사용하여 변환기 동역학을 분석함으로써 다중모달 입력이 모달리티 우위성을 완화하기보다는 강화할 수 있음을 보여준다."
      },
      "fr": {
        "title": "Caractérisation phénoménologique basée sur la physique du biais cross-modal dans les modèles multimodaux",
        "summary": "Cet article de position soutient que les modèles de langage multimodaux peuvent présenter des biais systématiques résultant d'interactions cross-modales complexes que les méthodes d'analyse existantes ne peuvent pas capturer. En utilisant des modèles de substitution basés sur la physique pour analyser la dynamique des transformateurs, les auteurs démontrent que les entrées multimodales peuvent renforcer la dominance de la modalité plutôt que de l'atténuer."
      },
      "de": {
        "title": "Physikbasierte phänomenologische Charakterisierung von Cross-Modal-Bias in Multimodal-Modellen",
        "summary": "Dieses Positionspapier argumentiert, dass multimodale Sprachmodelle systematische Verzerrungen aufweisen können, die sich aus komplexen Cross-Modal-Interaktionen ergeben, die bestehende Analysemethoden nicht erfassen können. Unter Verwendung physikbasierter Surrogatmodelle zur Analyse der Transformatordynamik zeigen die Autoren, dass multimodale Eingaben die Modalitätsdominanz eher verstärken als abschwächen können."
      },
      "es": {
        "title": "Caracterización fenomenológica basada en la física del sesgo cross-modal en modelos multimodales",
        "summary": "Este documento de posición argumenta que los modelos de lenguaje multimodal pueden presentar sesgos sistemáticos derivados de complejas interacciones cross-modales que los métodos de análisis existentes no pueden capturar. Utilizando modelos sustitutos basados en la física para analizar la dinámica de transformadores, los autores demuestran que las entradas multimodales pueden reforzar la dominancia modal en lugar de mitigarla."
      }
    }
  },
  {
    "title": "When can we trust untrusted monitoring? A safety case sketch across collusion strategies",
    "slug": "untrusted-monitoring-ai-safety",
    "url": "https://arxiv.org/abs/2602.20628",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "This research develops frameworks for rigorously evaluating the safety of using one AI model to monitor another, expanding the taxonomy of potential collusion strategies beyond previous work. The analysis identifies conditions where passive recognition could be an effective collusion strategy and presents explicit assumptions and unsolved challenges.",
    "content": "arXiv:2602.20628v1 Announce Type: new \nAbstract: AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "我们何时可以信任不可信的监控？跨勾结策略的安全案例草图",
        "summary": "本研究开发了框架，用于严格评估使用一个AI模型监控另一个AI模型的安全性，扩展了潜在勾结策略的分类法。分析确定了被动识别可能成为有效勾结策略的条件，并提出了明确的假设和未解决的挑战。"
      },
      "ja": {
        "title": "信頼できない監視をいつ信頼できるか？勾通戦略全体にわたる安全性ケーススケッチ",
        "summary": "本研究は、1つのAIモデルが別のAIモデルを監視することの安全性を厳密に評価するためのフレームワークを開発し、潜在的な勾通戦略の分類法を以前の研究を超えて拡張している。分析は、受動的認識が効果的な勾通戦略となり得る条件を特定し、明示的な仮定と未解決の課題を提示している。"
      },
      "ko": {
        "title": "신뢰할 수 없는 모니터링을 언제 신뢰할 수 있는가? 담합 전략 전반의 안전 사례 스케치",
        "summary": "본 연구는 한 AI 모델이 다른 모델을 모니터링하는 것의 안전성을 엄격히 평가하기 위한 프레임워크를 개발하며, 잠재적 담합 전략의 분류를 이전 작업을 넘어 확장한다. 분석은 수동 인식이 효과적인 담합 전략이 될 수 있는 조건을 식별하고 명시적인 가정과 해결되지 않은 과제를 제시한다."
      },
      "fr": {
        "title": "Quand pouvons-nous faire confiance à une surveillance non fiable ? Un croquis de cas de sécurité sur les stratégies de collusion",
        "summary": "Cette recherche développe des cadres pour évaluer rigoureusement la sécurité de l'utilisation d'un modèle d'IA pour surveiller un autre, élargissant la taxonomie des stratégies de collusion potentielles au-delà des travaux antérieurs. L'analyse identifie les conditions où la reconnaissance passive pourrait être une stratégie de collusion efficace et présente des hypothèses explicites et des défis non résolus."
      },
      "de": {
        "title": "Wann können wir nicht vertrauenswürdige Überwachung vertrauen? Eine Sicherheitsfallskizze über Kartellstrategien",
        "summary": "Diese Forschung entwickelt Rahmenwerke zur rigorosen Bewertung der Sicherheit der Verwendung eines KI-Modells zur Überwachung eines anderen und erweitert die Taxonomie potenzieller Kartellstrategien über bisherige Arbeiten hinaus. Die Analyse identifiziert Bedingungen, unter denen passive Erkennung eine wirksame Kartellstrategie sein könnte, und präsentiert explizite Annahmen und ungelöste Herausforderungen."
      },
      "es": {
        "title": "¿Cuándo podemos confiar en la supervisión no confiable? Un esbozo de caso de seguridad en estrategias de colusión",
        "summary": "Esta investigación desarrolla marcos para evaluar rigurosamente la seguridad de usar un modelo de IA para monitorear otro, expandiendo la taxonomía de posibles estrategias de colusión más allá del trabajo anterior. El análisis identifica condiciones donde el reconocimiento pasivo podría ser una estrategia de colusión efectiva y presenta suposiciones explícitas y desafíos sin resolver."
      }
    }
  },
  {
    "title": "Identifying two piecewise linear additive value functions from anonymous preference information",
    "slug": "piecewise-linear-preference-elicitation",
    "url": "https://arxiv.org/abs/2602.20638",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "This work addresses the problem of eliciting preference models from two decision-makers simultaneously without knowing which responses come from which person, assuming preferences can be represented by additive value functions with piecewise linear components.",
    "content": "arXiv:2602.20638v1 Announce Type: new \nAbstract: Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "从匿名偏好信息中识别两个分段线性加性价值函数",
        "summary": "本工作解决了在不知道哪些回答来自哪个人的情况下，同时从两个决策制定者处引出偏好模型的问题，假设偏好可以由具有分段线性成分的加性价值函数表示。"
      },
      "ja": {
        "title": "匿名嗜好情報から2つの区分線形加法型価値関数を特定する",
        "summary": "本研究は、2人の意思決定者から同時に嗜好モデルを抽出する問題に取り組んでいますが、どの回答がどの人から来たのかを知らずに、嗜好が区分線形成分を有する加法型価値関数で表現できると仮定しています。"
      },
      "ko": {
        "title": "익명 선호 정보로부터 두 개의 구간선형 가법 가치함수 식별",
        "summary": "본 논문은 어느 응답이 어느 사람으로부터 왔는지 알지 못한 채 두 의사결정자로부터 동시에 선호 모델을 도출하는 문제를 다루며, 선호는 구간선형 성분을 가진 가법 가치함수로 표현될 수 있다고 가정한다."
      },
      "fr": {
        "title": "Identification de deux fonctions de valeur additive linéaire par morceaux à partir d'informations de préférences anonymes",
        "summary": "Ce travail traite du problème d'élicitation de modèles de préférences de deux décideurs simultanément sans savoir quelles réponses proviennent de quelle personne, en supposant que les préférences peuvent être représentées par des fonctions de valeur additive avec des composantes linéaires par morceaux."
      },
      "de": {
        "title": "Identifikation von zwei stückweise linearen additiven Wertfunktionen aus anonymen Präferenzinformationen",
        "summary": "Diese Arbeit befasst sich mit dem Problem der gleichzeitigen Ermittlung von Präferenzmodellen von zwei Entscheidungsträgern, ohne zu wissen, welche Antworten von welcher Person stammen, unter der Annahme, dass Präferenzen durch additive Wertfunktionen mit stückweise linearen Komponenten dargestellt werden können."
      },
      "es": {
        "title": "Identificación de dos funciones de valor aditivo lineal por tramos a partir de información de preferencias anónimas",
        "summary": "Este trabajo aborda el problema de elicitar modelos de preferencia de dos tomadores de decisiones simultáneamente sin saber cuáles respuestas provienen de cuál persona, asumiendo que las preferencias pueden representarse mediante funciones de valor aditivo con componentes lineales por tramos."
      }
    }
  },
  {
    "title": "Grounding LLMs in Scientific Discovery via Embodied Actions",
    "slug": "grounding-llms-scientific-discovery-embodied-actions",
    "url": "https://arxiv.org/abs/2602.20639",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "Research proposes EmbodiedAct, a framework enabling LLMs to conduct scientific discovery through embodied actions with real-time perception and physical simulation. The approach addresses limitations of passive execution models by creating active agents that detect and respond to anomalies during long-horizon simulations. Experiments show significant improvements in reliability and accuracy for engineering design and scientific modeling tasks.",
    "content": "arXiv:2602.20639v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive \"execute-then-response\" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "通过体现行动将LLM接地于科学发现",
        "summary": "研究提出了EmbodiedAct框架，使LLM能够通过具有实时感知和物理仿真的体现行动进行科学发现。该方法通过创建能够在长视野仿真中检测和应对异常的主动代理，解决了被动执行模型的局限性。实验表明在工程设计和科学建模任务中的可靠性和准确性有显著提高。"
      },
      "fr": {
        "title": "Ancrer les LLM dans la découverte scientifique via des actions incarnées",
        "summary": "La recherche propose EmbodiedAct, un cadre permettant aux LLM de mener des découvertes scientifiques par des actions incarnées avec perception en temps réel et simulation physique. L'approche aborde les limitations des modèles d'exécution passifs en créant des agents actifs qui détectent et répondent aux anomalies lors de simulations à long horizon. Les expériences montrent des améliorations significatives de la fiabilité et de la précision pour les tâches de conception d'ingénierie et de modélisation scientifique."
      },
      "de": {
        "title": "Verankern von LLMs in der wissenschaftlichen Entdeckung durch verkörperte Aktionen",
        "summary": "Die Forschung schlägt EmbodiedAct vor, ein Framework, das es LLMs ermöglicht, wissenschaftliche Entdeckungen durch verkörperte Aktionen mit Echtzeit-Wahrnehmung und physikalischer Simulation durchzuführen. Der Ansatz behebt Einschränkungen passiver Ausführungsmodelle durch die Schaffung aktiver Agenten, die Anomalien während langfristiger Simulationen erkennen und reagieren. Experimente zeigen erhebliche Verbesserungen bei Zuverlässigkeit und Genauigkeit für Ingenieurdesign- und wissenschaftliche Modellierungsaufgaben."
      },
      "es": {
        "title": "Anclar LLMs en el descubrimiento científico a través de acciones encarnadas",
        "summary": "La investigación propone EmbodiedAct, un marco que permite a los LLM conducir descubrimiento científico a través de acciones encarnadas con percepción en tiempo real y simulación física. El enfoque aborda limitaciones de los modelos de ejecución pasivos al crear agentes activos que detectan y responden a anomalías durante simulaciones de largo horizonte. Los experimentos muestran mejoras significativas en confiabilidad y precisión para tareas de diseño de ingeniería y modelado científico."
      }
    }
  },
  {
    "title": "Recursive Belief Vision Language Model",
    "slug": "recursive-belief-vision-language-model",
    "url": "https://arxiv.org/abs/2602.20659",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "RB-VLA improves long-horizon manipulation for vision-language models by maintaining persistent task-aware state representations rather than relying solely on visual observations. The model achieves 52.5% higher success rates on complex pick-and-place tasks and reduces inference latency by 5x compared to existing approaches. The key innovation is a belief module that tracks task progress under partial observability without memory scaling.",
    "content": "arXiv:2602.20659v1 Announce Type: new \nAbstract: Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to {\\pi}0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "递归信念视觉语言模型",
        "summary": "RB-VLA通过维持持久的任务感知状态表示而非仅依赖视觉观察，改进了视觉语言模型的长视野操纵。该模型在复杂的拣选放置任务中实现了52.5%更高的成功率，并将推理延迟降低了5倍。主要创新是一个信念模块，在部分可观测性下跟踪任务进度而无需内存扩展。"
      },
      "fr": {
        "title": "Modèle de langage vision à croyance récursive",
        "summary": "RB-VLA améliore la manipulation à long horizon pour les modèles de langage visuel en maintenant des représentations d'état conscientes des tâches persistantes plutôt que de reposer uniquement sur les observations visuelles. Le modèle atteint des taux de succès 52,5% plus élevés sur les tâches complexes de saisir et placer et réduit la latence d'inférence de 5x par rapport aux approches existantes. L'innovation clé est un module de croyance qui suit la progression des tâches en observabilité partielle sans mise à l'échelle de la mémoire."
      },
      "de": {
        "title": "Rekursives Glaubens-Sicht-Sprachmodell",
        "summary": "RB-VLA verbessert die Manipulation im langen Horizont für Vision-Language-Modelle durch Aufrechterhaltung persistenter aufgabenbewusster Zustandsrepräsentationen statt ausschließlich auf visuelle Beobachtungen zu verlassen. Das Modell erreicht 52,5% höhere Erfolgsquoten bei komplexen Greif- und Platzierungsaufgaben und reduziert die Inferenzlatenz um das 5-fache im Vergleich zu bestehenden Ansätzen. Die Schlüsselinnovation ist ein Glaubensmodul, das Aufgabenfortschritt unter teilweiser Beobachtbarkeit ohne Speicherskalierung verfolgt."
      },
      "es": {
        "title": "Modelo de lenguaje de visión de creencia recursiva",
        "summary": "RB-VLA mejora la manipulación de largo horizonte para modelos de lenguaje visual manteniendo representaciones de estado conscientes de tareas persistentes en lugar de depender únicamente de observaciones visuales. El modelo logra tasas de éxito 52.5% más altas en tareas complejas de agarre y colocación y reduce la latencia de inferencia en 5x en comparación con enfoques existentes. La innovación clave es un módulo de creencia que rastrea el progreso de tareas bajo observabilidad parcial sin escalado de memoria."
      }
    }
  },
  {
    "title": "How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective",
    "slug": "foundational-skills-vlm-embodied-agents-native",
    "url": "https://arxiv.org/abs/2602.20687",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "Researchers present NativeEmbodied, a benchmark evaluating vision-language model embodied agents using native continuous action spaces rather than discretized commands. The benchmark reveals significant deficiencies in fundamental embodied skills that bottleneck performance on complex tasks. This work highlights critical gaps in how well VLMs translate visual understanding into precise real-world control.",
    "content": "arXiv:2602.20687v1 Announce Type: new \nAbstract: Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "基础技能如何影响基于VLM的体现代理：本地视角",
        "summary": "研究人员提出NativeEmbodied，一个基准，使用本地连续动作空间而非离散命令来评估视觉语言模型体现代理。基准揭示了在复杂任务性能瓶颈的基本体现技能中存在重大缺陷。这项工作突出了VLM将视觉理解转化为精确现实世界控制的严重差距。"
      },
      "fr": {
        "title": "Comment les compétences fondamentales influencent les agents incarnés basés sur VLM : une perspective native",
        "summary": "Les chercheurs présentent NativeEmbodied, un benchmark évaluant les agents incarnés de modèles de langage visuel utilisant des espaces d'action continus natifs plutôt que des commandes discrétisées. L'analyse comparative révèle des carences importantes dans les compétences incarnées fondamentales qui créent un goulot d'étranglement des performances sur les tâches complexes. Ce travail met en lumière des lacunes critiques dans la façon dont les VLM traduisent la compréhension visuelle en contrôle du monde réel précis."
      },
      "de": {
        "title": "Wie grundlegende Fähigkeiten VLM-basierte verkörperte Agenten beeinflussen: eine native Perspektive",
        "summary": "Forscher präsentieren NativeEmbodied, eine Benchmark zur Bewertung von verkörperten Agenten von Vision-Language-Modellen unter Verwendung nativer kontinuierlicher Aktionsräume anstelle von diskretisierten Befehlen. Die Benchmark offenbart erhebliche Mängel in grundlegenden verkörperten Fähigkeiten, die die Leistung bei komplexen Aufgaben beeinträchtigen. Diese Arbeit hebt kritische Lücken hervor, wie gut VLMs visuelles Verständnis in präzise reale Kontrolle übersetzen."
      },
      "es": {
        "title": "Cómo las habilidades fundamentales influyen en agentes encarnados basados en VLM: una perspectiva nativa",
        "summary": "Los investigadores presentan NativeEmbodied, un benchmark que evalúa agentes encarnados de modelos de lenguaje visual utilizando espacios de acción continuo nativos en lugar de comandos discretizados. El benchmark revela deficiencias significativas en habilidades encarnadas fundamentales que generan un cuello de botella en el desempeño de tareas complejas. Este trabajo destaca brechas críticas en cómo los VLMs traducen la comprensión visual en control del mundo real preciso."
      }
    }
  },
  {
    "title": "PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding",
    "slug": "promptcd-test-time-behavior-enhancement-contrastive",
    "url": "https://arxiv.org/abs/2602.20696",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "PromptCD introduces a test-time method for aligning LLM and vision-language model behavior without additional training by contrasting positive and negative prompts. The technique demonstrates consistent improvements on alignment objectives like helpfulness, honesty, and harmlessness while enhancing visual grounding in multimodal systems. This approach offers a cost-efficient alternative to traditional training-time alignment methods.",
    "content": "arXiv:2602.20696v1 Announce Type: new \nAbstract: Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the \"3H\" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.",
    "category": "ai"
  },
  {
    "title": "Online Algorithms with Unreliable Guidance",
    "slug": "online-algorithms-unreliable-guidance",
    "url": "https://arxiv.org/abs/2602.20706",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "This paper presents a framework for online decision-making where algorithms receive guidance that may be adversarially corrupted with known probability. The proposed Drop or Trust Blindly compiler transforms any online algorithm into a learning-augmented version with proven optimal or near-optimal performance on caching and bipartite matching. The method balances consistency and robustness as guidance reliability varies.",
    "content": "arXiv:2602.20706v1 Announce Type: new \nAbstract: This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $\\beta$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $\\beta = 0$ (a.k.a. consistency) as well as when $\\beta = 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $\\beta$ shifts from $0$ to $1$.\n  We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art.",
    "category": "ai"
  },
  {
    "title": "ICON: Indirect Prompt Injection Defense for Agents based on Inference-Time Correction",
    "slug": "icon-indirect-prompt-injection-defense-agents",
    "url": "https://arxiv.org/abs/2602.20708",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "ICON addresses indirect prompt injection attacks on LLM agents through a probing-to-mitigation framework that detects attacks in latent space and surgically corrects attention patterns. Unlike strict filtering approaches, ICON preserves task continuity while achieving competitive security metrics. The system reduces false positives by over 50% compared to baseline detection methods.",
    "content": "arXiv:2602.20708v1 Announce Type: new \nAbstract: Large Language Model (LLM) agents are susceptible to Indirect Prompt Injection (IPI) attacks, where malicious instructions in retrieved content hijack the agent's execution. Existing defenses typically rely on strict filtering or refusal mechanisms, which suffer from a critical limitation: over-refusal, prematurely terminating valid agentic workflows. We propose ICON, a probing-to-mitigation framework that neutralizes attacks while preserving task continuity. Our key insight is that IPI attacks leave distinct over-focusing signatures in the latent space. We introduce a Latent Space Trace Prober to detect attacks based on high intensity scores. Subsequently, a Mitigating Rectifier performs surgical attention steering that selectively manipulate adversarial query key dependencies while amplifying task relevant elements to restore the LLM's functional trajectory. Extensive evaluations on multiple backbones show that ICON achieves a competitive 0.4% ASR, matching commercial grade detectors, while yielding a over 50% task utility gain. Furthermore, ICON demonstrates robust Out of Distribution(OOD) generalization and extends effectively to multi-modal agents, establishing a superior balance between security and efficiency.",
    "category": "ai"
  },
  {
    "title": "Counterfactual Simulation Training for Chain-of-Thought Faithfulness",
    "slug": "counterfactual-simulation-training-chain-thought",
    "url": "https://arxiv.org/abs/2602.20710",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "Counterfactual Simulation Training improves the reliability of LLM reasoning explanations by training models to produce chain-of-thought outputs that accurately predict model behavior under hypothetical scenarios. Testing on models up to 235B parameters shows 35-point accuracy improvements in detecting spurious reasoning while maintaining generalizability. The approach offers a scalable method for monitoring and improving reasoning faithfulness.",
    "content": "arXiv:2602.20710v1 Announce Type: new \nAbstract: Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "反事实模拟训练用于思维链的忠实性",
        "summary": "反事实模拟训练通过训练模型生成准确预测假设场景下模型行为的思维链输出，来提高大语言模型推理解释的可靠性。在参数量达2350亿的模型上的测试显示，在检测虚假推理方面提高了35个百分点的准确率，同时保持了泛化性。该方法为监测和改进推理忠实性提供了一个可扩展的解决方案。"
      },
      "fr": {
        "title": "Entraînement par Simulation Contrefactuelle pour la Fidélité des Chaînes de Pensée",
        "summary": "L'Entraînement par Simulation Contrefactuelle améliore la fiabilité des explications de raisonnement des LLM en entraînant les modèles à produire des sorties de chaîne de pensée qui prédisent avec précision le comportement du modèle dans des scénarios hypothétiques. Les tests sur des modèles jusqu'à 235 milliards de paramètres montrent des améliorations de précision de 35 points dans la détection des raisonnements fallacieux tout en maintenant la généralisabilité. L'approche offre une méthode évolutive pour surveiller et améliorer la fidélité du raisonnement."
      },
      "de": {
        "title": "Kontrafaktisches Simulationstraining für Kettendurchdenken-Treue",
        "summary": "Kontrafaktisches Simulationstraining verbessert die Zuverlässigkeit von LLM-Erklärungen zum Denken durch das Training von Modellen zur Erzeugung von Kettendurchdenk-Ausgaben, die das Modellverhalten unter hypothetischen Szenarien genau vorhersagen. Tests mit Modellen bis zu 235 Milliarden Parametern zeigen Verbesserungen der Genauigkeit um 35 Punkte bei der Erkennung von fehlerhaften Überlegungen und erhalten gleichzeitig die Generalisierbarkeit. Der Ansatz bietet eine skalierbare Methode zur Überwachung und Verbesserung der Zuverlässigkeit des Denkens."
      },
      "es": {
        "title": "Entrenamiento de Simulación Contrafáctica para la Fidelidad de la Cadena de Pensamiento",
        "summary": "El Entrenamiento de Simulación Contrafáctica mejora la confiabilidad de las explicaciones de razonamiento de LLM al entrenar modelos para producir salidas de cadena de pensamiento que predicen con precisión el comportamiento del modelo bajo escenarios hipotéticos. Las pruebas en modelos de hasta 235 mil millones de parámetros muestran mejoras de precisión de 35 puntos en la detección de razonamientos espurios mientras se mantiene la generalización. El enfoque ofrece un método escalable para monitorear y mejorar la fidelidad del razonamiento."
      }
    }
  },
  {
    "title": "Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning",
    "slug": "buffer-matters-off-policy-reinforcement-learning",
    "url": "https://arxiv.org/abs/2602.20722",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "BAPO presents an off-policy reinforcement learning framework that improves data efficiency in LLM training by selectively reusing historically difficult samples and high-quality experiences. The approach achieves 12.5% average improvements across mathematics, planning, and visual reasoning tasks while resolving 40.7% of problems that base models consistently fail on. This addresses data waste and reward homogeneity limitations of traditional on-policy methods.",
    "content": "arXiv:2602.20722v1 Announce Type: new \nAbstract: Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "缓冲区很重要：大语言模型推理中离策略强化学习的力量",
        "summary": "BAPO提出了一个离策略强化学习框架，通过有选择性地重用历史上困难的样本和高质量的经验，改进了大语言模型训练中的数据效率。该方法在数学、规划和视觉推理任务上实现了12.5%的平均改进，同时解决了基础模型一直失败的40.7%的问题。这解决了传统在策略方法的数据浪费和奖励同质性的限制。"
      },
      "fr": {
        "title": "Le Buffer Importe : Libérer la Puissance de l'Apprentissage par Renforcement Hors-Politique dans le Raisonnement des Modèles de Langage",
        "summary": "BAPO présente un cadre d'apprentissage par renforcement hors-politique qui améliore l'efficacité des données dans l'entraînement des LLM en réutilisant sélectivement les échantillons historiquement difficiles et les expériences de haute qualité. L'approche réalise des améliorations moyennes de 12,5% sur les tâches de mathématiques, de planification et de raisonnement visuel tout en résolvant 40,7% des problèmes que les modèles de base ne réussissent pas. Cela répond aux limitations de gaspillage de données et d'homogénéité des récompenses des méthodes traditionnelles en-politique."
      },
      "de": {
        "title": "Buffer ist wichtig: Freisetzen der Kraft von Off-Policy-Verstärkungslernten in großen Sprachmodellen",
        "summary": "BAPO präsentiert ein Off-Policy-Verstärkungslernrahmenwerk, das die Dateneffizienz beim LLM-Training verbessert, indem historisch schwierige Stichproben und hochwertige Erfahrungen selektiv wiederverwendet werden. Der Ansatz erreicht durchschnittliche Verbesserungen von 12,5% bei Aufgaben in Mathematik, Planung und visuellem Denken und löst 40,7% der Probleme, bei denen Basismodelle konsistent scheitern. Dies behebt die Datenverschwendungs- und Belohnungshomogenitätsbeschränkungen traditioneller On-Policy-Methoden."
      },
      "es": {
        "title": "El Buffer Importa: Liberando el Poder del Aprendizaje por Refuerzo Fuera de Política en el Razonamiento de Modelos de Lenguaje Grande",
        "summary": "BAPO presenta un marco de aprendizaje por refuerzo fuera de política que mejora la eficiencia de datos en el entrenamiento de LLM reutilizando selectivamente muestras históricamente difíciles y experiencias de alta calidad. El enfoque logra mejoras promedio del 12.5% en tareas de matemáticas, planificación y razonamiento visual mientras resuelve el 40.7% de problemas en los que los modelos base fallan consistentemente. Esto aborda las limitaciones de desperdicio de datos y homogeneidad de recompensas de los métodos tradicionales en política."
      }
    }
  },
  {
    "title": "Modality-Guided Mixture of Graph Experts with Entropy-Triggered Routing for Multimodal Recommendation",
    "slug": "modality-guided-mixture-graph-experts-routing",
    "url": "https://arxiv.org/abs/2602.20723",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "MAGNET addresses multimodal fusion challenges in recommendation systems by using adaptive graph experts with explicit modality roles and entropy-triggered routing that evolves during training. The approach improves performance on sparse and long-tail item recommendations while maintaining interpretability through separate fusion pathways. A two-stage entropy mechanism progressively balances expert utilization throughout training.",
    "content": "arXiv:2602.20723v1 Announce Type: new \nAbstract: Multimodal recommendation enhances ranking by integrating user-item interactions with item content, which is particularly effective under sparse feedback and long-tail distributions. However, multimodal signals are inherently heterogeneous and can conflict in specific contexts, making effective fusion both crucial and challenging. Existing approaches often rely on shared fusion pathways, leading to entangled representations and modality imbalance. To address these issues, we propose \\textbf{MAGNET}, a \\textbf{M}odality-Guided Mixture of \\textbf{A}daptive \\textbf{G}raph Experts \\textbf{N}etwork with Progressive \\textbf{E}ntropy-\\textbf{T}riggered Routing for Multimodal Recommendation, designed to enhance controllability, stability, and interpretability in multimodal fusion. MAGNET couples interaction-conditioned expert routing with structure-aware graph augmentation, so that both \\emph{what} to fuse and \\emph{how} to fuse are explicitly controlled and interpretable. At the representation level, a dual-view graph learning module augments the interaction graph with content-induced edges, improving coverage for sparse and long-tail items while preserving collaborative structure via parallel encoding and lightweight fusion. At the fusion level, MAGNET employs structured experts with explicit modality roles -- dominant, balanced, and complementary -- enabling a more interpretable and adaptive combination of behavioral, visual, and textual cues. To further stabilize sparse routing and prevent expert collapse, we introduce a two-stage entropy-weighting mechanism that monitors routing entropy. This mechanism automatically transitions training from an early coverage-oriented regime to a later specialization-oriented regime, progressively balancing expert utilization and routing confidence. Extensive experiments on public benchmarks demonstrate consistent improvements over strong baselines.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "模态引导的图专家混合与熵触发路由的多模态推荐",
        "summary": "MAGNET通过使用具有显式模态角色和在训练过程中演变的熵触发路由的自适应图专家，解决推荐系统中的多模态融合挑战。该方法改进了稀疏和长尾项目推荐的性能，同时通过独立的融合路径保持可解释性。一个两阶段熵机制在整个训练过程中逐步平衡专家利用。"
      },
      "fr": {
        "title": "Mélange d'Experts Graphiques Guidé par la Modalité avec Routage Déclenché par Entropie pour la Recommandation Multimodale",
        "summary": "MAGNET aborde les défis de fusion multimodale dans les systèmes de recommandation en utilisant des experts graphiques adaptatifs avec des rôles de modalité explicites et un routage déclenché par entropie qui évolue pendant l'entraînement. L'approche améliore les performances sur les recommandations d'éléments rares et à longue traîne tout en maintenant l'interprétabilité grâce à des chemins de fusion séparés. Un mécanisme d'entropie en deux étapes équilibre progressivement l'utilisation des experts tout au long de l'entraînement."
      },
      "de": {
        "title": "Modalitätsgesteuerte Mischung von Graphenexperten mit Entropie-ausgelöstem Routing für multimodale Empfehlungen",
        "summary": "MAGNET befasst sich mit multimodalen Fusionsproblemen in Empfehlungssystemen durch die Verwendung adaptiver Graphenexperten mit expliziten Modalitätsrollen und entropie-ausgelöstem Routing, das sich während des Trainings entwickelt. Der Ansatz verbessert die Leistung bei spärlichen und Long-Tail-Artikelempfehlungen, während die Interpretierbarkeit durch separate Fusionspfade erhalten bleibt. Ein zweistufiger Entropie-Mechanismus gleicht die Expertennutzung während des Trainings progressiv aus."
      },
      "es": {
        "title": "Mezcla de Expertos en Gráficos Guiada por Modalidad con Enrutamiento Activado por Entropía para Recomendación Multimodal",
        "summary": "MAGNET aborda los desafíos de fusión multimodal en sistemas de recomendación utilizando expertos de gráficos adaptativos con roles de modalidad explícitos y enrutamiento activado por entropía que evoluciona durante el entrenamiento. El enfoque mejora el desempeño en recomendaciones de artículos escasos y de cola larga mientras mantiene la interpretabilidad a través de rutas de fusión separadas. Un mecanismo de entropía de dos etapas equilibra progresivamente la utilización de expertos durante todo el entrenamiento."
      }
    }
  },
  {
    "title": "Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback",
    "slug": "balancing-multiple-objectives-urban-traffic-control",
    "url": "https://arxiv.org/abs/2602.20728",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "This work extends reinforcement learning from AI feedback to multi-objective systems where LLMs generate preference labels for balancing conflicting goals without explicit reward engineering. Applied to urban traffic control, the approach produces policies reflecting user priorities while maintaining stable trade-offs. The research demonstrates a scalable path toward user-aligned learning in complex real-world domains.",
    "content": "arXiv:2602.20728v1 Announce Type: new \nAbstract: Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.",
    "category": "ai"
  },
  {
    "title": "CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference",
    "slug": "chess-context-aware-hierarchical-efficient-semantic",
    "url": "https://arxiv.org/abs/2602.20732",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "CHESS optimizes KV-cache management for long-context LLM inference through context-aware hierarchical selection that maintains semantic coherence while reducing memory usage to 1% of standard requirements. The system achieves up to 4.56x throughput improvement through algorithm-system co-design that eliminates expensive data movement. The approach surpasses full KV-cache quality while dramatically reducing latency.",
    "content": "arXiv:2602.20732v1 Announce Type: new \nAbstract: Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \\textbf{CHESS}, an \\textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \\textbf{1\\%} of the KV cache, delivers low-latency stable inference with up to \\textbf{4.56$\\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \\href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.",
    "category": "ai"
  },
  {
    "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
    "slug": "pyvision-rl-open-agentic-vision-models",
    "url": "https://arxiv.org/abs/2602.20739",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "PyVision-RL addresses interaction collapse in multimodal reinforcement learning through an oversampling-filtering-ranking strategy that encourages sustained multi-turn tool use and reasoning. The framework produces open-weight models for image and video understanding with on-demand context construction that reduces visual token usage. Experiments demonstrate significant improvements in agent efficiency and capability for multimodal reasoning tasks.",
    "content": "arXiv:2602.20739v1 Announce Type: new \nAbstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.",
    "category": "ai"
  },
  {
    "title": "Pipeline for Verifying LLM-Generated Mathematical Solutions",
    "slug": "pipeline-verifying-llm-generated-mathematical",
    "url": "https://arxiv.org/abs/2602.20770",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "This work presents an automatic and interactive verification pipeline that more accurately assesses LLM mathematical reasoning compared to answer-only checking. The system uses proof assistants and small language models to formally verify solutions in both formal and informal languages, reducing false positives. The open-source implementation enables more reliable benchmarking of large reasoning models.",
    "content": "arXiv:2602.20770v1 Announce Type: new \nAbstract: With the growing popularity of Large Reasoning Models and their results in solving mathematical problems, it becomes crucial to measure their capabilities. We introduce a pipeline for both automatic and interactive verification as a more accurate alternative to only checking the answer which is currently the most popular approach for benchmarks. The pipeline can also be used as a generator of correct solutions both in formal and informal languages. 3 AI agents, which can be chosen for the benchmark accordingly, are included in the structure. The key idea is the use of prompts to obtain the solution in the specific form which allows for easier verification using proof assistants and possible use of small models ($\\le 8B$). Experiments on several datasets suggest low probability of False Positives. The open-source implementation with instructions on setting up a server is available at https://github.com/LogicEnj/lean4_verification_pipeline.",
    "category": "ai"
  },
  {
    "title": "POMDPPlanners: Open-Source Package for POMDP Planning",
    "slug": "pomdpplanners-open-source-pomdp-planning",
    "url": "https://arxiv.org/abs/2602.20810",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "POMDPPlanners is an open-source Python package facilitating empirical evaluation of partially observable Markov decision process algorithms with automated hyperparameter optimization and parallel simulation. The toolkit includes safety-critical benchmark environments and addresses gaps in decision-making under uncertainty research. It enables scalable, reproducible research particularly useful for risk-sensitive applications.",
    "content": "arXiv:2602.20810v1 Announce Type: new \nAbstract: We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.",
    "category": "ai"
  },
  {
    "title": "Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset",
    "slug": "qwen-bim-llm-bim-design-domain-specific",
    "url": "https://arxiv.org/abs/2602.20812",
    "source": "arXiv cs.AI",
    "date": "2026-02-25T05:00:00.000Z",
    "summary": "Qwen-BIM introduces the first domain-specific LLM for Building Information Modeling design, accompanied by a comprehensive benchmark and dataset for evaluating LLM capabilities in construction automation. Fine-tuned Qwen-BIM achieves 21% average performance improvement and matches general 671B models despite using only 14B parameters. This work establishes a foundation for developing specialized LLMs for architecture and construction industries.",
    "content": "arXiv:2602.20812v1 Announce Type: new \nAbstract: As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with corresponding quantitative indicators to evaluate the performance of LLMs, 2) a method for generating textual data from BIM and constructing corresponding BIM-derived datasets for LLM evaluation and fine-tuning, and 3) a fine-tuning strategy to adapt LLMs for BIM-based design. Results demonstrate that the proposed domain-specific benchmark effectively and comprehensively assesses LLM capabilities, highlighting that general LLMs are still incompetent for domain-specific tasks. Meanwhile, with the proposed benchmark and datasets, Qwen-BIM is developed and achieves a 21.0% average increase in G-Eval score compared to the base LLM model. Notably, with only 14B parameters, performance of Qwen-BIM is comparable to that of general LLMs with 671B parameters for BIM-based design tasks. Overall, this study develops the first domain-specific LLM for BIM-based design by introducing a comprehensive benchmark and high-quality dataset, which provide a solid foundation for developing BIM-related LLMs in various fields.",
    "category": "ai"
  }
]