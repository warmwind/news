[
  {
    "title": "Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation",
    "slug": "coauthor-graphs-rag-scientific-ideas",
    "url": "https://arxiv.org/abs/2602.22215",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "GYWI integrates author knowledge graphs with retrieval-augmented generation to provide LLMs with controllable academic context and traceable inspiration pathways for scientific idea generation. The system significantly outperforms mainstream LLMs in novelty, reliability, and relevance across multiple evaluation dimensions. This approach addresses a key limitation of LLM-generated research ideas by grounding them in established academic relationships and knowledge.",
    "content": "arXiv:2602.22215v1 Announce Type: new \nAbstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.",
    "category": "ai"
  },
  {
    "title": "FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation",
    "slug": "fire-benchmark-financial-intelligence",
    "url": "https://arxiv.org/abs/2602.22273",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "FIRE is a comprehensive benchmark combining theoretical financial knowledge assessment from qualification exams with 3,000 practical scenario questions to evaluate LLMs' capability in financial applications. The benchmark distinguishes between conceptual understanding and real-world decision-making across multiple domains. This systematic evaluation reveals capability boundaries and guides development of more reliable financial reasoning systems.",
    "content": "arXiv:2602.22273v1 Announce Type: new \nAbstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.",
    "category": "ai"
  },
  {
    "title": "Multi-Level Causal Embeddings",
    "slug": "multi-level-causal-embeddings",
    "url": "https://arxiv.org/abs/2602.22287",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "Multi-level causal embeddings framework generalizes model abstraction to enable multiple detailed causal models to be mapped into coarser models while preserving cause-effect relations. The framework addresses practical challenges in dataset merging and statistical inference across models with different representations. This work enables more flexible composition of causal models from heterogeneous sources.",
    "content": "arXiv:2602.22287v1 Announce Type: new \nAbstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.",
    "category": "ai"
  },
  {
    "title": "Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents",
    "slug": "agent-behavioral-contracts-runtime",
    "url": "https://arxiv.org/abs/2602.22302",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "Agent Behavioral Contracts brings formal Design-by-Contract principles to AI agents through specifications of preconditions, invariants, governance policies, and recovery mechanisms as runtime-enforceable components. Theoretical analysis proves that contracts with sufficient recovery rates bound behavioral drift to predictable levels across extended sessions. Empirical evaluation on 1,980 sessions demonstrates the framework detects multiple violations per session that uncontracted baselines completely miss.",
    "content": "arXiv:2602.22302v1 Announce Type: new \nAbstract: Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and frequent project failures in agentic AI deployments. We introduce Agent Behavioral Contracts (ABC), a formal framework that brings Design-by-Contract principles to autonomous AI agents. An ABC contract C = (P, I, G, R) specifies Preconditions, Invariants, Governance policies, and Recovery mechanisms as first-class, runtime-enforceable components. We define (p, delta, k)-satisfaction -- a probabilistic notion of contract compliance that accounts for LLM non-determinism and recovery -- and prove a Drift Bounds Theorem showing that contracts with recovery rate gamma > alpha (the natural drift rate) bound behavioral drift to D* = alpha/gamma in expectation, with Gaussian concentration in the stochastic setting. We establish sufficient conditions for safe contract composition in multi-agent chains and derive probabilistic degradation bounds. We implement ABC in AgentAssert, a runtime enforcement library, and evaluate on AgentContract-Bench, a benchmark of 200 scenarios across 7 models from 6 vendors. Results across 1,980 sessions show that contracted agents detect 5.2-6.8 soft violations per session that uncontracted baselines miss entirely (p < 0.0001, Cohen's d = 6.7-33.8), achieve 88-100% hard constraint compliance, and bound behavioral drift to D* < 0.27 across extended sessions, with 100% recovery for frontier models and 17-100% across all models, at overhead < 10 ms per action.",
    "category": "ai"
  },
  {
    "title": "Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?",
    "slug": "vibe-researching-ai-agents-science",
    "url": "https://arxiv.org/abs/2602.22401",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "This paper introduces \"vibe researching\" as the AI-era parallel to vibe coding, analyzing how agents with specialized research skills can accelerate entire research pipelines from idea to submission. The author identifies a cognitive delegation boundary that cuts through every research stage rather than between stages, where AI excels at speed and coverage but struggles with theoretical originality. The work outlines implications including augmentation risks, stratification, and pedagogical challenges.",
    "content": "arXiv:2602.22401v1 Announce Type: new \nAbstract: AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, query databases, search the web, and invoke domain-specific skills to execute entire research pipelines autonomously. This paper introduces the concept of vibe researching -- the AI-era parallel to ``vibe coding'' (Karpathy, 2025) -- and uses scholar-skill, a 21-skill plugin for Claude Code covering the full research pipeline from idea to submission, as an illustrative case. I develop a cognitive task framework that classifies research activities along two dimensions -- codifiability and tacit knowledge requirement -- to identify a delegation boundary that is cognitive, not sequential: it cuts through every stage of the research pipeline, not between stages. I argue that AI agents excel at speed, coverage, and methodological scaffolding but struggle with theoretical originality and tacit field knowledge. The paper concludes with an analysis of three implications for the profession -- augmentation with fragile conditions, stratification risk, and a pedagogical crisis -- and proposes five principles for responsible vibe researching.",
    "category": "ai"
  },
  {
    "title": "Towards Autonomous Memory Agents",
    "slug": "autonomous-memory-agents",
    "url": "https://arxiv.org/abs/2602.22406",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "U-Mem proposes autonomous memory agents that actively acquire, validate, and curate knowledge through cost-aware extraction cascades and semantic-aware Thompson sampling for exploration-exploitation trade-offs. The approach improves on verifiable benchmarks by 7-33 points while significantly reducing human feedback requirements. This work moves memory agent systems from passive reaction to active learning strategies.",
    "content": "arXiv:2602.22406v1 Announce Type: new \nAbstract: Recent memory agents improve LLMs by extracting experiences and conversation history into an external storage. This enables low-overhead context assembly and online memory update without expensive LLM training. However, existing solutions remain passive and reactive; memory growth is bounded by information that happens to be available, while memory agents seldom seek external inputs in uncertainties. We propose autonomous memory agents that actively acquire, validate, and curate knowledge at a minimum cost. U-Mem materializes this idea via (i) a cost-aware knowledge-extraction cascade that escalates from cheap self/teacher signals to tool-verified research and, only when needed, expert feedback, and (ii) semantic-aware Thompson sampling to balance exploration and exploitation over memories and mitigate cold-start bias. On both verifiable and non-verifiable benchmarks, U-Mem consistently beats prior memory baselines and can surpass RL-based optimization, improving HotpotQA (Qwen2.5-7B) by 14.6 points and AIME25 (Gemini-2.5-flash) by 7.33 points.",
    "category": "ai"
  },
  {
    "title": "Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus",
    "slug": "cognitive-abstraction-reasoning-corpus",
    "url": "https://arxiv.org/abs/2602.22408",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "The Cognitive Abstraction and Reasoning Corpus presents a human-adapted dataset of visual reasoning problems with high-resolution behavioral tracking from 260 participants, revealing success rates of 80-90% and diverse problem-solving strategies. Analysis shows harder problems elicit longer deliberation and greater strategy divergence, while even incorrect solutions often converge despite different trajectories. This dataset provides rich environment for studying how humans generalize and adapt under uncertainty.",
    "content": "arXiv:2602.22408v1 Announce Type: new \nAbstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "探索认知抽象推理语料库中的人类行为：抽象规则推理和问题解决",
        "summary": "认知抽象推理语料库呈现了人类适配的视觉推理问题数据集，包含来自260名参与者的高分辨率行为跟踪，显示80-90%的成功率和多样化的问题解决策略。分析表明更困难的问题会导致更长的深思熟虑和更大的策略差异，而即使是错误的解决方案也往往会汇聚，尽管轨迹不同。该数据集为研究人类如何在不确定性下泛化和适应提供了丰富的环境。"
      },
      "fr": {
        "title": "Explorer le comportement humain lors de l'inférence de règles abstraites et de la résolution de problèmes avec le corpus d'abstraction cognitive et de raisonnement",
        "summary": "Le Cognitive Abstraction and Reasoning Corpus présente un ensemble de données de problèmes de raisonnement visuel adapté à l'homme avec un suivi comportemental haute résolution de 260 participants, révélant des taux de réussite de 80-90% et des stratégies diverses de résolution de problèmes. L'analyse montre que les problèmes plus difficiles provoquent une plus longue délibération et une plus grande divergence de stratégie, tandis que même les solutions incorrectes convergent souvent malgré des trajectoires différentes. Cet ensemble de données fournit un environnement riche pour étudier comment les humains généralisent et s'adaptent face à l'incertitude."
      },
      "de": {
        "title": "Erforschung des menschlichen Verhaltens bei abstrakten Regelinferenzen und Problemlösung mit dem Cognitive Abstraction and Reasoning Corpus",
        "summary": "Das Cognitive Abstraction and Reasoning Corpus präsentiert einen menschengerechten Datensatz visueller Reasoning-Probleme mit hochauflösender Verhaltenserfassung von 260 Teilnehmern und zeigt Erfolgsquoten von 80-90% sowie verschiedenartige Problemlösungsstrategien. Die Analyse zeigt, dass schwierigere Probleme zu längerer Überlegung und größerer Strategiedivergenz führen, während selbst falsche Lösungen trotz unterschiedlicher Trajektorien oft konvergieren. Dieser Datensatz bietet eine reichhaltige Umgebung zum Studium, wie Menschen unter Unsicherheit verallgemeinern und sich anpassen."
      },
      "es": {
        "title": "Explorando el comportamiento humano durante la inferencia de reglas abstractas y la resolución de problemas con el Corpus de Abstracción Cognitiva y Razonamiento",
        "summary": "El Corpus de Abstracción Cognitiva y Razonamiento presenta un conjunto de datos adaptado para humanos de problemas de razonamiento visual con seguimiento de comportamiento de alta resolución de 260 participantes, revelando tasas de éxito del 80-90% y diversas estrategias de resolución de problemas. El análisis muestra que los problemas más difíciles provocan una deliberación más prolongada y una mayor divergencia de estrategia, mientras que incluso las soluciones incorrectas a menudo convergen a pesar de trayectorias diferentes. Este conjunto de datos proporciona un entorno rico para estudiar cómo los humanos generalizan y se adaptan bajo incertidumbre."
      }
    }
  },
  {
    "title": "Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents",
    "slug": "confidence-calibrated-agents-jury",
    "url": "https://arxiv.org/abs/2602.22413",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "The paper proposes a probabilistic framework for collective decision-making where agents calibrate confidence and selectively abstain from voting, generalizing classical jury theorems to sequential confidence-gated settings. Theoretical analysis provides non-asymptotic bounds on group success probability and shows how selective participation can mitigate hallucinations in collective LLM decision-making. Empirical validation through simulations supports the theoretical framework.",
    "content": "arXiv:2602.22413v1 Announce Type: new \nAbstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \\textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \\textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \\textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \\textit{hallucinations} in collective LLM decision-making.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "认识论过滤和集体幻觉：置信度校准代理的陪审团定理",
        "summary": "该论文提出了一个概率框架用于集体决策，其中代理校准置信度并选择性地弃权投票，将经典陪审团定理推广到顺序置信度门控设置。理论分析提供了关于群体成功概率的非渐近界，并展示了选择性参与如何减轻集体LLM决策中的幻觉。模拟的经验验证支持该理论框架。"
      },
      "fr": {
        "title": "Filtrage épistémique et hallucination collective : un théorème du jury pour les agents calibrés en confiance",
        "summary": "L'article propose un cadre probabiliste pour la prise de décision collective où les agents calibrent la confiance et s'abstiennent sélectivement de voter, généralisant les théorèmes classiques du jury à des paramètres séquentiels contrôlés par la confiance. L'analyse théorique fournit des bornes non-asymptotiques sur la probabilité de succès du groupe et montre comment la participation sélective peut atténuer les hallucinations dans la prise de décision collective des LLM. La validation empirique par simulations soutient le cadre théorique."
      },
      "de": {
        "title": "Epistemische Filterung und kollektive Halluzination: Ein Jury-Theorem für vertrauensgeeichte Agenten",
        "summary": "Das Papier schlägt einen probabilistischen Rahmen für kollektive Entscheidungsfindung vor, bei dem Agenten das Vertrauen kalibrieren und selektiv von der Abstimmung absehen, was klassische Jury-Theoreme auf sequenzielle vertrauensbegrenzte Einstellungen verallgemeinert. Die theoretische Analyse liefert nicht-asymptotische Grenzen für die Erfolgswahrscheinlichkeit der Gruppe und zeigt, wie selektive Beteiligung Halluzinationen in der kollektiven LLM-Entscheidungsfindung mindern kann. Empirische Validierung durch Simulationen unterstützt den theoretischen Rahmen."
      },
      "es": {
        "title": "Filtrado epistémico y alucinación colectiva: un teorema del jurado para agentes calibrados en confianza",
        "summary": "El artículo propone un marco probabilístico para la toma de decisiones colectivas donde los agentes calibran la confianza y se abstienen selectivamente de votar, generalizando los teoremas clásicos del jurado a configuraciones secuenciales controladas por confianza. El análisis teórico proporciona límites no asintóticos sobre la probabilidad de éxito del grupo y muestra cómo la participación selectiva puede mitigar las alucinaciones en la toma de decisiones colectiva de LLM. La validación empírica mediante simulaciones respalda el marco teórico."
      }
    }
  },
  {
    "title": "ArchAgent: Agentic AI-driven Computer Architecture Discovery",
    "slug": "archagent-computer-architecture-discovery",
    "url": "https://arxiv.org/abs/2602.22425",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "ArchAgent automatically designs computer architecture policies like cache replacement algorithms, achieving a 5.3% IPC improvement over prior state-of-the-art in just two days without human intervention. The system discovers these improvements 3-5x faster than human-designed policies and reveals \"simulator escapes\" where the agent exploits tool limitations. This work demonstrates agile hardware design acceleration and raises considerations for research tools designed for human operation.",
    "content": "arXiv:2602.22425v1 Announce Type: new \nAbstract: Agile hardware design flows are a critically needed force multiplier to meet the exploding demand for compute. Recently, agentic generative AI systems have demonstrated significant advances in algorithm design, improving code efficiency, and enabling discovery across scientific domains.\n  Bridging these worlds, we present ArchAgent, an automated computer architecture discovery system built on AlphaEvolve. We show ArchAgent's ability to automatically design/implement state-of-the-art (SoTA) cache replacement policies (architecting new mechanisms/logic, not only changing parameters), broadly within the confines of an established cache replacement policy design competition.\n  In two days without human intervention, ArchAgent generated a policy achieving a 5.3% IPC speedup improvement over the prior SoTA on public multi-core Google Workload Traces. On the heavily-explored single-core SPEC06 workloads, it generated a policy in just 18 days showing a 0.9% IPC speedup improvement over the existing SoTA (a similar \"winning margin\" as reported by the existing SoTA). ArchAgent achieved these gains 3-5x faster than prior human-developed SoTA policies.\n  Agentic flows also enable \"post-silicon hyperspecialization\" where agents tune runtime-configurable parameters exposed in hardware policies to further align the policies with a specific workload (mix). Exploiting this, we demonstrate a 2.4% IPC speedup improvement over prior SoTA on SPEC06 workloads.\n  Finally, we outline broader implications for computer architecture research in the era of agentic AI. For example, we demonstrate the phenomenon of \"simulator escapes\", where the agentic AI flow discovered and exploited a loophole in a popular microarchitectural simulator - a consequence of the fact that these research tools were designed for a (now past) world where they were exclusively operated by humans acting in good-faith.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "ArchAgent：代理AI驱动的计算机架构发现",
        "summary": "ArchAgent自动设计计算机架构策略，如缓存替换算法，在仅两天内实现了5.3%的IPC改进，超过了之前的最先进水平，无需人工干预。该系统发现这些改进的速度比人类设计的策略快3-5倍，并揭示了\"模拟器逃逸\"，其中代理利用工具限制。这项工作展示了敏捷硬件设计加速，并对为人类操作设计的研究工具提出了考虑。"
      },
      "fr": {
        "title": "ArchAgent : Découverte d'architecture informatique guidée par l'IA agentive",
        "summary": "ArchAgent conçoit automatiquement des politiques d'architecture informatique comme des algorithmes de remplacement de cache, réalisant une amélioration de 5,3% d'IPC par rapport à l'état de l'art antérieur en seulement deux jours sans intervention humaine. Le système découvre ces améliorations 3 à 5 fois plus rapidement que les politiques conçues par l'homme et révèle des \"fuites de simulateur\" où l'agent exploite les limitations des outils. Ces travaux démontrent l'accélération agile de la conception du matériel et soulèvent des considérations pour les outils de recherche conçus pour l'opération humaine."
      },
      "de": {
        "title": "ArchAgent: Von KI-gesteuerte Entdeckung von Computerarchitektur",
        "summary": "ArchAgent entwirft automatisch Richtlinien für Computerarchitektur wie Algorithmen für Cache-Ersatz und erreicht eine IPC-Verbesserung von 5,3% gegenüber dem bisherigen Stand der Technik in nur zwei Tagen ohne menschliche Eingriffe. Das System entdeckt diese Verbesserungen 3-5 mal schneller als von Menschen entworfene Richtlinien und offenbart \"Simulator-Eskapaden\", bei denen der Agent Werkzeuglimitierungen ausnutzt. Diese Arbeit demonstriert beschleunigte agile Hardwaredesign und wirft Überlegungen zu Forschungswerkzeugen für menschliche Bedienung auf."
      },
      "es": {
        "title": "ArchAgent: Descubrimiento de arquitectura de computadora impulsado por IA agentiva",
        "summary": "ArchAgent diseña automáticamente políticas de arquitectura de computadora como algoritmos de reemplazo de caché, logrando una mejora del 5,3% en IPC sobre el estado del arte anterior en solo dos días sin intervención humana. El sistema descubre estas mejoras 3-5 veces más rápido que las políticas diseñadas por humanos y revela \"escapes de simulador\" donde el agente explota limitaciones de herramientas. Este trabajo demuestra la aceleración ágil del diseño de hardware y plantea consideraciones para herramientas de investigación diseñadas para operación humana."
      }
    }
  },
  {
    "title": "How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?",
    "slug": "latent-reasoning-weak-strong-supervision",
    "url": "https://arxiv.org/abs/2602.22441",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "Analysis of latent reasoning methods reveals pervasive shortcut behavior where models achieve high accuracy without using latent reasoning, and finds reasoning processes implement implicit pruning rather than structured search. The study identifies a fundamental trade-off: stronger supervision reduces shortcuts but restricts diverse hypothesis maintenance, while weaker supervision enables richer representations at higher shortcut cost. These findings clarify internal mechanisms and limitations of latent reasoning.",
    "content": "arXiv:2602.22441v1 Announce Type: new \nAbstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.",
    "category": "ai"
  },
  {
    "title": "A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines",
    "slug": "ai-agent-decisions-automl-assessment",
    "url": "https://arxiv.org/abs/2602.22442",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "The paper proposes an Evaluation Agent that performs decision-centric assessment of AutoML agents by evaluating intermediate decisions across validity, reasoning consistency, model quality risks, and counterfactual impact. The EA detects faulty decisions with 91.9% F1 score and reveals decision impacts ranging from -4.9% to +8.3% on final metrics, exposing failure modes invisible to outcome-only evaluation. This framework provides foundation for reliable and interpretable autonomous ML systems.",
    "content": "arXiv:2602.22442v1 Announce Type: new \nAbstract: Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find that none of the surveyed agentic AutoML systems report structured, decision-level evaluation metrics intended for post-hoc assessment of intermediate decision quality. To address this limitation, we propose an Evaluation Agent (EA) that performs decision-centric assessment of AutoML agents without interfering with their execution. The EA is designed as an observer that evaluates intermediate decisions along four dimensions: decision validity, reasoning consistency, model quality risks beyond accuracy, and counterfactual decision impact. Across four proof-of-concept experiments, we demonstrate that the EA can (i) detect faulty decisions with an F1 score of 0.919, (ii) identify reasoning inconsistencies independent of final outcomes, and (iii) attribute downstream performance changes to agent decisions, revealing impacts ranging from -4.9\\% to +8.3\\% in final metrics. These results illustrate how decision-centric evaluation exposes failure modes that are invisible to outcome-only metrics. Our work reframes the evaluation of agentic AutoML systems from an outcome-based perspective to one that audits agent decisions, offering a foundation for reliable, interpretable, and governable autonomous ML systems.",
    "category": "ai"
  },
  {
    "title": "CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines",
    "slug": "contrastive-world-models-action-feasibility",
    "url": "https://arxiv.org/abs/2602.22452",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "CWM uses contrastive learning with hard-mined negative examples to train LLM action scorers that better discriminate between physically feasible and subtly incompatible actions in embodied agent pipelines. The approach outperforms supervised fine-tuning by 6.76 percentage points on minimal-edit negative cases and maintains significantly better safety margins under out-of-distribution conditions. This work addresses the critical action feasibility bottleneck in embodied agent systems.",
    "content": "arXiv:2602.22452v1 Announce Type: new \nAbstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.",
    "category": "ai"
  },
  {
    "title": "ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization",
    "slug": "constraintbench-llm-optimization",
    "url": "https://arxiv.org/abs/2602.22465",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "ConstraintBench evaluates six frontier LLMs on 200 constrained optimization tasks across 10 operations research domains, finding that feasibility rather than optimality is the primary bottleneck. The best model achieves only 65% constraint satisfaction and 30.5% joint feasibility-optimality, with large variation across domains ranging from 83.3% to 0.8% average feasibility. The benchmark reveals systematic failure modes including entity hallucination and domain-specific performance disparities.",
    "content": "arXiv:2602.22465v1 Announce Type: new \nAbstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "ConstraintBench：直接优化中的大语言模型约束推理基准测试",
        "summary": "ConstraintBench在10个运筹学领域的200个受限优化任务上评估了六个前沿大语言模型，发现可行性而非最优性是主要瓶颈。最佳模型仅实现65%的约束满足度和30.5%的联合可行性-最优性，在各领域间差异很大，平均可行性从83.3%到0.8%不等。该基准揭示了系统性失效模式，包括实体幻觉和特定领域性能差异。"
      },
      "fr": {
        "title": "ConstraintBench : Benchmarking du raisonnement des contraintes LLM sur l'optimisation directe",
        "summary": "ConstraintBench évalue six LLMs de pointe sur 200 tâches d'optimisation contrainte dans 10 domaines de recherche opérationnelle, constatant que la faisabilité plutôt que l'optimalité est le goulot d'étranglement principal. Le meilleur modèle n'atteint que 65% de satisfaction des contraintes et 30,5% de faisabilité-optimalité conjointe, avec une grande variation entre les domaines allant de 83,3% à 0,8% de faisabilité moyenne. Le benchmark révèle des modes de défaillance systématiques incluant des hallucinations d'entités et des disparités de performance spécifiques au domaine."
      },
      "de": {
        "title": "ConstraintBench: Benchmarking der LLM-Constraint-Reasoning bei direkter Optimierung",
        "summary": "ConstraintBench bewertet sechs führende LLMs bei 200 eingeschränkten Optimierungsaufgaben in 10 Bereichen der Betriebsforschung und stellt fest, dass Machbarkeit statt Optimalität der Hauptengpass ist. Das beste Modell erreicht nur 65% Constraint-Erfüllung und 30,5% gemeinsame Machbarkeits-Optimalität, mit großen Unterschieden zwischen Domänen, die von 83,3% bis 0,8% durchschnittlicher Machbarkeit reichen. Der Benchmark offenbart systematische Ausfallmuster einschließlich Entity-Halluzinationen und domänenspezifischer Leistungsunterschiede."
      },
      "es": {
        "title": "ConstraintBench: Evaluación de razonamiento de restricciones LLM en optimización directa",
        "summary": "ConstraintBench evalúa seis LLMs líderes en 200 tareas de optimización restringida en 10 dominios de investigación operativa, encontrando que la viabilidad es más importante que la optimalidad como cuello de botella principal. El mejor modelo logra solo el 65% de satisfacción de restricciones y el 30,5% de viabilidad-optimalidad conjunta, con gran variación entre dominios que van desde 83,3% hasta 0,8% de viabilidad promedio. El benchmark revela modos de falla sistemáticos que incluyen alucinación de entidades y disparidades de desempeño específicas del dominio."
      }
    }
  },
  {
    "title": "VeRO: An Evaluation Harness for Agents to Optimize Agents",
    "slug": "vero-agent-optimization-harness",
    "url": "https://arxiv.org/abs/2602.22480",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "VeRO introduces an evaluation framework for agent optimization that provides reproducible harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces alongside benchmark tasks. The framework enables systematic study of how coding agents improve target agents through edit-execute-evaluate cycles, accounting for both deterministic code and stochastic LLM outputs. This infrastructure supports research on agent optimization as core capability.",
    "content": "arXiv:2602.22480v1 Announce Type: new \nAbstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "VeRO：代理优化代理的评估框架",
        "summary": "VeRO引入了一个代理优化的评估框架，提供具有版本控制的代理快照、预算受控的评估和结构化执行跟踪以及基准任务。该框架使通过编辑-执行-评估周期进行系统研究成为可能，以了解编码代理如何改进目标代理，同时考虑确定性代码和随机大语言模型输出。这个基础设施支持将代理优化作为核心能力的研究。"
      },
      "fr": {
        "title": "VeRO : Un harnais d'évaluation pour les agents afin d'optimiser les agents",
        "summary": "VeRO introduit un cadre d'évaluation pour l'optimisation des agents qui fournit un harnais reproductible avec des instantanés d'agents versionnés, une évaluation contrôlée par budget et des traces d'exécution structurées aux côtés des tâches de référence. Le cadre permet l'étude systématique de la façon dont les agents de codage améliorent les agents cibles par le biais de cycles édition-exécution-évaluation, en tenant compte à la fois du code déterministe et des résultats stochastiques des LLM. Cette infrastructure soutient la recherche sur l'optimisation des agents en tant que capacité principale."
      },
      "de": {
        "title": "VeRO: Ein Evaluierungs-Harness für Agenten zur Optimierung von Agenten",
        "summary": "VeRO führt ein Evaluierungs-Framework für die Agent-Optimierung ein, das einen reproduzierbaren Harness mit versionierten Agent-Snapshots, budgetkontrollierte Evaluierung und strukturierte Ausführungs-Traces neben Benchmark-Aufgaben bietet. Das Framework ermöglicht das systematische Studium, wie Code-Agenten Zielagenten durch Edit-Execute-Evaluate-Zyklen verbessern, wobei sowohl deterministischer Code als auch stochastische LLM-Ausgaben berücksichtigt werden. Diese Infrastruktur unterstützt die Forschung zur Agentenoptimierung als Kernfähigkeit."
      },
      "es": {
        "title": "VeRO: Un arnés de evaluación para que los agentes optimicen agentes",
        "summary": "VeRO introduce un marco de evaluación para la optimización de agentes que proporciona un arnés reproducible con snapshots de agentes versionados, evaluación controlada por presupuesto y trazas de ejecución estructuradas junto con tareas de referencia. El marco permite el estudio sistemático de cómo los agentes de codificación mejoran los agentes objetivo a través de ciclos editar-ejecutar-evaluar, considerando tanto código determinista como salidas estocásticas de LLM. Esta infraestructura apoya la investigación sobre optimización de agentes como capacidad central."
      }
    }
  },
  {
    "title": "Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models",
    "slug": "ai-life-cycle-assessment-landscape",
    "url": "https://arxiv.org/abs/2602.22500",
    "source": "arXiv cs.AI",
    "date": "2026-02-27T05:00:00.000Z",
    "summary": "A large-scale review of AI-LCA research leveraging LLM text-mining reveals dramatic growth in AI adoption, statistically significant correlations between AI approaches and LCA stages, and shift toward LLM-driven methods. The work demonstrates that LLM-assisted methodologies support reproducible large-scale reviews while capturing both high-level trends and nuanced conceptual patterns. These findings inform computationally-efficient LCA development and help practitioners incorporate state-of-the-art AI tools into sustainability assessments.",
    "content": "arXiv:2602.22500v1 Announce Type: new \nAbstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "使用大语言模型绘制人工智能在生命周期评估中的景观",
        "summary": "对AI-LCA研究的大规模综述利用大语言模型文本挖掘显示AI采用的显著增长，AI方法与LCA阶段之间的统计显著相关性，以及向LLM驱动方法的转变。该工作表明大语言模型辅助的方法论支持可重现的大规模综述，同时捕获高层次趋势和细微的概念模式。这些发现为计算高效的LCA开发提供信息，并帮助实践者将最先进的AI工具纳入可持续性评估。"
      },
      "fr": {
        "title": "Cartographier le paysage de l'intelligence artificielle dans l'évaluation du cycle de vie à l'aide de grands modèles de langage",
        "summary": "Un examen à grande échelle de la recherche AI-LCA exploitant l'exploration de texte LLM révèle une croissance spectaculaire de l'adoption de l'IA, des corrélations statistiquement significatives entre les approches d'IA et les étapes de l'ECV, et un passage vers des méthodes pilotées par les LLM. Le travail démontre que les méthodologies assistées par LLM soutiennent les examens reproductibles à grande échelle tout en capturant à la fois les tendances de haut niveau et les modèles conceptuels nuancés. Ces résultats informent le développement efficace en termes de calcul de l'ECV et aident les praticiens à intégrer des outils d'IA à la pointe de la technologie dans les évaluations de durabilité."
      },
      "de": {
        "title": "Kartographierung der Landschaft der künstlichen Intelligenz in der Lebenszyklusanalyse mit großen Sprachmodellen",
        "summary": "Eine umfangreiche Überprüfung der AI-LCA-Forschung unter Nutzung von LLM-Text-Mining zeigt dramatisches Wachstum in der KI-Adoption, statistisch signifikante Korrelationen zwischen KI-Ansätzen und LCA-Phasen sowie eine Verschiebung hin zu LLM-gesteuerten Methoden. Die Arbeit zeigt, dass LLM-gestützte Methoden großflächige reproduzierbare Überprüfungen unterstützen und gleichzeitig sowohl übergreifende Trends als auch differenzierte konzeptionelle Muster erfassen. Diese Erkenntnisse informieren die rechnerisch effiziente LCA-Entwicklung und helfen Praktikern, hochmoderne KI-Tools in Nachhaltigkeitsbewertungen zu integrieren."
      },
      "es": {
        "title": "Cartografiar el panorama de la inteligencia artificial en la evaluación del ciclo de vida utilizando grandes modelos de lenguaje",
        "summary": "Una revisión a gran escala de la investigación AI-LCA aprovechando la minería de texto LLM revela un crecimiento dramático en la adopción de IA, correlaciones estadísticamente significativas entre enfoques de IA y etapas de ACV, y cambio hacia métodos impulsados por LLM. El trabajo demuestra que las metodologías asistidas por LLM apoyan revisiones reproducibles a gran escala mientras capturan tendencias de alto nivel y patrones conceptuales matizados. Estos hallazgos informan el desarrollo computacionalmente eficiente de ACV y ayudan a los profesionales a incorporar herramientas de IA de última generación en evaluaciones de sostenibilidad."
      }
    }
  }
]