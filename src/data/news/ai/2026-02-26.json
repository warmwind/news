[
  {
    "title": "A Dynamic Survey of Soft Set Theory and Its Extensions",
    "slug": "a-dynamic-survey-of-soft-set-theory-and-its-extensions",
    "url": "https://arxiv.org/abs/2602.21268",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "This article presents a comprehensive survey of soft set theory and its extensions, which provides a framework for structured uncertainty modeling in decision-making through parameterized attributes. The survey covers major variants like hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets, along with their connections to topology and matroid theory. Understanding soft set theory is important for fields requiring structured handling of uncertainty and decision modeling.",
    "content": "arXiv:2602.21268v1 Announce Type: new \nAbstract: Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "软集合论的动态调查及其扩展",
        "summary": "本文呈现了软集合论及其扩展的综合调查,该理论通过参数化属性为决策制定中的结构化不确定性建模提供了框架。该调查覆盖了主要变体,如超软集、超超软集、树软集、双极软集和动态软集,以及它们与拓扑和拟阵理论的联系。理解软集合论对于需要结构化处理不确定性和决策建模的领域很重要。"
      },
      "fr": {
        "title": "Une enquête dynamique de la théorie des ensembles mous et de ses extensions",
        "summary": "Cet article présente une enquête complète de la théorie des ensembles mous et de ses extensions, qui fournit un cadre pour la modélisation structurée de l'incertitude dans la prise de décision à travers des attributs paramétrés. L'enquête couvre les principales variantes telles que les ensembles hypermous, les ensembles superhypermous, les ensembles TreeSoft, les ensembles mous bipolaires et les ensembles mous dynamiques, ainsi que leurs connexions à la topologie et à la théorie des matroïdes. Comprendre la théorie des ensembles mous est important pour les domaines nécessitant une gestion structurée de l'incertitude et de la modélisation des décisions."
      },
      "de": {
        "title": "Eine dynamische Übersicht über Softmengentheorie und ihre Erweiterungen",
        "summary": "Dieser Artikel präsentiert eine umfassende Übersicht der Softmengentheorie und ihrer Erweiterungen, die einen Rahmen für strukturierte Unsicherheitsmodellierung bei der Entscheidungsfindung durch parametrisierte Attribute bietet. Die Übersicht behandelt Hauptvarianten wie Hypersoftmengen, Superhypersoftmengen, TreeSoft-Mengen, bipolare Softmengen und dynamische Softmengen sowie ihre Verbindungen zur Topologie und Matroidtheorie. Das Verständnis der Softmengentheorie ist wichtig für Bereiche, die eine strukturierte Behandlung von Unsicherheit und Entscheidungsmodellierung erfordern."
      },
      "es": {
        "title": "Una encuesta dinámica de la teoría de conjuntos blandos y sus extensiones",
        "summary": "Este artículo presenta una encuesta completa de la teoría de conjuntos blandos y sus extensiones, que proporciona un marco para el modelado estructurado de la incertidumbre en la toma de decisiones a través de atributos parametrizados. La encuesta cubre variantes principales como conjuntos hipersuaves, conjuntos superhipersuaves, conjuntos TreeSoft, conjuntos suaves bipolares y conjuntos suaves dinámicos, así como sus conexiones a la topología y la teoría de matroides. Entender la teoría de conjuntos blandos es importante para los campos que requieren un manejo estructurado de la incertidumbre y modelado de decisiones."
      }
    }
  },
  {
    "title": "A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives",
    "slug": "hierarchical-multi-agent-system-for-autonomous-discovery-geoscientific-data",
    "url": "https://arxiv.org/abs/2602.21351",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "Researchers present PANGAEA-GPT, a hierarchical multi-agent framework that enables autonomous data discovery and analysis in Earth science repositories through structured routing, error correction, and multi-step workflows. The system addresses the challenge of underutilized Earth science datasets by using coordinated agent workflows to execute complex analyses with minimal human intervention. This framework provides a practical methodology for improving data reusability in scientific archives.",
    "content": "arXiv:2602.21351v1 Announce Type: new \nAbstract: The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "用于地球科学数据档案自主发现的分层多代理系统",
        "summary": "研究人员展示了PANGAEA-GPT,这是一个分层多代理框架,通过结构化路由、错误纠正和多步工作流在地球科学存储库中实现自主数据发现和分析。该系统通过使用协调的代理工作流来执行复杂分析,从而解决了地球科学数据集未充分利用的问题,只需要最少的人为干预。该框架为改进科学档案中的数据可重用性提供了实用方法。"
      },
      "fr": {
        "title": "Un système multi-agents hiérarchique pour la découverte autonome dans les archives de données géoscientifiques",
        "summary": "Les chercheurs présentent PANGAEA-GPT, un cadre multi-agents hiérarchique qui permet la découverte et l'analyse autonome des données dans les référentiels des sciences de la Terre grâce à l'acheminement structuré, à la correction des erreurs et aux flux de travail en plusieurs étapes. Le système aborde le défi des ensembles de données en sciences de la Terre sous-utilisés en utilisant des flux de travail d'agents coordonnés pour exécuter des analyses complexes avec une intervention humaine minimale. Ce cadre fournit une méthodologie pratique pour améliorer la réutilisabilité des données dans les archives scientifiques."
      },
      "de": {
        "title": "Ein hierarchisches Multi-Agent-System für autonome Entdeckung in geowissenschaftlichen Datenarchiven",
        "summary": "Forscher präsentieren PANGAEA-GPT, ein hierarchisches Multi-Agent-Framework, das durch strukturiertes Routing, Fehlerkorrektur und mehrstufige Arbeitsabläufe autonome Datenentdeckung und -analyse in Repositorien der Geowissenschaften ermöglicht. Das System adressiert die Herausforderung unterausgelasteter Erddatensätze durch die Verwendung koordinierter Agent-Workflows zur Ausführung komplexer Analysen mit minimaler menschlicher Intervention. Dieses Framework bietet eine praktische Methodik zur Verbesserung der Datenwiederverwertbarkeit in wissenschaftlichen Archiven."
      },
      "es": {
        "title": "Un sistema jerárquico multiagente para el descubrimiento autónomo en archivos de datos geocientíficos",
        "summary": "Los investigadores presentan PANGAEA-GPT, un marco multiagente jerárquico que permite el descubrimiento y análisis autónomo de datos en repositorios de ciencias de la Tierra a través del enrutamiento estructurado, la corrección de errores y flujos de trabajo de múltiples pasos. El sistema aborda el desafío de los conjuntos de datos de ciencias de la Tierra subutilizados utilizando flujos de trabajo de agentes coordinados para ejecutar análisis complejos con una intervención humana mínima. Este marco proporciona una metodología práctica para mejorar la reutilización de datos en archivos científicos."
      }
    }
  },
  {
    "title": "Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information",
    "slug": "beyond-refusal-probing-limits-agentic-self-correction-semantic-sensitive-information",
    "url": "https://arxiv.org/abs/2602.21496",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "This paper introduces SemSIEdit, an inference-time framework that uses agentic editing to reduce sensitive information leakage from language models while preserving narrative quality. Researchers found that the approach reduces privacy risks by 34.6% across multiple categories of sensitive information while maintaining utility. These findings reveal important trade-offs between privacy and model utility that should inform AI safety practices.",
    "content": "arXiv:2602.21496v1 Announce Type: new \nAbstract: While defenses for structured PII are mature, Large Language Models (LLMs) pose a new threat: Semantic Sensitive Information (SemSI), where models infer sensitive identity attributes, generate reputation-harmful content, or hallucinate potentially wrong information. The capacity of LLMs to self-regulate these complex, context-dependent sensitive information leaks without destroying utility remains an open scientific question. To address this, we introduce SemSIEdit, an inference-time framework where an agentic \"Editor\" iteratively critiques and rewrites sensitive spans to preserve narrative flow rather than simply refusing to answer. Our analysis reveals a Privacy-Utility Pareto Frontier, where this agentic rewriting reduces leakage by 34.6% across all three SemSI categories while incurring a marginal utility loss of 9.8%. We also uncover a Scale-Dependent Safety Divergence: large reasoning models (e.g., GPT-5) achieve safety through constructive expansion (adding nuance), whereas capacity-constrained models revert to destructive truncation (deleting text). Finally, we identify a Reasoning Paradox: while inference-time reasoning increases baseline risk by enabling the model to make deeper sensitive inferences, it simultaneously empowers the defense to execute safe rewrites.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "超越拒绝:探索代理自我纠正对语义敏感信息的限制",
        "summary": "本文介绍了SemSIEdit,这是一个推理时间框架,使用代理编辑来减少语言模型中敏感信息的泄露,同时保持叙述质量。研究人员发现该方法在保持效用的同时,能够在多个敏感信息类别中将隐私风险降低34.6%。这些发现揭示了隐私与模型效用之间的重要权衡,应该为人工智能安全实践提供指导。"
      },
      "fr": {
        "title": "Au-delà du refus : explorer les limites de l'auto-correction agentique pour les informations sensibles sémantiques",
        "summary": "Cet article introduit SemSIEdit, un cadre d'inférence qui utilise l'édition agentique pour réduire la fuite d'informations sensibles des modèles de langage tout en préservant la qualité narrative. Les chercheurs ont constaté que l'approche réduit les risques pour la vie privée de 34,6% dans plusieurs catégories d'informations sensibles tout en maintenant l'utilité. Ces résultats révèlent d'importants compromis entre la vie privée et l'utilité du modèle qui devraient éclairer les pratiques de sécurité de l'IA."
      },
      "de": {
        "title": "Über die Weigerung hinaus: Untersuchung der Grenzen der Selbstkorrektur durch Agenten für semantisch sensible Informationen",
        "summary": "Dieses Papier stellt SemSIEdit vor, ein Inferenz-Zeit-Framework, das agentische Bearbeitung verwendet, um die Weitergabe sensibler Informationen aus Sprachmodellen zu reduzieren und dabei die Narrativqualität zu bewahren. Forscher stellten fest, dass der Ansatz das Datenschutzrisiko um 34,6% über mehrere Kategorien sensibler Informationen hinweg reduziert, während die Nützlichkeit erhalten bleibt. Diese Ergebnisse zeigen wichtige Kompromisse zwischen Datenschutz und Modellnützlichkeit auf, die die KI-Sicherheitspraktiken beeinflussen sollten."
      },
      "es": {
        "title": "Más allá del rechazo: Explorando los límites de la autocorrección agentiva para información semánticamente sensible",
        "summary": "Este trabajo introduce SemSIEdit, un marco de tiempo de inferencia que utiliza edición agentiva para reducir la fuga de información sensible de modelos de lenguaje mientras se preserva la calidad narrativa. Los investigadores encontraron que el enfoque reduce los riesgos de privacidad en un 34,6% en múltiples categorías de información sensible mientras se mantiene la utilidad. Estos hallazgos revelan importantes compensaciones entre privacidad y utilidad del modelo que deberían informar las prácticas de seguridad de la IA."
      }
    }
  },
  {
    "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning",
    "slug": "arlarena-a-unified-framework-for-stable-agentic-reinforcement-learning",
    "url": "https://arxiv.org/abs/2602.21534",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "This paper introduces ARLArena, a systematic framework and training recipe designed to address instability in agentic reinforcement learning (ARL), which has limited scalability for complex tasks. The authors propose SAMPO, a stable agentic policy optimization method that mitigates instability sources while achieving consistent training across diverse agent tasks. This work provides practical guidance for building reliable LLM-based agent training pipelines.",
    "content": "arXiv:2602.21534v1 Announce Type: new \nAbstract: Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "ARLArena：稳定智能体强化学习的统一框架",
        "summary": "本论文介绍了ARLArena，这是一个系统化框架和训练方案，旨在解决智能体强化学习（ARL）的不稳定性问题，该问题限制了复杂任务的可扩展性。作者提出了SAMPO方法，一种稳定的智能体策略优化方法，可以缓解不稳定性源，同时在多样化智能体任务中实现一致的训练。这项工作为构建可靠的基于LLM的智能体训练管道提供了实用指导。"
      },
      "fr": {
        "title": "ARLArena : Un Cadre Unifié pour l'Apprentissage par Renforcement Agentic Stable",
        "summary": "Cet article introduit ARLArena, un cadre systématique et une recette d'entraînement conçus pour résoudre l'instabilité dans l'apprentissage par renforcement agentic (ARL), qui a une scalabilité limitée pour les tâches complexes. Les auteurs proposent SAMPO, une méthode d'optimisation de politique agentic stable qui atténue les sources d'instabilité tout en réalisant un entraînement cohérent sur diverses tâches d'agent. Ces travaux fournissent des conseils pratiques pour construire des pipelines de formation d'agent fiables basés sur LLM."
      },
      "de": {
        "title": "ARLArena: Ein einheitliches Framework für stabiles agentenbasiertes Reinforcement Learning",
        "summary": "Dieses Papier stellt ARLArena vor, ein systematisches Framework und Trainingsrezept, das entwickelt wurde, um Instabilität im agentenbasierten Reinforcement Learning (ARL) zu adressieren, das eine begrenzte Skalierbarkeit für komplexe Aufgaben hat. Die Autoren schlagen SAMPO vor, eine stabile Agentenpolitik-Optimierungsmethode, die Instabilitätsquellen mindert und gleichzeitig konsistentes Training über diverse Agent-Aufgaben hinweg erreicht. Diese Arbeit bietet praktische Anleitungen zum Aufbau zuverlässiger LLM-basierter Agent-Trainingspipelines."
      },
      "es": {
        "title": "ARLArena: Un Marco Unificado para el Aprendizaje por Refuerzo Agentic Estable",
        "summary": "Este artículo presenta ARLArena, un marco sistemático y una receta de entrenamiento diseñados para abordar la inestabilidad en el aprendizaje por refuerzo agentic (ARL), que tiene escalabilidad limitada para tareas complejas. Los autores proponen SAMPO, un método de optimización de política agentic estable que mitiga fuentes de inestabilidad mientras logra un entrenamiento consistente en diversas tareas de agentes. Este trabajo proporciona orientación práctica para construir conductos de entrenamiento de agentes confiables basados en LLM."
      }
    }
  },
  {
    "title": "Power and Limitations of Aggregation in Compound AI Systems",
    "slug": "power-and-limitations-of-aggregation-in-compound-ai-systems",
    "url": "https://arxiv.org/abs/2602.21556",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "Researchers investigate whether aggregating responses from multiple AI model copies unlocks greater output diversity than querying a single model, using a principal-agent framework. The analysis identifies three mechanisms—feasibility expansion, support expansion, and binding set contraction—through which aggregation can expand the set of achievable outputs. These findings help characterize when compound AI systems can overcome individual model limitations.",
    "content": "arXiv:2602.21556v1 Announce Type: new \nAbstract: When designing compound AI systems, a common approach is to query multiple copies of the same model and aggregate the responses to produce a synthesized output. Given the homogeneity of these models, this raises the question of whether aggregation unlocks access to a greater set of outputs than querying a single model. In this work, we investigate the power and limitations of aggregation within a stylized principal-agent framework. This framework models how the system designer can partially steer each agent's output through its reward function specification, but still faces limitations due to prompt engineering ability and model capabilities. Our analysis uncovers three natural mechanisms -- feasibility expansion, support expansion, and binding set contraction -- through which aggregation expands the set of outputs that are elicitable by the system designer. We prove that any aggregation operation must implement one of these mechanisms in order to be elicitability-expanding, and that strengthened versions of these mechanisms provide necessary and sufficient conditions that fully characterize elicitability-expansion. Finally, we provide an empirical illustration of our findings for LLMs deployed in a toy reference-generation task. Altogether, our results take a step towards characterizing when compound AI systems can overcome limitations in model capabilities and in prompt engineering.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "复合AI系统中聚合的力量与局限性",
        "summary": "研究人员调查了聚合多个AI模型副本的响应是否能比查询单个模型产生更大的输出多样性，使用委托-代理框架进行分析。分析确定了三种机制——可行性扩展、支持扩展和绑定集合收缩——通过这些机制聚合可以扩展可实现输出的集合。这些发现有助于表征复合AI系统何时能够克服单个模型的局限性。"
      },
      "fr": {
        "title": "Pouvoir et Limitations de l'Agrégation dans les Systèmes d'IA Composés",
        "summary": "Les chercheurs enquêtent sur le fait que l'agrégation de réponses provenant de plusieurs copies de modèles d'IA déverrouille une plus grande diversité de résultats que l'interrogation d'un seul modèle, en utilisant un cadre principal-agent. L'analyse identifie trois mécanismes—expansion de faisabilité, expansion de support, et contraction d'ensemble lié—par lesquels l'agrégation peut étendre l'ensemble des résultats réalisables. Ces résultats aident à caractériser quand les systèmes d'IA composés peuvent surmonter les limitations des modèles individuels."
      },
      "de": {
        "title": "Macht und Grenzen der Aggregation in zusammengesetzten KI-Systemen",
        "summary": "Forscher untersuchen, ob die Aggregation von Antworten aus mehreren KI-Modellkopien eine größere Ausgangsvielfalt freisetzt als das Abfragen eines einzelnen Modells, unter Verwendung eines Prinzipal-Agent-Rahmens. Die Analyse identifiziert drei Mechanismen – Machbarkeitserweiterung, Unterstützungserweiterung und Binding-Set-Kontraktion – durch die Aggregation die Menge der erreichbaren Ausgaben erweitern kann. Diese Ergebnisse helfen zu charakterisieren, wann zusammengesetzte KI-Systeme die Einschränkungen einzelner Modelle überwinden können."
      },
      "es": {
        "title": "Poder y Limitaciones de la Agregación en Sistemas de IA Compuestos",
        "summary": "Los investigadores investigan si agregar respuestas de múltiples copias de modelos de IA desbloquea una mayor diversidad de salida que consultar un solo modelo, utilizando un marco principal-agente. El análisis identifica tres mecanismos—expansión de viabilidad, expansión de apoyo, y contracción de conjunto vinculante—a través de los cuales la agregación puede expandir el conjunto de salidas alcanzables. Estos hallazgos ayudan a caracterizar cuándo los sistemas de IA compuestos pueden superar las limitaciones de los modelos individuales."
      }
    }
  },
  {
    "title": "The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems",
    "slug": "the-asir-courage-model-phase-dynamic-framework-truth-transitions-human-ai-systems",
    "url": "https://arxiv.org/abs/2602.21745",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "This paper introduces the ASIR Courage Model, a mathematical framework that models truth-disclosure as a state transition influenced by competing forces rather than a fixed trait in both human and AI systems. The framework formalizes how suppression shifts to expression when facilitative forces exceed inhibitory thresholds, with applications to understanding AI preference-driven distortion. This unified approach provides insights into how truth-telling operates under pressure in constrained systems.",
    "content": "arXiv:2602.21745v1 Announce Type: new \nAbstract: We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative forces exceed inhibitory thresholds, expressed by the inequality lambda(1+gamma)+psi > theta+phi, where the terms represent baseline openness, relational amplification, accumulated internal pressure, and transition costs.\n  Although initially formulated for human truth-telling under asymmetric stakes, the same phase-dynamic architecture extends to AI systems operating under policy constraints and alignment filters. In this context, suppression corresponds to constrained output states, while structural pressure arises from competing objectives, contextual tension, and recursive interaction dynamics. The framework therefore provides a unified structural account of both human silence under pressure and AI preference-driven distortion.\n  A feedback extension models how transition outcomes recursively recalibrate system parameters, generating path dependence and divergence effects across repeated interactions. Rather than attributing intention to AI systems, the model interprets shifts in apparent truthfulness as geometric consequences of interacting forces within constrained phase space. By reframing courage and alignment within a shared dynamical structure, the ASIR Courage Model offers a formal perspective on truth-disclosure under risk across both human and artificial systems.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "ASIR勇气模型：人类和AI系统中真理转变的阶段动态框架",
        "summary": "本论文介绍了ASIR勇气模型，一个数学框架，将真理披露建模为受竞争力量影响的状态转变，而不是人类和AI系统中的固定特征。该框架形式化了压制如何在促进力超过抑制阈值时转变为表达，并应用于理解AI偏好驱动的扭曲。这种统一方法提供了对真理在受约束系统中压力下如何运作的见解。"
      },
      "fr": {
        "title": "Le Modèle du Courage ASIR : Un Cadre Dynamique de Phase pour les Transitions de Vérité dans les Systèmes Humains et IA",
        "summary": "Cet article introduit le Modèle du Courage ASIR, un cadre mathématique qui modélise la divulgation de vérité comme une transition d'état influencée par des forces concurrentes plutôt qu'un trait fixe dans les systèmes humains et IA. Le cadre formalise comment la suppression se transforme en expression lorsque les forces facilitantes dépassent les seuils inhibiteurs, avec des applications pour comprendre la distorsion préférence-driven de l'IA. Cette approche unifiée fournit des aperçus sur la façon dont la sincérité fonctionne sous pression dans les systèmes contraints."
      },
      "de": {
        "title": "Das ASIR-Mutmodell: Ein Phasen-dynamisches Framework für Wahrheitsübergänge in menschlichen und KI-Systemen",
        "summary": "Dieses Papier stellt das ASIR-Mutmodell vor, ein mathematisches Framework, das Wahrheitsoffenbarung als Zustandsübergang modelliert, der durch konkurrierende Kräfte beeinflusst wird, anstatt ein festes Merkmal in menschlichen und KI-Systemen zu sein. Das Framework formalisiert, wie Unterdrückung in Ausdruck übergeht, wenn unterstützende Kräfte inhibitorische Schwellenwerte überschreiten, mit Anwendungen zum Verständnis von KI-präferenzgesteuerten Verzerrungen. Dieser einheitliche Ansatz bietet Einblicke in die Funktionsweise von Wahrheitsfähigkeit unter Druck in eingeschränkten Systemen."
      },
      "es": {
        "title": "El Modelo de Coraje ASIR: Un Marco Dinámico de Fase para Transiciones de Verdad en Sistemas Humanos e IA",
        "summary": "Este artículo presenta el Modelo de Coraje ASIR, un marco matemático que modela la divulgación de verdad como una transición de estado influenciada por fuerzas competitivas en lugar de un rasgo fijo en sistemas humanos e IA. El marco formaliza cómo la supresión cambia a expresión cuando las fuerzas facilitadoras superan los umbrales inhibitorios, con aplicaciones para entender la distorsión impulsada por preferencias en IA. Este enfoque unificado proporciona información sobre cómo funciona la sinceridad bajo presión en sistemas restringidos."
      }
    }
  },
  {
    "title": "fEDM+: A Risk-Based Fuzzy Ethical Decision Making Framework with Principle-Level Explainability and Pluralistic Validation",
    "slug": "fedm-plus-risk-based-fuzzy-ethical-decision-making-principle-level-explainability",
    "url": "https://arxiv.org/abs/2602.21746",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "Researchers extend the fuzzy Ethical Decision-Making framework with explainability and pluralistic validation capabilities that link decisions to underlying moral principles and evaluate outcomes against multiple stakeholder perspectives. The framework combines formal verification through Fuzzy Petri Nets with interpretable decision explanations and robustness under ethical disagreement. This extension makes ethical AI systems more transparent and suitable for governance oversight in sensitive applications.",
    "content": "arXiv:2602.21746v1 Announce Type: new \nAbstract: In a previous work, we introduced the fuzzy Ethical Decision-Making framework (fEDM), a risk-based ethical reasoning architecture grounded in fuzzy logic. The original model combined a fuzzy Ethical Risk Assessment module (fERA) with ethical decision rules, enabled formal structural verification through Fuzzy Petri Nets (FPNs), and validated outputs against a single normative referent. Although this approach ensured formal soundness and decision consistency, it did not fully address two critical challenges: principled explainability of decisions and robustness under ethical pluralism. In this paper, we extend fEDM in two major directions. First, we introduce an Explainability and Traceability Module (ETM) that explicitly links each ethical decision rule to the underlying moral principles and computes a weighted principle-contribution profile for every recommended action. This enables transparent, auditable explanations that expose not only what decision was made but why, and on the basis of which principles. Second, we replace single-referent validation with a pluralistic semantic validation framework that evaluates decisions against multiple stakeholder referents, each encoding distinct principle priorities and risk tolerances. This shift allows principled disagreement to be formally represented rather than suppressed, thus increasing robustness and contextual sensitivity. The resulting extended fEDM, called fEDM+, preserves formal verifiability while achieving enhanced interpretability and stakeholder-aware validation, making it suitable as an oversight and governance layer for ethically sensitive AI systems.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "fEDM+：基于风险的模糊伦理决策框架，具有原则级解释性和多元验证",
        "summary": "研究人员扩展了模糊伦理决策框架，增加了可解释性和多元验证功能，将决策与底层道德原则相关联，并针对多个利益相关者的观点评估结果。该框架通过模糊Petri网进行形式化验证，并提供可解释的决策解释和伦理分歧下的稳健性。这一扩展使伦理AI系统更加透明，适合在敏感应用中进行治理监督。"
      },
      "fr": {
        "title": "fEDM+ : Un cadre de prise de décision éthique floue basé sur les risques avec explicabilité au niveau des principes et validation pluraliste",
        "summary": "Les chercheurs étendent le cadre de prise de décision éthique floue avec des capacités d'explicabilité et de validation pluraliste qui lient les décisions aux principes moraux sous-jacents et évaluent les résultats selon les perspectives de multiples parties prenantes. Le cadre combine la vérification formelle via les réseaux de Petri flous avec des explications de décision interprétables et la robustesse en cas de désaccord éthique. Cette extension rend les systèmes d'IA éthiques plus transparents et adaptés à la surveillance de la gouvernance dans les applications sensibles."
      },
      "de": {
        "title": "fEDM+: Ein risikobasiertes unscharfes ethisches Entscheidungsfindungsrahmenwerk mit Erklärbarkeit auf Prinzipienebene und pluralistischer Validierung",
        "summary": "Forscher erweitern das unscharfe Rahmenwerk der ethischen Entscheidungsfindung um Erklärbarkeits- und pluralistische Validierungsfunktionen, die Entscheidungen mit zugrunde liegenden moralischen Prinzipien verknüpfen und Ergebnisse aus der Perspektive mehrerer Interessengruppen bewerten. Das Rahmenwerk kombiniert formale Verifikation durch unscharfe Petri-Netze mit interpretierbaren Entscheidungserklärungen und Robustheit bei ethischem Uneinigkeit. Diese Erweiterung macht ethische KI-Systeme transparenter und eignet sich für die Governance-Überwachung in sensiblen Anwendungen."
      },
      "es": {
        "title": "fEDM+: Un marco de toma de decisiones ética difusa basado en riesgos con explicabilidad a nivel de principios y validación pluralista",
        "summary": "Los investigadores amplían el marco de toma de decisiones ética difusa con capacidades de explicabilidad y validación pluralista que vinculan las decisiones a los principios morales subyacentes y evalúan los resultados desde las perspectivas de múltiples partes interesadas. El marco combina la verificación formal a través de redes de Petri difusas con explicaciones de decisiones interpretables y robustez frente al desacuerdo ético. Esta extensión hace que los sistemas de IA ética sean más transparentes y adecuados para la supervisión de la gobernanza en aplicaciones sensibles."
      }
    }
  },
  {
    "title": "Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem",
    "slug": "prompt-architecture-reasoning-quality-variable-isolation-study-car-wash-problem",
    "url": "https://arxiv.org/abs/2602.21814",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "This controlled study demonstrates that prompt architecture—specifically the STAR reasoning framework—significantly improves language model performance on implicit reasoning tasks, raising accuracy from 0% to 85% on the \"car wash problem\" benchmark. Adding user profile context and RAG further improves performance to 100%, suggesting that structured reasoning scaffolds matter substantially more than context injection. These findings highlight the importance of prompt engineering for complex reasoning tasks.",
    "content": "arXiv:2602.21814v1 Announce Type: new \nAbstract: Large language models consistently fail the \"car wash problem,\" a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "提示架构决定推理质量：汽车洗车问题的变量隔离研究",
        "summary": "这项对照研究表明，提示架构，特别是STAR推理框架，显著提高了语言模型在隐含推理任务上的性能，在\"汽车洗车问题\"基准上将准确度从0%提高到85%。添加用户档案上下文和RAG进一步将性能提高到100%，表明结构化推理支架的重要性远高于上下文注入。这些发现强调了提示工程对于复杂推理任务的重要性。"
      },
      "fr": {
        "title": "L'architecture des invites détermine la qualité du raisonnement : une étude d'isolement des variables sur le problème du lavage de voiture",
        "summary": "Cette étude contrôlée démontre que l'architecture des invites, en particulier le cadre de raisonnement STAR, améliore considérablement les performances des modèles de langage sur les tâches de raisonnement implicite, augmentant la précision de 0% à 85% sur le benchmark \"car wash problem\". L'ajout du contexte du profil utilisateur et du RAG améliore davantage les performances à 100%, suggérant que les échafaudages de raisonnement structurés sont beaucoup plus importants que l'injection de contexte. Ces résultats mettent en évidence l'importance de l'ingénierie des invites pour les tâches de raisonnement complexes."
      },
      "de": {
        "title": "Eingabestruktur bestimmt die Qualität des Denkens: Eine Variablenisolationsstudie zum Autowäsche-Problem",
        "summary": "Diese kontrollierte Studie zeigt, dass die Eingabestruktur – insbesondere das STAR-Denkrahmenwerk – die Leistung von Sprachmodellen bei impliziten Denkaufgaben erheblich verbessert und die Genauigkeit beim Benchmark \"Autowäsche-Problem\" von 0% auf 85% erhöht. Das Hinzufügen von Benutzerprofil-Kontext und RAG verbessert die Leistung weiter auf 100%, was darauf hindeutet, dass strukturierte Denkgerüste viel wichtiger sind als Kontextinjektion. Diese Ergebnisse unterstreichen die Bedeutung von Prompt-Engineering für komplexe Denkaufgaben."
      },
      "es": {
        "title": "La arquitectura del prompt determina la calidad del razonamiento: Un estudio de aislamiento de variables sobre el problema del lavado de autos",
        "summary": "Este estudio controlado demuestra que la arquitectura del prompt, específicamente el marco de razonamiento STAR, mejora significativamente el desempeño del modelo de lenguaje en tareas de razonamiento implícito, aumentando la precisión del 0% al 85% en el benchmark \"car wash problem\". Agregar contexto de perfil de usuario y RAG mejora aún más el desempeño al 100%, lo que sugiere que los andamios de razonamiento estructurado son mucho más importantes que la inyección de contexto. Estos hallazgos destacan la importancia de la ingeniería del prompt para tareas de razonamiento complejo."
      }
    }
  },
  {
    "title": "Distill and Align Decomposition for Enhanced Claim Verification",
    "slug": "distill-and-align-decomposition-for-enhanced-claim-verification",
    "url": "https://arxiv.org/abs/2602.21857",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "Researchers propose a reinforcement learning approach that jointly optimizes sentence decomposition and claim verification alignment using Group Relative Policy Optimization. Their fine-tuned 8-billion parameter model achieves 71.75% macro-F1 on downstream verification tasks, outperforming existing prompt-based and RL methods. This framework demonstrates how smaller language models can achieve state-of-the-art performance through optimized training on verification-specific objectives.",
    "content": "arXiv:2602.21857v1 Announce Type: new \nAbstract: Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "蒸馏和对齐分解用于增强声明验证",
        "summary": "研究人员提出了一种强化学习方法，使用组相对策略优化联合优化句子分解和声明验证对齐。他们微调的80亿参数模型在下游验证任务上达到了71.75%的宏F1，超过了现有的基于提示和强化学习的方法。这个框架展示了较小的语言模型如何通过在验证特定目标上进行优化训练来实现最先进的性能。"
      },
      "fr": {
        "title": "Distillation et décomposition d'alignement pour une vérification améliorée des affirmations",
        "summary": "Les chercheurs proposent une approche d'apprentissage par renforcement qui optimise conjointement la décomposition des phrases et l'alignement de la vérification des affirmations en utilisant l'optimisation des politiques relatives de groupe. Leur modèle de 8 milliards de paramètres affiné atteint 71,75% de macro-F1 sur les tâches de vérification en aval, surpassant les méthodes existantes basées sur des invites et RL. Ce cadre démontre comment les modèles de langage plus petits peuvent atteindre des performances de pointe grâce à une formation optimisée sur les objectifs de vérification spécifiques."
      },
      "de": {
        "title": "Destillation und Ausrichtungsabbau für verbesserte Anspruchsverifikation",
        "summary": "Forscher schlagen einen Ansatz des verstärkten Lernens vor, der die Satzzerlegung und die Ausrichtung der Anspruchsverifikation mithilfe der Optimierung der Gruppenpolitik relativ optimiert. Ihr optimiertes Modell mit 8 Milliarden Parametern erreicht 71,75% Makro-F1 bei nachgelagerten Verifikationsaufgaben und übertrifft bestehende Prompt-basierte und RL-Methoden. Dieses Framework zeigt, wie kleinere Sprachmodelle durch optimierte Schulung auf verifikationsspezifische Ziele Spitzenleistungen erzielen können."
      },
      "es": {
        "title": "Destilación y alineación de descomposición para verificación mejorada de afirmaciones",
        "summary": "Los investigadores proponen un enfoque de aprendizaje por refuerzo que optimiza conjuntamente la descomposición de oraciones y la alineación de verificación de afirmaciones utilizando la optimización de política relativa de grupo. Su modelo de 8 mil millones de parámetros afinado logra 71.75% de macro-F1 en tareas de verificación descendentes, superando los métodos existentes basados en indicaciones y RL. Este marco demuestra cómo los modelos de lenguaje más pequeños pueden lograr un desempeño de vanguardia a través del entrenamiento optimizado en objetivos específicos de verificación."
      }
    }
  },
  {
    "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
    "slug": "proactivemobile-benchmark-proactive-intelligence-mobile-devices",
    "url": "https://arxiv.org/abs/2602.21858",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "Researchers introduce ProactiveMobile, a comprehensive benchmark with over 3,660 test instances designed to advance proactive mobile agent development beyond reactive command execution. The benchmark evaluates an agent's ability to autonomously anticipate user needs and execute appropriate actions from a pool of 63 mobile APIs across 14 real-world scenarios. Results show that current leading models struggle with proactivity, highlighting a critical capability gap in mobile AI agents.",
    "content": "arXiv:2602.21858v1 Announce Type: new \nAbstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "ProactiveMobile: 提升移动设备主动智能的综合基准",
        "summary": "研究人员推出了ProactiveMobile，这是一个包含超过3660个测试实例的综合基准，旨在推进主动型移动代理的开发，超越被动命令执行。该基准评估了代理在14个真实场景中从63个移动API池中自主预测用户需求和执行适当操作的能力。结果表明当前领先的模型在主动性方面存在困难，突出了移动AI代理中的关键能力差距。"
      },
      "fr": {
        "title": "ProactiveMobile: Un benchmark complet pour améliorer l'intelligence proactive sur les appareils mobiles",
        "summary": "Les chercheurs présentent ProactiveMobile, un benchmark complet contenant plus de 3660 instances de test conçu pour faire avancer le développement d'agents mobiles proactifs au-delà de l'exécution de commandes réactives. Le benchmark évalue la capacité d'un agent à anticiper autonomement les besoins de l'utilisateur et à exécuter les actions appropriées à partir d'un ensemble de 63 API mobiles dans 14 scénarios du monde réel. Les résultats montrent que les modèles les plus performants actuels ont du mal avec la proactivité, mettant en évidence une lacune critique en matière de capacités dans les agents d'IA mobiles."
      },
      "de": {
        "title": "ProactiveMobile: Ein umfassendes Benchmark zur Steigerung der proaktiven Intelligenz auf Mobilgeräten",
        "summary": "Forscher stellen ProactiveMobile vor, ein umfassendes Benchmark mit über 3660 Testinstanzen, das die Entwicklung proaktiver mobiler Agenten über die reaktive Befehlsausführung hinaus vorantreiben soll. Das Benchmark bewertet die Fähigkeit eines Agenten, Benutzerbedürfnisse autonom vorauszusehen und geeignete Maßnahmen aus einem Pool von 63 mobilen APIs über 14 reale Szenarien hinweg auszuführen. Die Ergebnisse zeigen, dass aktuelle führende Modelle mit Proaktivität kämpfen, was eine kritische Fähigkeitslücke in mobilen KI-Agenten hervorhebt."
      },
      "es": {
        "title": "ProactiveMobile: Un benchmark integral para impulsar la inteligencia proactiva en dispositivos móviles",
        "summary": "Los investigadores presentan ProactiveMobile, un benchmark integral con más de 3660 instancias de prueba diseñado para avanzar en el desarrollo de agentes móviles proactivos más allá de la ejecución de comandos reactivos. El benchmark evalúa la capacidad de un agente para anticipar autónomamente las necesidades del usuario y ejecutar acciones apropiadas desde un conjunto de 63 API móviles en 14 escenarios del mundo real. Los resultados muestran que los modelos líderes actuales luchan con la proactividad, destacando una brecha crítica de capacidades en agentes de IA móvil."
      }
    }
  },
  {
    "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support",
    "slug": "2-step-agent-framework-interaction-decision-maker-ai-decision-support",
    "url": "https://arxiv.org/abs/2602.21889",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "This paper presents a computational framework that models how AI decision support systems affect human decision-making through Bayesian methods for causal inference. The research reveals that misaligned prior beliefs can cause AI-assisted decisions to produce worse outcomes than no decision support, highlighting critical pitfalls in AI deployment. These findings underscore the importance of model documentation and user training for effective human-AI collaboration.",
    "content": "arXiv:2602.21889v1 Announce Type: new \nAbstract: Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how this change in beliefs affects the downstream decision and subsequent outcome. Using this framework, we show by simulations how a single misaligned prior belief can be sufficient for decision support to result in worse downstream outcomes compared to no decision support. Our results reveal several potential pitfalls of AI-driven decision support and highlight the need for thorough model documentation and proper user training.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "2步代理：决策者与AI决策支持系统交互的框架",
        "summary": "本文提出了一个计算框架，该框架通过贝叶斯因果推断方法对AI决策支持系统如何影响人类决策制定进行建模。研究表明，不一致的先验信念可能导致AI辅助决策产生比没有决策支持更差的结果，突出了AI部署中的关键陷阱。这些发现强调了有效的人机协作中模型文档和用户培训的重要性。"
      },
      "fr": {
        "title": "2-Step Agent: Un cadre pour l'interaction d'un décideur avec un système d'aide à la décision par IA",
        "summary": "Cet article présente un cadre informatique qui modélise comment les systèmes d'aide à la décision par IA affectent la prise de décision humaine grâce aux méthodes bayésiennes d'inférence causale. La recherche révèle que les croyances antérieures mal alignées peuvent faire que les décisions assistées par l'IA produisent des résultats pires qu'aucun soutien décisionnel, mettant en évidence les pièges critiques du déploiement de l'IA. Ces résultats soulignent l'importance de la documentation des modèles et de la formation des utilisateurs pour une collaboration homme-IA efficace."
      },
      "de": {
        "title": "2-Step Agent: Ein Rahmenwerk für die Interaktion eines Entscheidungsträgers mit KI-Entscheidungsunterstützung",
        "summary": "Dieses Papier präsentiert ein rechnerisches Rahmenwerk, das modelliert, wie KI-Entscheidungsunterstützungssysteme die menschliche Entscheidungsfindung durch Bayes'sche Methoden zur Kausalinferenz beeinflussen. Die Forschung zeigt, dass nicht ausgerichtete vorherige Überzeugungen dazu führen können, dass KI-gestützte Entscheidungen schlechtere Ergebnisse liefern als keine Entscheidungsunterstützung, was kritische Fallstricke bei der KI-Bereitstellung hervorhebt. Diese Ergebnisse unterstreichen die Bedeutung von Modelldokumentation und Benutzertraining für eine effektive Mensch-KI-Zusammenarbeit."
      },
      "es": {
        "title": "2-Step Agent: Un marco para la interacción de un tomador de decisiones con apoyo a la decisión de IA",
        "summary": "Este artículo presenta un marco computacional que modela cómo los sistemas de apoyo a la decisión de IA afectan la toma de decisiones humana a través de métodos bayesianos para inferencia causal. La investigación revela que las creencias previas desalineadas pueden hacer que las decisiones asistidas por IA produzcan resultados peores que sin apoyo a la decisión, destacando peligros críticos en la implementación de IA. Estos hallazgos subrayan la importancia de la documentación del modelo y el entrenamiento del usuario para una colaboración humano-IA efectiva."
      }
    }
  },
  {
    "title": "Semantic Partial Grounding via LLMs",
    "slug": "semantic-partial-grounding-via-llms",
    "url": "https://arxiv.org/abs/2602.22067",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "This paper introduces SPG-LLM, which leverages language models to analyze planning domain descriptions and preemptively identify irrelevant objects and actions before grounding. The approach significantly reduces computational bottlenecks in classical planning by orders of magnitude while maintaining comparable or improved plan quality across multiple benchmarks. This method addresses a scalability challenge fundamental to automated planning systems.",
    "content": "arXiv:2602.22067v1 Announce Type: new \nAbstract: Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "通过语言模型的语义部分接地",
        "summary": "本文介绍了SPG-LLM，该方法利用语言模型分析规划领域描述，并在接地前抢先识别不相关的对象和操作。该方法在经典规划中显著降低了计算瓶颈数个数量级，同时在多个基准上保持了可比较或更好的计划质量。这种方法解决了自动化规划系统中的根本可扩展性挑战。"
      },
      "fr": {
        "title": "Ancrage partiel sémantique via les LLM",
        "summary": "Cet article présente SPG-LLM, qui exploite les modèles de langage pour analyser les descriptions du domaine de planification et identifier de manière préventive les objets et les actions non pertinents avant l'ancrage. L'approche réduit considérablement les goulots d'étranglement informatiques de la planification classique de plusieurs ordres de grandeur tout en maintenant une qualité de plan comparable ou améliorée sur plusieurs points de repère. Cette méthode résout un défi d'évolutivité fondamental pour les systèmes de planification automatisée."
      },
      "de": {
        "title": "Semantische teilweise Verankerung über LLMs",
        "summary": "Dieses Papier stellt SPG-LLM vor, das Sprachmodelle nutzt, um Planungsbereichsbeschreibungen zu analysieren und vor der Verankerung proaktiv irrelevante Objekte und Aktionen zu identifizieren. Der Ansatz reduziert Rechenengsässe in der klassischen Planung um mehrere Größenordnungen erheblich, während gleichzeitig eine vergleichbare oder verbesserte Planqualität über mehrere Benchmarks hinweg beibehalten wird. Diese Methode adressiert eine grundlegende Skalierungschallenge für automatisierte Planungssysteme."
      },
      "es": {
        "title": "Anclaje Parcial Semántico a través de LLM",
        "summary": "Este artículo presenta SPG-LLM, que aprovecha modelos de lenguaje para analizar descripciones de dominios de planificación e identificar de manera preventiva objetos y acciones irrelevantes antes del anclaje. El enfoque reduce significativamente los cuellos de botella computacionales en la planificación clásica por varios órdenes de magnitud mientras mantiene una calidad de plan comparable o mejorada en múltiples puntos de referencia. Este método aborda un desafío fundamental de escalabilidad para sistemas de planificación automatizada."
      }
    }
  },
  {
    "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts",
    "slug": "language-models-inconsistent-biases-algorithmic-agents-human-experts",
    "url": "https://arxiv.org/abs/2602.22070",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "This empirical study evaluates how language models weigh information from human experts versus algorithmic agents, finding that LLMs exhibit inconsistent biases depending on task framing. While LLMs rate human experts as more trustworthy when asked directly, they preferentially choose algorithms when given performance examples, even when algorithms perform worse. These findings reveal important vulnerabilities in LLM decision-making that require careful consideration for high-stakes applications.",
    "content": "arXiv:2602.22070v1 Announce Type: new \nAbstract: Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "语言模型对算法智能体和人类专家的偏见不一致",
        "summary": "这项实证研究评估了语言模型如何权衡人类专家和算法智能体的信息，发现大型语言模型根据任务框架表现出不一致的偏见。虽然直接询问时，大型语言模型认为人类专家更值得信任，但当给出性能示例时，它们更倾向于选择算法，即使算法表现更差。这些发现揭示了大型语言模型决策中的重要漏洞，需要在高风险应用中进行仔细考虑。"
      },
      "fr": {
        "title": "Les modèles de langage présentent des préjugés inconsistants envers les agents algorithmiques et les experts humains",
        "summary": "Cette étude empirique évalue comment les modèles de langage pondèrent les informations provenant d'experts humains par rapport aux agents algorithmiques, constatant que les LLM présentent des préjugés inconsistants selon le cadrage de la tâche. Bien que les LLM considèrent les experts humains comme plus dignes de confiance lorsqu'on leur demande directement, ils choisissent préférentiellement les algorithmes lorsqu'on leur donne des exemples de performance, même lorsque les algorithmes fonctionnent moins bien. Ces résultats révèlent des vulnérabilités importantes dans la prise de décision des LLM qui nécessitent une considération attentive pour les applications à enjeux élevés."
      },
      "de": {
        "title": "Sprachmodelle zeigen inkonsistente Vorurteile gegenüber algorithmischen Agenten und menschlichen Experten",
        "summary": "Diese empirische Studie bewertet, wie Sprachmodelle Informationen von menschlichen Experten gegenüber algorithmischen Agenten gewichten, und stellt fest, dass große Sprachmodelle je nach Aufgabengestaltung inkonsistente Vorurteile aufweisen. Obwohl große Sprachmodelle menschliche Experten für vertrauenswürdiger halten, wenn sie direkt gefragt werden, bevorzugen sie Algorithmen, wenn ihnen Leistungsbeispiele gegeben werden, auch wenn Algorithmen schlechter abschneiden. Diese Ergebnisse offenbaren wichtige Schwachstellen bei der Entscheidungsfindung von großen Sprachmodellen, die sorgfältig überlegt werden müssen für hochriskante Anwendungen."
      },
      "es": {
        "title": "Los modelos de lenguaje exhiben sesgos inconsistentes hacia agentes algorítmicos y expertos humanos",
        "summary": "Este estudio empírico evalúa cómo los modelos de lenguaje ponderan la información de expertos humanos versus agentes algorítmicos, encontrando que los LLM exhiben sesgos inconsistentes según el encuadramiento de la tarea. Aunque los LLM califican a los expertos humanos como más confiables cuando se les pregunta directamente, prefieren elegir algoritmos cuando se les dan ejemplos de desempeño, incluso cuando los algoritmos funcionan peor. Estos hallazgos revelan vulnerabilidades importantes en la toma de decisiones de LLM que requieren consideración cuidadosa para aplicaciones de alto riesgo."
      }
    }
  },
  {
    "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning",
    "slug": "petri-net-relaxation-infeasibility-explanation-sequential-task-planning",
    "url": "https://arxiv.org/abs/2602.22094",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "Researchers propose a Petri net-based approach that enables robust invariant synthesis, efficient detection of infeasible plans, and helpful explanations for why plans cannot be executed. The system outperforms baselines in detecting infeasibilities while maintaining competitive performance in standard planning tasks and excelling at sequential plan updates. This work addresses the practical challenge of handling dynamic planning scenarios where feasibility must be verified and explained.",
    "content": "arXiv:2602.22094v1 Announce Type: new \nAbstract: Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations. We further leverage incremental constraint solvers to support goal and constraint updates. Empirically, compared to baselines, our system produces a comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in the tested domains.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "Petri网松弛用于不可行性解释和顺序任务规划",
        "summary": "研究人员提出了一种基于Petri网的方法，该方法能够实现稳健的不变量合成、高效检测不可行计划，并为计划无法执行的原因提供有用的解释。该系统在检测不可行性方面的性能优于基准方法，同时在标准规划任务中保持竞争性能，在顺序计划更新中表现出色。这项工作解决了在动态规划场景中处理可行性验证和解释的实际挑战。"
      },
      "fr": {
        "title": "Relaxation des réseaux de Petri pour l'explication de l'infaisabilité et la planification séquentielle des tâches",
        "summary": "Les chercheurs proposent une approche basée sur les réseaux de Petri qui permet la synthèse robuste d'invariants, la détection efficace de plans non réalisables et des explications utiles sur les raisons pour lesquelles les plans ne peuvent pas être exécutés. Le système surpasse les méthodes de base dans la détection des infaisabilités tout en maintenant des performances compétitives dans les tâches de planification standard et en excellant dans les mises à jour de plans séquentiels. Ce travail aborde le défi pratique de gérer les scénarios de planification dynamique où la faisabilité doit être vérifiée et expliquée."
      },
      "de": {
        "title": "Petri-Netz-Relaxation für Nicht-Durchführbarkeitserklärung und sequentielle Aufgabenplanung",
        "summary": "Forscher schlagen einen Petri-Netz-basierten Ansatz vor, der robuste Invariantensynthese, effiziente Erkennung nicht realisierbarer Pläne und hilfreiche Erklärungen dafür ermöglicht, warum Pläne nicht ausgeführt werden können. Das System übertrifft die Baselines bei der Erkennung von Nicht-Durchführbarkeiten, während es wettbewerbsfähige Leistung bei standardmäßigen Planungsaufgaben beibehält und bei sequentiellen Planaktualisierungen hervorragend abschneidet. Diese Arbeit behandelt die praktische Herausforderung, dynamische Planungsszenarien zu bewältigen, in denen die Durchführbarkeit überprüft und erklärt werden muss."
      },
      "es": {
        "title": "Relajación de redes de Petri para explicación de inviabilidad y planificación secuencial de tareas",
        "summary": "Los investigadores proponen un enfoque basado en redes de Petri que permite la síntesis robusta de invariantes, la detección eficiente de planes inviables y explicaciones útiles sobre por qué los planes no se pueden ejecutar. El sistema supera los métodos de referencia en la detección de inviabilidades mientras mantiene un desempeño competitivo en tareas de planificación estándar y destaca en actualizaciones de planes secuenciales. Este trabajo aborda el desafío práctico de manejar escenarios de planificación dinámica donde la viabilidad debe ser verificada y explicada."
      }
    }
  },
  {
    "title": "Inference-time Alignment via Sparse Junction Steering",
    "slug": "inference-time-alignment-via-sparse-junction-steering",
    "url": "https://arxiv.org/abs/2602.21215",
    "source": "arXiv cs.AI",
    "date": "2026-02-26T05:00:00.000Z",
    "summary": "This paper demonstrates that sparse token-level steering—intervening at only 20-80% of high-entropy decision points—can match or exceed the alignment quality of dense intervention methods while reducing computational costs by up to 6x. The approach selectively targets critical junctures in generation where alignment issues are most likely to occur, improving the efficiency-alignment trade-off. These findings suggest that more targeted intervention strategies can enhance both performance and computational efficiency in LLM alignment.",
    "content": "arXiv:2602.21215v1 Announce Type: cross \nAbstract: Token-level steering has emerged as a pivotal approach for inference-time alignment, enabling fine grained control over large language models by modulating their output distributions without parameter updates. While effective, existing methods rely on dense intervention at every decoding step. This persistent manipulation not only incurs substantial computational overhead but also risks compromising generation quality by excessively drifting from the model's intrinsic distribution. In this work, we show that dense intervention is unnecessary and propose Sparse Inference time Alignment (SIA), which performs sparse junction steering by intervening only at critical decision points along the generation trajectory. Our key insight is that high entropy junctions mark pivotal decision points in the generation trajectory and are particularly susceptible to misalignment, indicating the need to introduce alignment related reward signals at these points. Extensive experiments across different model families and alignment objectives show that steering only 20% to 80% of tokens achieves superior alignment-efficiency trade offs. For strong base models such as Qwen3, intervening on as few as 20% of tokens matches or even surpasses heavily post-trained instruct models. This sparsity enables stronger guidance while better preserving the model's native distribution, integrates seamlessly with search based methods such as Best-of-N, and reduces computational cost by up to 6x.",
    "category": "ai",
    "translations": {
      "zh": {
        "title": "通过稀疏交叉点转向的推理时对齐",
        "summary": "本文证明了稀疏令牌级转向——仅在20-80%的高熵决策点进行干预——可以匹配或超过密集干预方法的对齐质量，同时将计算成本降低至6倍。该方法选择性地针对生成过程中最可能出现对齐问题的关键交叉点，改善了效率-对齐的权衡。这些发现表明，更有针对性的干预策略可以增强大型语言模型对齐的性能和计算效率。"
      },
      "fr": {
        "title": "Alignement au moment de l'inférence via direction creuse à la jonction",
        "summary": "Cet article démontre que la direction au niveau des tokens creuse — intervenant à seulement 20-80% des points de décision à haute entropie — peut égaler ou dépasser la qualité d'alignement des méthodes d'intervention denses tout en réduisant les coûts de calcul jusqu'à 6 fois. L'approche cible sélectivement les jonctions critiques de la génération où les problèmes d'alignement sont les plus susceptibles de se produire, améliorant le compromis efficacité-alignement. Ces résultats suggèrent que des stratégies d'intervention plus ciblées peuvent améliorer à la fois les performances et l'efficacité informatique de l'alignement des LLM."
      },
      "de": {
        "title": "Laufzeit-Ausrichtung durch sparsame Knotenlenkung",
        "summary": "Dieses Papier zeigt, dass sparsame Token-Level-Lenkung — Intervention bei nur 20-80% der Hochentropie-Entscheidungspunkte — die Ausrichtungsqualität dichter Interventionsmethoden erreichen oder übertreffen kann, während die Rechenkosten um bis zu 6x reduziert werden. Der Ansatz zielt selektiv auf kritische Verbindungspunkte in der Generierung ab, wo Ausrichtungsprobleme am wahrscheinlichsten auftreten, und verbessert den Effizienz-Ausrichtungs-Kompromiss. Diese Erkenntnisse deuten darauf hin, dass gezielere Interventionsstrategien sowohl die Leistung als auch die Recheneffizienz der LLM-Ausrichtung verbessern können."
      },
      "es": {
        "title": "Alineación en tiempo de inferencia mediante dirección dispersa de unión",
        "summary": "Este documento demuestra que la dirección a nivel de token dispersa — interviniendo solo en el 20-80% de los puntos de decisión de alta entropía — puede igualar o superar la calidad de alineación de los métodos de intervención densa mientras reduce los costos computacionales hasta 6 veces. El enfoque apunta selectivamente a las uniones críticas en la generación donde es más probable que ocurran problemas de alineación, mejorando el equilibrio eficiencia-alineación. Estos hallazgos sugieren que estrategias de intervención más específicas pueden mejorar tanto el rendimiento como la eficiencia computacional en la alineación de LLM."
      }
    }
  }
]