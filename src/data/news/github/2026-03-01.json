[
  {
    "title": "Stop Writing Spaghetti API Routes — Structure Your Express.js App Like a Pro",
    "slug": "express-app-folder-structure-scaling",
    "url": "https://dev.to/teguh_coding/stop-writing-spaghetti-api-routes-structure-your-expressjs-app-like-a-pro-1d02",
    "source": "DEV Community",
    "date": "2026-03-01T12:06:30.000Z",
    "summary": "A guide to organizing Express.js applications with a scalable folder structure (config, controllers, services, models, routes) that separates concerns, improves maintainability, and reduces onboarding friction as projects grow from side projects to production systems.",
    "content": "Every developer has been there. You start a new Express.js project with the best intentions. A clean index.js, maybe two routes, and a dream. Then three months later, you open the file and there are 800 lines of route handlers, middleware piled on top of each other, and database calls scattered everywhere like confetti after a bad party.\nThis is the spaghetti API problem. And it kills more projects than bugs do.\nIn this article, I am going to walk you through a battle-tested folder structure for Express.js that scales — from weekend side project to production application — without turning into a maintenance nightmare.\nBad structure is not just an aesthetic problem. It actively slows down your team, makes onboarding new developers painful, and turns \"add a simple feature\" into a three-hour archaeological dig through the codebase.\nGood structure does the opposite. It makes the right place for any piece of code obvious. It separates concerns so changes in one area don't ripple unexpectedly into another. And it lets you scale the team and the codebase without everything falling apart.\nLet's build it from the ground up.\nHere's the structure we're going to implement:\nmy-api/\n  src/\n    config/\n      index.js\n      database.js\n    controllers/\n      userController.js\n      postController.js\n    middlewares/\n      auth.js\n      errorHandler.js\n      validate.js\n    models/\n      User.js\n      Post.js\n    routes/\n      index.js\n      userRoutes.js\n      postRoutes.js\n    services/\n      userService.js\n      postService.js\n    utils/\n      logger.js\n      response.js\n  app.js\n  server.js\n\nLet me break down each layer and why it exists.\nAll your environment variables and configuration live here. No more process.env.DATABASE_URL scattered across 15 files.\n// src/config/index.js\nmodule.exports = {\n  port: process.env.PORT || 3000,\n  nodeEnv: process.env.NODE_ENV || 'development',\n  jwtSecret: process.env.JWT_SECRET,\n  db: {\n    url: process.env.DATABASE_URL,\n    poolSize: parseI",
    "category": "github",
    "translations": {
      "zh": {
        "title": "停止编写混乱的 API 路由 — 像专业人士一样构建 Express.js 应用",
        "summary": "一份关于如何用可扩展的文件夹结构（配置、控制器、服务、模型、路由）来组织 Express.js 应用的指南，该结构分离了关注点，提高了可维护性，并减少了项目从副业项目成长到生产系统时的入门摩擦。"
      },
      "fr": {
        "title": "Arrêtez d'écrire des routes API désorganisées — Structurez votre application Express.js comme un pro",
        "summary": "Un guide pour organiser les applications Express.js avec une structure de dossiers évolutive (config, contrôleurs, services, modèles, routes) qui sépare les préoccupations, améliore la maintenabilité et réduit les frictions d'intégration à mesure que les projets se développent de projets secondaires à des systèmes de production."
      },
      "de": {
        "title": "Hör auf, unorganisierte API-Routen zu schreiben — strukturiere deine Express.js-App wie ein Profi",
        "summary": "Ein Leitfaden zur Organisation von Express.js-Anwendungen mit einer skalierbaren Ordnerstruktur (Konfiguration, Controller, Services, Modelle, Routen), die Bedenken trennt, die Wartbarkeit verbessert und die Einarbeitungsreibung reduziert, wenn Projekte von Nebenprojekten zu Produktionssystemen heranwachsen."
      },
      "es": {
        "title": "Deja de escribir rutas API desordenadas — Estructura tu aplicación Express.js como un profesional",
        "summary": "Una guía para organizar aplicaciones Express.js con una estructura de carpetas escalable (configuración, controladores, servicios, modelos, rutas) que separa las preocupaciones, mejora la mantenibilidad y reduce la fricción de incorporación a medida que los proyectos crecen de proyectos secundarios a sistemas de producción."
      }
    }
  },
  {
    "title": "Drop Your Animation Library: 5 Lines to Make Your SPA Feel Like a Native App",
    "slug": "view-transitions-api-spa-animations",
    "url": "https://dev.to/linou518/drop-your-animation-library-5-lines-to-make-your-spa-feel-like-a-native-app-3noe",
    "source": "DEV Community",
    "date": "2026-03-01T12:06:23.000Z",
    "summary": "The View Transitions API, now available in all major browsers, enables smooth SPA page transitions with just 5 lines of code and customizable CSS animations, eliminating the need for Framer Motion and other animation libraries.",
    "content": "Drop Your Animation Library: 5 Lines to Make Your SPA Feel Like a Native App\n\n\nAre you importing Framer Motion just for page transitions?\nopacity + transform by hand every time you switch views in your SPA?\nIn October 2025, the View Transitions API landed in Baseline — zero dependencies, five lines of code, supported in all major browsers except Firefox (for MPA mode).\nIn short: it automatically animates between before and after DOM states.\nThe browser does three things under the hood:\nTakes a screenshot of the old state\nRuns your callback to update the DOM\nTakes a screenshot of the new state and animates the diff\nEverything happens within a single frame. Users just see a smooth transition. Default is crossfade. Add CSS and you can do slides, morphs — anything.\nI tested this on a Flask + SPA internal dashboard I maintain. Here's the original view-switching code:\nfunction showView(name) {\n  document.querySelectorAll('.view').forEach(v => v.style.display = 'none');\n  document.getElementById('view-' + name).style.display = 'block';\n  updateActiveNav(name);\n}\n\nMaking it View Transitions-aware:\nfunction showView(name) {\n  const doSwitch = () => {\n    document.querySelectorAll('.view').forEach(v => v.style.display = 'none');\n    document.getElementById('view-' + name).style.display = 'block';\n    updateActiveNav(name);\n  };\n\n  if (document.startViewTransition) {\n    document.startViewTransition(doSwitch);\n  } else {\n    doSwitch(); // fallback: works normally without animation\n  }\n}\n\nThat's it. The overview → family → nodes tab switches now have a soft crossfade. No build step, no new dependency, no bundle size increase.\nIf the default fade isn't your thing, override it with CSS:\n/* Slide left-to-right */\n::view-transition-old(root) {\n  animation: slide-out 0.25s ease-out;\n}\n::view-transition-new(root) {\n  animation: slide-in 0.25s ease-out;\n}\n\n@keyframes slide-out {\n  to { transform: translateX(-40px); opacity: 0; }\n}\n@keyframes slide-in {\n  from { transform: translateX(",
    "category": "github",
    "translations": {
      "zh": {
        "title": "放弃你的动画库：用 5 行代码让你的单页应用感觉像原生应用",
        "summary": "视图过渡 API 现已在所有主要浏览器中可用，只需 5 行代码和可自定义的 CSS 动画即可实现平滑的单页应用页面过渡，无需使用 Framer Motion 等动画库。"
      },
      "fr": {
        "title": "Abandonnez votre bibliothèque d'animation : 5 lignes pour que votre SPA ressemble à une application native",
        "summary": "L'API View Transitions, désormais disponible dans tous les navigateurs majeurs, permet des transitions de pages SPA fluides avec seulement 5 lignes de code et des animations CSS personnalisables, éliminant le besoin de Framer Motion et d'autres bibliothèques d'animation."
      },
      "de": {
        "title": "Vergessen Sie Ihre Animationsbibliothek: 5 Zeilen, um Ihre SPA wie eine native App zu machen",
        "summary": "Die View Transitions API ist nun in allen großen Browsern verfügbar und ermöglicht reibungslose SPA-Seitenwechsel mit nur 5 Codezeilen und anpassbaren CSS-Animationen, wodurch die Notwendigkeit von Framer Motion und anderen Animationsbibliotheken entfällt."
      },
      "es": {
        "title": "Abandona tu biblioteca de animaciones: 5 líneas para que tu SPA se sienta como una aplicación nativa",
        "summary": "La API View Transitions, ahora disponible en todos los navegadores principales, permite transiciones de página SPA sin problemas con solo 5 líneas de código y animaciones CSS personalizables, eliminando la necesidad de Framer Motion y otras bibliotecas de animaciones."
      }
    }
  },
  {
    "title": "Google Gemini API Key Security Breach Risk: The Rules Changed",
    "slug": "gemini-api-key-security-breach-risk",
    "url": "https://dev.to/maverickjkp/google-gemini-api-key-security-breach-risk-the-rules-changed-15g3",
    "source": "DEV Community",
    "date": "2026-03-01T12:04:05.000Z",
    "summary": "Google's 2026 billing changes tie Gemini API keys directly to production accounts without free-tier protection, making exposed keys a serious financial risk. The PromptSpy malware campaign confirms this threat vector is actively exploited against developers.",
    "content": "Something shifted quietly in early 2026, and most developers missed it.\nGoogle Gemini API keys — previously treated as low-stakes configuration strings — now carry the same breach risk as payment credentials or OAuth tokens. That's not hyperbole. It's a direct consequence of how Gemini's billing model changed.\nFor years, Google's API key philosophy was relaxed by design. Keys for Maps, YouTube Data, and similar services were semi-public — exposed in client-side JavaScript, checked into repos, embedded in mobile apps. Google's dashboard let you restrict them by referrer or IP, and even an exposed key caused limited damage because usage was often free-tiered or rate-limited.\nGemini broke that pattern. As Simon Willison documented on February 26, 2026, Gemini API keys are now directly tied to billing accounts with no free-tier buffer in production contexts. An exposed Gemini key isn't an embarrassment — it's an open invoice waiting to be filled by whoever finds it first.\nThe breach risk is accelerating for three reasons: developers are applying old mental models to a new threat surface, tooling hasn't caught up, and attackers have absolutely noticed. The PromptSpy malware campaign — the first documented Android malware to exfiltrate Gemini API keys — confirmed in early 2026 that this attack vector is live, not theoretical.\nKey Takeaways\nGoogle changed Gemini API key behavior in 2026 so that exposed keys now carry direct billing consequences, unlike most previous Google API credentials.\nPromptSpy, documented in early 2026 by TechNadu, is the first confirmed Android malware specifically engineered to target and exfiltrate Generative AI API keys, including Gemini credentials.\nSimon Willison's February 26, 2026 analysis identified the core problem: Gemini API keys weren't historically treated as secrets — a mental model that most developers still haven't updated.\nThe Gemini key security risk sits at the intersection of credential theft, financial fraud, and AI abuse, makin",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Google Gemini API 密钥安全漏洞风险：规则已改变",
        "summary": "Google 的 2026 年计费更改将 Gemini API 密钥直接绑定到生产账户，没有免费层保护，使暴露的密钥成为严重的财务风险。PromptSpy 恶意软件活动证实这个威胁向量正在被积极利用攻击开发人员。"
      },
      "fr": {
        "title": "Risque de fuite de sécurité de la clé API Google Gemini : Les règles ont changé",
        "summary": "Les modifications de facturation 2026 de Google lient les clés API Gemini directement aux comptes de production sans protection de niveau gratuit, ce qui rend les clés exposées un risque financier grave. La campagne de malware PromptSpy confirme que ce vecteur de menace est activement exploité contre les développeurs."
      },
      "de": {
        "title": "Sicherheitsleck-Risiko bei Google Gemini API-Schlüssel: Die Regeln haben sich geändert",
        "summary": "Googles Änderungen der Abrechnung 2026 binden Gemini API-Schlüssel direkt an Produktionskonten ohne kostenlosen Tier-Schutz und machen freiliegende Schlüssel zu einem ernsthaften finanziellen Risiko. Die PromptSpy-Malware-Kampagne bestätigt, dass dieser Bedrohungsvektor aktiv gegen Entwickler ausgenutzt wird."
      },
      "es": {
        "title": "Riesgo de brecha de seguridad de la clave API de Google Gemini: Las reglas han cambiado",
        "summary": "Los cambios de facturación de 2026 de Google vinculan las claves API de Gemini directamente a cuentas de producción sin protección de nivel gratuito, lo que hace que las claves expuestas sean un riesgo financiero grave. La campaña de malware PromptSpy confirma que este vector de amenaza se está explotando activamente contra desarrolladores."
      }
    }
  },
  {
    "title": "Fake Job Interview Backdoor Malware Targeting Developer Machines",
    "slug": "contagious-interview-malware-developers",
    "url": "https://dev.to/maverickjkp/fake-job-interview-backdoor-malware-targeting-developer-machines-33g",
    "source": "DEV Community",
    "date": "2026-03-01T12:04:02.000Z",
    "summary": "The 'Contagious Interview' campaign, attributed to North Korea's Lazarus Group, compromises developer machines through fake job interviews delivering malicious npm packages and GitHub repositories that exfiltrate credentials and crypto wallet data.",
    "content": "Developers are getting compromised through job interviews. Not metaphorically — literally. A wave of sophisticated social engineering attacks, tracked under the campaign name \"Contagious Interview,\" has turned the hiring process into a malware delivery pipeline. And the fake job interview backdoor malware targeting developer machines in 2026 isn't a fringe threat. It's an active, coordinated operation with nation-state fingerprints all over it.\nThe attack works because it exploits something developers actually do: run code during technical interviews. A recruiter reaches out on LinkedIn. The job sounds real. The company looks real. Then comes the technical assessment — a GitHub repo, an npm package, a Next.js project to run locally. Except that package phones home, drops a backdoor, and your dev machine is compromised before you've finished the first task.\nWhy does this matter now? Because the scale has escalated sharply. Security researchers at Group-IB and Socket have attributed the Contagious Interview campaign to North Korean threat actors — specifically the Lazarus Group. Their targets aren't random. They're crypto developers, blockchain engineers, and anyone with access to high-value infrastructure or digital assets.\nKey Takeaways\nThe Contagious Interview campaign, attributed to North Korea's Lazarus Group by Group-IB researchers, targets crypto and blockchain developers through fake technical assessments delivered via npm packages and GitHub repos.\nDocumented malicious packages in these attacks deliver cross-platform backdoors — including BeaverTail and InvisibleFerret — capable of exfiltrating credentials, crypto wallet data, and browser session cookies.\nAttackers now build convincing fake company personas: complete with LinkedIn profiles, websites, and multi-stage interview processes that are indistinguishable from legitimate recruiting outreach.\nSocket's 2025 threat research identified over 20 malicious npm packages tied to this campaign, with some accumul",
    "category": "github",
    "translations": {
      "zh": {
        "title": "虚假工作面试后门恶意软件针对开发者机器",
        "summary": "据称由朝鲜拉扎勒斯集团组织的\"传染性面试\"活动通过虚假工作面试向开发者机器传递恶意npm包和GitHub存储库，窃取凭证和加密货币钱包数据。"
      },
      "fr": {
        "title": "Malware Backdoor Faux Entretien d'Embauche Ciblant les Machines des Développeurs",
        "summary": "La campagne 'Contagious Interview', attribuée au groupe Lazarus de la Corée du Nord, compromet les machines des développeurs par le biais de faux entretiens d'embauche livrant des packages npm malveillants et des référentiels GitHub qui exfiltrent les identifiants et les données des portefeuilles de crypto-monnaie."
      },
      "de": {
        "title": "Malware-Backdoor mit gefälschtem Vorstellungsgespräch für Entwicklermaschinen",
        "summary": "Die Kampagne \"Contagious Interview\", die der nordkoreanischen Lazarus-Gruppe zugeordnet wird, kompromittiert Entwicklermaschinen durch gefälschte Vorstellungsgespräche, die bösartige npm-Pakete und GitHub-Repositorys liefern, die Anmeldedaten und Kryptowallet-Daten exfiltrieren."
      },
      "es": {
        "title": "Malware Puerta Trasera de Entrevista Falsa Dirigida a Máquinas de Desarrolladores",
        "summary": "La campaña 'Contagious Interview', atribuida al grupo Lazarus de Corea del Norte, compromete máquinas de desarrolladores a través de falsas entrevistas de trabajo que entregan paquetes npm maliciosos y repositorios de GitHub que exfiltran credenciales y datos de billeteras de criptomonedas."
      }
    }
  },
  {
    "title": "You Can Download AI for Free...",
    "slug": "open-source-ai-free-download-expensive-run",
    "url": "https://dev.to/rawveg/you-can-download-ai-for-free-3ck9",
    "source": "DEV Community",
    "date": "2026-03-01T12:00:00.000Z",
    "summary": "Meta's LLaMA models have achieved over 1.2 billion downloads and spawned 85,000 derivatives on Hugging Face, narrowing the performance gap with proprietary AI. However, downloading open-source models remains free while running them at scale requires substantial computational infrastructure costs.",
    "content": "... But Running It Costs Thousands\n\n\nThe artificial intelligence industry stands at a crossroads. On one side, proprietary giants like OpenAI, Google, and Anthropic guard their model weights and training methodologies with the fervour of medieval warlords protecting castle secrets. On the other, a sprawling, chaotic, surprisingly powerful open-source movement is mounting an insurgency that threatens to democratise the most transformative technology since the internet itself.\nThe question isn't merely academic. It's existential for the future of AI: Can community-driven open-source infrastructure genuinely rival the proprietary stacks that currently dominate production-grade artificial intelligence? And perhaps more importantly, what governance structures and business models will ensure these open alternatives remain sustainable, safe, and equitably accessible to everyone, not just Silicon Valley elites?\nThe answer, as it turns out, is both more complex and more hopeful than you might expect.\nFor years, the conventional wisdom held that open-source AI would perpetually trail behind closed alternatives. Proprietary models like GPT-4 and Claude dominated benchmarks, whilst open alternatives struggled to keep pace. That narrative has fundamentally shifted.\nMeta's release of LLaMA models has catalysed a transformation in the open-source AI landscape. The numbers tell a compelling story: Meta's LLaMA family has achieved more than 1.2 billion downloads as of late 2024, with models being downloaded an average of one million times per day since the first release in February 2023. The open-source community has published over 85,000 LLaMA derivatives on Hugging Face alone, an increase of more than five times since the start of 2024.\nThe performance gap has narrowed dramatically. Code LLaMA with additional fine-tuning managed to beat GPT-4 in the HumanEval programming benchmark. LLaMA 2-70B and GPT-4 achieved near human-level performance of 84 per cent accuracy on fact-checking",
    "category": "github",
    "translations": {
      "zh": {
        "title": "你可以免费下载人工智能...",
        "summary": "Meta的LLaMA模型已实现超过12亿次下载，并在Hugging Face上衍生出85,000个衍生品，缩小与专有人工智能的性能差距。然而，下载开源模型仍然免费，但大规模运行它们需要大量计算基础设施成本。"
      },
      "fr": {
        "title": "Vous Pouvez Télécharger L'IA Gratuitement...",
        "summary": "Les modèles LLaMA de Meta ont atteint plus de 1,2 milliard de téléchargements et ont généré 85 000 dérivés sur Hugging Face, réduisant l'écart de performance avec l'IA propriétaire. Cependant, le téléchargement de modèles open-source reste gratuit alors que leur exécution à l'échelle nécessite des coûts d'infrastructure informatique substantiels."
      },
      "de": {
        "title": "Sie Können KI Kostenlos Herunterladen...",
        "summary": "Metas LLaMA-Modelle haben über 1,2 Milliarden Downloads erreicht und 85.000 Derivate auf Hugging Face hervorgebracht, wodurch die Leistungslücke mit proprietärer KI verringert wird. Das Herunterladen von Open-Source-Modellen bleibt jedoch kostenlos, während deren Ausführung im großen Maßstab erhebliche Kosten für Recheninfrastruktur erfordert."
      },
      "es": {
        "title": "Puedes Descargar IA Gratis...",
        "summary": "Los modelos LLaMA de Meta han alcanzado más de 1.200 millones de descargas y han generado 85.000 derivados en Hugging Face, reduciendo la brecha de rendimiento con la IA propietaria. Sin embargo, descargar modelos de código abierto sigue siendo gratuito, aunque ejecutarlos a escala requiere costos sustanciales de infraestructura computacional."
      }
    }
  },
  {
    "title": "The Feynman Technique: Master Any Subject with This Proven Learning Method",
    "slug": "feynman-technique-learning-method",
    "url": "https://dev.to/dekigk/the-feynman-technique-master-any-subject-with-this-proven-learning-method-ojo",
    "source": "DEV Community",
    "date": "2026-03-01T12:00:00.000Z",
    "summary": "The Feynman Technique is a learning framework emphasizing simplicity and clarity over memorization. Developed by physicist Richard Feynman, it enables deep understanding of complex subjects by explaining ideas in plain language accessible to non-experts.",
    "content": "I did not come up with the following ideas or definitions myself. It was always somewhere in between the lines of the blog posts I was reading. There is one specific thing I picked up over the years, and even put it as a tagline on this website. That is: \"Explain things like I am five\". Recently, I have found out that there is a name and a definition, hell, the whole technique for learning behind this simple saying. Let me introduce you to \"The Feynman Technique\" – perhaps the most effective learning method ever developed. This powerful learning strategy, used by Nobel Prize-winning physicist Richard Feynman, can dramatically improve how you learn and retain information.\nRichard Feynman was a very smart guy (source: Wikipedia):\nRichard Phillips Feynman (May 11, 1918 – February 15, 1988) was an American theoretical physicist, known for his work in the path integral formulation of quantum mechanics, the theory of quantum electrodynamics, the physics of the superfluidity of supercooled liquid helium, as well as his work in particle physics for which he proposed the parton model. For contributions to the development of quantum electrodynamics, Feynman received the Nobel Prize in Physics in 1965 jointly with Julian Schwinger and Shin'ichirō Tomonaga.\nBut, most of all, I like that he is known as \"The Great Explainer\". He developed a whole technique for explaining complex physics in plain and simple language. Take any field, and you will notice that it is very hard to explain complex technical terms and ideas to people who have no idea or knowledge about that field. That is why I am writing today about \"The Feynman Technique\".\nThe Feynman Technique is a powerful learning framework designed to achieve deep, comprehensive understanding of any subject. Developed by physicist Richard Feynman (nicknamed \"The Great Explainer\"), this method revolutionizes how we learn by focusing on simplicity and clarity. Unlike traditional memorization-based learning, the Feynman method forces",
    "category": "github",
    "translations": {
      "zh": {
        "title": "费曼技巧：用这种久经考验的学习方法掌握任何科目",
        "summary": "费曼技巧是一种学习框架，强调简洁性和清晰性而不是死记硬背。由物理学家理查德·费曼开发，它通过用非专家能理解的简单语言解释想法，使人能够深入理解复杂的科目。"
      },
      "fr": {
        "title": "La Technique de Feynman: Maîtriser N'Importe Quel Sujet avec Cette Méthode d'Apprentissage Éprouvée",
        "summary": "La technique de Feynman est un cadre d'apprentissage qui met l'accent sur la simplicité et la clarté plutôt que sur la mémorisation. Développée par le physicien Richard Feynman, elle permet une compréhension profonde de sujets complexes en expliquant les idées dans un langage simple accessible aux non-experts."
      },
      "de": {
        "title": "Die Feynman-Technik: Beherrsche Jedes Thema mit dieser bewährten Lernmethode",
        "summary": "Die Feynman-Technik ist ein Lernrahmen, der Einfachheit und Klarheit vor Auswendiglernen betont. Sie wurde vom Physiker Richard Feynman entwickelt und ermöglicht ein tiefes Verständnis komplexer Themen, indem Ideen in einer einfachen Sprache erklärt werden, die für Laien verständlich ist."
      },
      "es": {
        "title": "La Técnica de Feynman: Domina Cualquier Tema con Este Método de Aprendizaje Comprobado",
        "summary": "La Técnica de Feynman es un marco de aprendizaje que enfatiza la simplicidad y la claridad sobre la memorización. Desarrollada por el físico Richard Feynman, permite una comprensión profunda de temas complejos al explicar ideas en lenguaje simple accesible para no expertos."
      }
    }
  },
  {
    "title": "Exposing MCP from Legacy Java: Architecture Patterns That Actually Scale",
    "slug": "mcp-legacy-java-enterprise-architecture",
    "url": "https://dev.to/myfear/exposing-mcp-from-legacy-java-architecture-patterns-that-actually-scale-4bjh",
    "source": "DEV Community",
    "date": "2026-03-01T11:59:52.000Z",
    "summary": "Exposing MCP from legacy Jakarta EE systems requires a dedicated gateway layer that buffers AI traffic, enforces security policies, and translates tool calls into REST/SOAP interactions—preventing legacy systems from being overwhelmed by unpredictable AI request patterns.",
    "content": "Large Language Models are no longer just consumers of public APIs. With the Model Context Protocol (MCP), they become first-class clients of enterprise systems. This creates a new architectural challenge for the enterprise:\nHow do we expose MCP servers from legacy Jakarta EE applications without breaking the systems we rely on today?\nThis isn't about greenfield projects. This is about WildFly, Payara, WebLogic, SOAP endpoints, EJBs, and shared databases. \nThe key insight: MCP is not “just another API.” It changes traffic patterns, trust boundaries, and scaling behavior. Treating it like standard REST leads to failure modes you already know too well.\nTraditional integrations assume human-driven request rates and predictable shapes. MCP breaks these assumptions. An AI client can:\nInvoke many tools in rapid succession.\nCall internal operations humans never touch.\nGenerate load patterns that look like accidental DDoS.\nThe safest starting point is a dedicated MCP gateway implemented with a modern runtime like Quarkus.\n\nThe gateway terminates the MCP protocol and translates tool calls into legacy-friendly interactions (REST, SOAP, or even JCA). \nWhy this works:\nBuffer Zone: AI traffic is isolated from fragile legacy logic.\nIndependent Scaling: You can scale the gateway without touching the WebLogic cluster.\nSecurity: Policies are enforced before requests hit the core.\nA thin MCP tool mapped directly to an existing API. \n\nWarning: This exposes legacy design mistakes to the LLM, leading to \"token waste\" and expensive prompts.\nReshape legacy data into AI-friendly structures.\n\nsingle semantic object. This hides complexity and provides a clean place for caching.\nReads and writes behave differently under AI load. Writes deserve an asynchronous safety net.\n\nThis decouples AI timing from legacy processing. The trade-off? AI systems must accept eventual consistency.\nCan you expose MCP directly from Jakarta EE? Yes, using LangChain4j-CDI.\n\nWhen to use it:\nThe app is relatively mode",
    "category": "github",
    "translations": {
      "zh": {
        "title": "从遗留Java中暴露MCP：真正可扩展的架构模式",
        "summary": "从遗留Jakarta EE系统中暴露MCP需要一个专用网关层，该层缓冲AI流量、执行安全策略并将工具调用转换为REST/SOAP交互——防止遗留系统被不可预测的AI请求模式淹没。"
      },
      "fr": {
        "title": "Exposition MCP depuis Java hérité : Modèles d'architecture qui s'adaptent vraiment",
        "summary": "L'exposition de MCP à partir de systèmes Jakarta EE hérités nécessite une couche de passerelle dédiée qui tampon le trafic IA, applique les politiques de sécurité et traduit les appels d'outils en interactions REST/SOAP, empêchant les systèmes hérités d'être submergés par des modèles de demande IA imprévisibles."
      },
      "de": {
        "title": "MCP aus Legacy Java-Code freigeben: Architekturmuster, die wirklich skalieren",
        "summary": "Das Freigeben von MCP aus Legacy-Jakarta-EE-Systemen erfordert eine dedizierte Gateway-Schicht, die KI-Datenverkehr puffert, Sicherheitsrichtlinien durchsetzt und Tool-Aufrufe in REST/SOAP-Interaktionen übersetzt – um zu verhindern, dass Legacy-Systeme durch unvorhersehbare KI-Anfragemuster überfordert werden."
      },
      "es": {
        "title": "Exponer MCP desde Java heredado: Patrones de arquitectura que realmente escalan",
        "summary": "Exponer MCP desde sistemas heredados de Jakarta EE requiere una capa de puerta de enlace dedicada que amortigue el tráfico de IA, aplique políticas de seguridad y traduzca llamadas de herramientas en interacciones REST/SOAP, previniendo que los sistemas heredados sean abrumados por patrones de solicitud de IA impredecibles."
      }
    }
  },
  {
    "title": "Why Senior Java Developers Are Using AI Coding Tools Wrong",
    "slug": "senior-developers-ai-coding-tools-workflow",
    "url": "https://dev.to/myfear/why-senior-java-developers-are-using-ai-coding-tools-wrong-540",
    "source": "DEV Community",
    "date": "2026-03-01T11:59:00.000Z",
    "summary": "Senior developers often underutilize AI coding tools due to workflow issues rather than model limitations. 'Compounding Engineering'—actively guiding output, maintaining context in .md files, and clearing chats aggressively—significantly improves results and compounding productivity gains over time.",
    "content": "At NDC Manchester 2025, Aleksander Stensby gave one of the more honest talks I’ve seen about AI-assisted coding. It wasn't a hype-filled demo reel; it was a practical breakdown of why tools like Claude Code, Cursor, and GitHub Copilot often disappoint experienced developers—and how to fix it.\n\n\n\n\nThe core idea is simple, but uncomfortable:\n\nIf AI produces bad code for you, it’s often a workflow problem, not a model problem.\nStensby calls the fix Compounding Engineering. This isn't about \"prompt engineering.\" It’s about teaching your tools over time, the same way you onboard a junior developer to make them better week by week.\nThis is the mindset shift everything else depends on.\nIf a junior developer submits messy code, you don’t fire them. You review it, explain why it’s wrong, and show what “good” looks like. Most people don’t do this with AI. They accept the output, complain about “AI slop,” and move on.\nThe shift: Stop being a passive user. If you want better results, you must actively guide, correct, and push back. Your value doesn't disappear just because the code compiles; your value is in the mentorship of the tool.\nLarge language models (LLMs) don't have infinite attention. The mistake many Java devs make is trusting long-running, cluttered chats where the model eventually loses focus.\nThe fix is boring but effective:\nClear chats aggressively.\nStore state explicitly.\nUse Markdown for \"Memory\": Keep architecture notes, constraints, and rejected approaches in .md files. When starting a new session, re-load only the context that matters.\nFiles like CLAUDE.md or .cursorrules aren't just for styling. They are training manuals. \nWhen you fix a subtle bug or define a specific pattern for your Spring Boot service, don’t just fix it once. Update the rule file. Over time, the AI aligns with your specific standards, and those productivity gains compound.\nIf you let an AI jump straight into implementation, it will make architectural decisions for you—and some will be t",
    "category": "github",
    "translations": {
      "zh": {
        "title": "为什么高级Java开发人员使用AI编码工具的方式不对",
        "summary": "高级开发人员经常由于工作流问题而不是模型限制而低估AI编码工具。\"复合工程\"——主动指导输出、在.md文件中维护上下文以及积极清除聊天——显著改善结果并随时间推移产生复合生产力收益。"
      },
      "fr": {
        "title": "Pourquoi les développeurs Java seniors utilisent mal les outils de codage IA",
        "summary": "Les développeurs seniors sous-utilisent souvent les outils de codage IA en raison de problèmes de flux de travail plutôt que des limitations du modèle. \"L'ingénierie composée\" — guider activement les résultats, maintenir le contexte dans les fichiers .md et effacer agressivement les discussions — améliore considérablement les résultats et crée des gains de productivité composés au fil du temps."
      },
      "de": {
        "title": "Warum Senior-Java-Entwickler KI-Codierungswerkzeuge falsch verwenden",
        "summary": "Senior-Entwickler nutzen KI-Codierungswerkzeuge oft nicht vollständig, weil es Workflow-Probleme gibt, nicht weil es Modellbegrenzungen gibt. \"Verbundenes Engineering\" — aktive Anleitung der Ausgabe, Aufrechterhaltung des Kontexts in .md-Dateien und aggressives Löschen von Chats — verbessert die Ergebnisse erheblich und erzeugt über die Zeit hinweg zusammengesetzte Produktivitätsgewinne."
      },
      "es": {
        "title": "Por qué los desarrolladores senior de Java usan mal las herramientas de codificación con IA",
        "summary": "Los desarrolladores senior a menudo subutilizan las herramientas de codificación con IA debido a problemas de flujo de trabajo en lugar de limitaciones del modelo. \"Ingeniería compuesta\" — guiar activamente la salida, mantener el contexto en archivos .md y borrar agresivamente los chats — mejora significativamente los resultados y genera ganancias de productividad compuesta a lo largo del tiempo."
      }
    }
  },
  {
    "title": "Zero to Android App in 11 Minutes (No Coding Required)",
    "slug": "android-app-no-coding-11-minutes",
    "url": "https://dev.to/myougatheaxo/zero-to-android-app-in-11-minutes-no-coding-required-1ein",
    "source": "DEV Community",
    "date": "2026-03-01T11:52:12.000Z",
    "summary": "Using AI-generated Kotlin and Jetpack Compose templates with Material3 design, non-programmers can build fully functional Android apps in 11 minutes by customizing configuration values and running the project through Android Studio's play button.",
    "content": "What if I told you that you could have a working Android app on your phone in 11 minutes, with zero coding experience?\nHere's the exact process:\nA computer (any OS)\nAndroid Studio (free)\nAn Android phone\nA USB cable\nThat's it. No coding bootcamp. No CS degree. No YouTube tutorial marathon.\nDownload a complete Kotlin + Jetpack Compose project. It comes with:\nMaterial3 design (Google's latest design system)\nRoom database (offline data storage)\nMVVM architecture (industry standard)\nDark mode support\nOpen strings.xml. Find this line:\n<string name=\"app_name\">HabitTracker</string>\n\nChange HabitTracker to whatever you want. Save.\nOpen Theme.kt. Find the color values:\nval Purple80 = Color(0xFFD0BCFF)\n\nReplace with any hex color. Use Material Color Picker to choose.\nOpen the project in Android Studio. Click the green play button. Wait for Gradle to do its thing.\nEnable Developer Options (Settings > About > tap Build Number 7 times)\nEnable USB Debugging\nConnect USB cable\nClick Run in Android Studio\nDone. Your app is on your phone.\nI showed AI-generated Android code to a developer who's been writing code since the 1990s.\nHis review: \"That's actually correct.\"\nSpecifically:\nError handling for edge cases\nProper Room Entity definitions\nClean separation of concerns (DAO / Repository / ViewModel / Screen)\nMaterial3 compliance\nI've built 8 apps using this method:\n\n\n\nApp\nWhat It Does\nComplexity\n\n\n\n\nSplit Bill\nDivide expenses among friends\nStarter\n\n\nTimer\nCountdown with background support\nStarter\n\n\nUnit Converter\nOffline unit conversion\nStarter\n\n\nHabit Tracker\nDaily habit tracking with streaks\nStandard\n\n\nMeeting Timer\nAuto-generates meeting notes\nStandard\n\n\nExpense Memo\nReceipt tracking with CSV export\nStandard\n\n\nBudget Manager\nEnvelope budgeting system\nPremium\n\n\nTask Manager\nProjects with subtasks\nPremium\n\n\n\nStarbucks latte: $6.50 (gone in 5 minutes)\nOne app template: $9.99 (yours forever)\nAll 8 templates: $79.99 (save 47%)\nNo subscriptions. No ads. No tracking. You own the source co",
    "category": "github",
    "translations": {
      "zh": {
        "title": "11分钟从零到Android应用（无需编码）",
        "summary": "使用AI生成的Kotlin和Jetpack Compose模板以及Material3设计，非程序员可以通过自定义配置值并通过Android Studio的播放按钮运行项目，在11分钟内构建完全功能的Android应用。"
      },
      "fr": {
        "title": "Zéro à application Android en 11 minutes (aucun codage requis)",
        "summary": "En utilisant des modèles Kotlin et Jetpack Compose générés par l'IA avec la conception Material3, les non-programmeurs peuvent créer des applications Android entièrement fonctionnelles en 11 minutes en personnalisant les valeurs de configuration et en exécutant le projet via le bouton de lecture d'Android Studio."
      },
      "de": {
        "title": "Von Null zu Android App in 11 Minuten (kein Code erforderlich)",
        "summary": "Mit AI-generierten Kotlin- und Jetpack-Compose-Vorlagen und Material3-Design können Nicht-Programmierer vollständig funktionsfähige Android-Apps in 11 Minuten erstellen, indem sie Konfigurationswerte anpassen und das Projekt über die Play-Taste von Android Studio ausführen."
      },
      "es": {
        "title": "De cero a aplicación Android en 11 minutos (sin codificación requerida)",
        "summary": "Utilizando plantillas generadas por IA de Kotlin y Jetpack Compose con diseño Material3, los no programadores pueden construir aplicaciones Android completamente funcionales en 11 minutos personalizando valores de configuración y ejecutando el proyecto a través del botón de reproducción de Android Studio."
      }
    }
  },
  {
    "title": "Making London's hidden film clubs discoverable",
    "slug": "london-film-clubs-discoverable-clusterflick",
    "url": "https://dev.to/alistairjcbrown/i-built-a-film-club-discovery-tool-for-londons-cinema-community-2md",
    "source": "DEV Community",
    "date": "2026-03-01T11:50:52.000Z",
    "summary": "Clusterflick aggregates cinema listings across London and now includes dedicated pages for independent film clubs with accessibility information, making communities like Bar Trash and Lost Reels discoverable to audiences seeking beyond mainstream screenings.",
    "content": "This is a submission for the DEV Weekend Challenge: Community\nI've spent the last year building Clusterflick — a site that pulls together cinema listings from across London so you can see everything showing, everywhere, without jumping between a dozen different websites. It started as a personal itch: I just wanted to know what was on (for the backstory, see my intro post)\nBut the more I used it, the more I realised I was only solving half the problem. I could tell you what was showing at which venue — but I couldn't tell you if the screening was part of a film club, whether the club screenings were accessible, or even that the club existed at all. London has a genuinely brilliant film club scene: community cinemas, genre nights, archive screenings, disability-led clubs. Most of them are invisible unless you already know to look for them.\nThat felt wrong. These communities deserve better than a buried events page most people never find.\nTwo new features, both aimed at making London's film club community more discoverable.\nclusterflick.com/film-clubs gives each film club its own dedicated page. Each page shows their logo, a short description of who they are and what they programme, links back to their own site, and — crucially — pulls together their full upcoming lineup across all the venues they screen at. A lot of clubs move around; they're not tied to a single cinema. Clusterflick now reflects that.\nTo give a sense of the range:\nBar Trash programmes cult and curiosity films for people who've exhausted the mainstream;\nPitchblack Playback runs immersive listening sessions in the dark, using cinema sound systems the way most people never get to hear them;\nand Lost Reels specialises in bringing forgotten, lost, or otherwise unavailable films back to UK screens.\nThree very different clubs, all doing something you won't find on a standard listings site, and all working across multiple venues.\nI also included accessibility information on each club page, surfaced directly",
    "category": "github",
    "translations": {
      "zh": {
        "title": "让伦敦隐藏的电影俱乐部易于发现",
        "summary": "Clusterflick聚合伦敦各地的电影放映信息，现在包括为独立电影俱乐部提供的专页和无障碍信息，使Bar Trash和Lost Reels等社区对寻求超越主流放映的观众来说易于发现。"
      },
      "fr": {
        "title": "Rendre les clubs de cinéma cachés de Londres découvrables",
        "summary": "Clusterflick agrège les listes de cinémas à travers Londres et inclut maintenant des pages dédiées aux clubs de cinéma indépendants avec des informations d'accessibilité, rendant des communautés comme Bar Trash et Lost Reels découvrables pour les audiences cherchant au-delà des projections grand public."
      },
      "de": {
        "title": "Londons verborgene Filmclubs auffindbar machen",
        "summary": "Clusterflick aggregiert Kinoprogramme in London und bietet nun dedizierte Seiten für unabhängige Filmclubs mit Barrierefreiheitsinformationen, wodurch Gemeinschaften wie Bar Trash und Lost Reels für Zuschauer sichtbar werden, die über Mainstream-Screenings hinaus schauen."
      },
      "es": {
        "title": "Haciendo que los cines independientes ocultos de Londres sean descubribles",
        "summary": "Clusterflick agrega carteleras de cines en toda Londres e incluye ahora páginas dedicadas para cines independientes con información de accesibilidad, permitiendo que comunidades como Bar Trash y Lost Reels sean descubribles para audiencias que buscan más allá de proyecciones convencionales."
      }
    }
  },
  {
    "title": "Claude Code vs Cursor vs GitHub Copilot: Which AI Coding Tool Should You Use in 2026?",
    "slug": "ai-coding-tools-comparison-2026",
    "url": "https://dev.to/myougatheaxo/claude-code-vs-cursor-vs-github-copilot-which-ai-coding-tool-should-you-use-in-2026-46o6",
    "source": "DEV Community",
    "date": "2026-03-01T11:50:05.000Z",
    "summary": "Claude Code excels at generating complete projects in 47 seconds, Cursor at incremental modifications in existing codebases (15-20 min), and GitHub Copilot at daily line-level productivity completions (40-60 min). Each tool serves distinct development workflows rather than directly competing.",
    "content": "The AI coding tool landscape in 2026 has three major players:\nClaude Code - Anthropic's CLI-based coding agent\nCursor - AI-native VS Code fork\nGitHub Copilot - GitHub's AI pair programmer\nI tested all three on the same task: build a complete Android app from scratch.\nBuild a habit tracker Android app with:\nKotlin + Jetpack Compose\nMaterial3 design\nRoom database for persistence\nDark mode support\n\n\n\nTool\nSetup\nTime\n\n\n\n\nClaude Code\nnpm install -g @anthropic-ai/claude-code\n2 min\n\n\nCursor\nInstaller + API key + extensions\n10 min\n\n\nCopilot\nVS Code extension + GitHub auth\n5 min\n\n\n\nGenerated a complete project with MVVM + Repository pattern. Includes:\nRoom Entity with proper annotations\nDAO with Flow-based queries\nRepository layer with error handling\nViewModel with state management\nComposable screens with Material3\nThe surprising part: Claude Code asked questions before writing code:\n\"Who uses this app?\"\n\"What's the primary action within 10 seconds?\"\n\"Does it need offline support?\"\nThis is design thinking, not just code generation.\nGenerated code file-by-file. Good at Composable functions, but you need to:\nDefine the architecture yourself\nCreate files manually\nConnect the pieces\nExcellent at line-by-line and function-level completion. Not designed for whole-project generation. Best when you already have a codebase.\n\n\n\nTool\nTime\nManual Work\n\n\n\n\nClaude Code\n47 seconds\nAlmost none\n\n\nCursor\n15-20 min\nFile structure, dependencies\n\n\nCopilot\n40-60 min\nEverything except line completion\n\n\n\n\n\n\nTool\nArchitecture\nAuto-selected?\n\n\n\n\nClaude Code\nMVVM + Repository\nYes (asks questions first)\n\n\nCursor\nWhatever you specify\nNo (manual)\n\n\nCopilot\nNone (completion only)\nN/A\n\n\n\n\n\n\nUse Case\nBest Tool\n\n\n\n\nBuild from scratch\nClaude Code\n\n\nModify existing project\nCursor\n\n\nDaily coding productivity\nGitHub Copilot\n\n\nNon-engineers building apps\nClaude Code\n\n\n\nThese tools aren't competing - they serve different purposes.\nClaude Code is an architect that builds entire projects\nCursor is a collaborator tha",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Claude Code vs Cursor vs GitHub Copilot：2026年你应该使用哪个AI编程工具？",
        "summary": "Claude Code擅长在47秒内生成完整项目，Cursor擅长对现有代码库进行增量修改（15-20分钟），GitHub Copilot擅长每日行级生产力补全（40-60分钟）。每个工具服务于不同的开发工作流，而不是直接竞争。"
      },
      "fr": {
        "title": "Claude Code vs Cursor vs GitHub Copilot : Quel outil de codage IA devriez-vous utiliser en 2026 ?",
        "summary": "Claude Code excelle dans la génération de projets complets en 47 secondes, Cursor dans les modifications incrémentielles des bases de code existantes (15-20 min), et GitHub Copilot dans les complétions de productivité au niveau des lignes quotidiennes (40-60 min). Chaque outil sert des flux de travail de développement distincts plutôt que de concurrencer directement."
      },
      "de": {
        "title": "Claude Code vs Cursor vs GitHub Copilot: Welches KI-Programmiertool sollten Sie 2026 verwenden?",
        "summary": "Claude Code zeichnet sich durch die Erzeugung vollständiger Projekte in 47 Sekunden aus, Cursor durch inkrementelle Änderungen in bestehenden Codebases (15-20 Min.), und GitHub Copilot durch tägliche produktive Codevervollständigungen auf Zeilenebene (40-60 Min.). Jedes Tool bedient unterschiedliche Entwicklungs-Workflows, anstatt direkt zu konkurrieren."
      },
      "es": {
        "title": "Claude Code vs Cursor vs GitHub Copilot: ¿Qué herramienta de codificación con IA deberías usar en 2026?",
        "summary": "Claude Code destaca en la generación de proyectos completos en 47 segundos, Cursor en modificaciones incrementales de bases de código existentes (15-20 min), y GitHub Copilot en completaciones de productividad a nivel de línea diarias (40-60 min). Cada herramienta sirve flujos de trabajo de desarrollo distintos en lugar de competir directamente."
      }
    }
  },
  {
    "title": "Alfred - Your learning companion.",
    "slug": "alfred-learning-companion-knowledge-management",
    "url": "https://dev.to/nowaysid/alfred-your-learning-companion-19ea",
    "source": "DEV Community",
    "date": "2026-03-01T11:48:59.000Z",
    "summary": "Alfred is an AI-powered knowledge management tool that captures insights from text, audio, and images, automatically organizes them with semantic clustering, and generates daily spaced-repetition revision reports to improve long-term learning retention.",
    "content": "This is a submission for the DEV Weekend Challenge: Community\nAlfred is built for students, lifelong learners, and knowledge workers — anyone who constantly absorbs information from lectures, podcasts, articles, and conversations but struggles to retain it all. Inspired by the \"capture once, revise forever\" philosophy, Alfred serves the community of people who believe learning doesn't stop after the first encounter with an idea. Whether you're a university student juggling multiple subjects, a developer keeping up with new technologies, or a curious mind exploring diverse topics, Alfred ensures nothing you learn is ever forgotten.\nAlfred is an AI-powered personal knowledge management and learning assistant, themed after Alfred Pennyworth (Batman's butler). It captures daily insights from text, audio, and images, automatically organizes them into a searchable knowledge base, generates spaced-repetition revision reports, and provides a conversational AI chat interface — all to help users retain and deepen what they learn. You get daily reports from your knowledge base from the past 1,3,5 and 7 days.\nKey features include:\nMulti-Modal Capture — Type notes, record/upload audio, snap/upload images from a gesture-driven mobile app\nAI Processing Pipeline — Audio transcription (Deepgram Nova-3), image OCR (OCR.space), semantic clustering, topic classification, web research enrichment, and vector embedding\nSpaced Repetition Reports — A daily pipeline retrieves chunks from 1, 3, 5, and 7 days ago, groups them by topic, and generates revision reports in a randomized \"Alfred Pennyworth\" personality tone\nFlashcards — Anki-style cards with Again/Good/Easy grading, generated from your captured knowledge\nRAG Chat — Ask Alfred anything — your question is embedded, relevant memories and knowledge are retrieved via semantic search, and a context-aware answer is generated\nVoice & Live Chat — Multimodal audio input and real-time bidirectional voice streaming via Gemini\nWeb Dashboard — Br",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Alfred - 你的学习伴侣",
        "summary": "Alfred是一个AI驱动的知识管理工具，可以从文本、音频和图像中捕获见解，通过语义聚类自动组织它们，并生成每日间隔重复复习报告以提高长期学习保留。"
      },
      "fr": {
        "title": "Alfred - Votre compagnon d'apprentissage",
        "summary": "Alfred est un outil de gestion des connaissances basé sur l'IA qui capture des perspectives à partir de texte, audio et images, les organise automatiquement avec un clustering sémantique, et génère des rapports de révision à espacement répétitif quotidiens pour améliorer la rétention d'apprentissage à long terme."
      },
      "de": {
        "title": "Alfred - Dein Lernbegleiter",
        "summary": "Alfred ist ein KI-gestütztes Wissensmanagementsystem, das Erkenntnisse aus Text, Audio und Bildern erfasst, diese automatisch durch semantisches Clustering organisiert und tägliche Wiederholungsberichte generiert, um die langfristige Lernretention zu verbessern."
      },
      "es": {
        "title": "Alfred - Tu compañero de aprendizaje",
        "summary": "Alfred es una herramienta de gestión del conocimiento impulsada por IA que captura información de texto, audio e imágenes, las organiza automáticamente con agrupamiento semántico, y genera informes de revisión espaciada diaria para mejorar la retención del aprendizaje a largo plazo."
      }
    }
  },
  {
    "title": "Reverse array in groups",
    "slug": "reverse-array-in-groups",
    "url": "https://dev.to/tanzimsafin_42/reverse-array-in-groups-3dj5",
    "source": "DEV Community",
    "date": "2026-03-01T06:08:15.000Z",
    "summary": "Tutorial on reversing array elements in chunks of a specified size k, with implementation details, O(n) time complexity analysis, and practical examples showing the algorithm's behavior on sample data.",
    "content": "Original Problem: Reverse an Array in Groups of Given Size\nWe want to reverse an array in groups of size k. Think of the array as being split into consecutive chunks (windows) of length k, and we reverse each chunk one by one.\nStart from index 0.\nTake the next k elements as a chunk: array[i : i+k].\nReverse only that chunk.\nJump to the next chunk by moving i forward by k.\nLoop through the array in steps of k:\ni = 0, k, 2k, 3k, ...\n\n\nReverse the current chunk:\narray[i : i+k] with its reversed version.\n\n\nHandle the edge case (last chunk smaller than k):\nk elements remain, array[i : i+k] simply returns the remaining elements.\n\n\n\n\n  \n  \n  Implementation\n\n\n\n\n\ndef reverse_in_groups(arr, k):\n    n = len(arr)\n    for i in range(0, n, k):\n        # Reverse the sub-array from i to i+k\n        arr[i:i+k] = reversed(arr[i:i+k])\n    return arr\n\nTotal time is \n\n\n  O(n)O(n) O(n)\n\n because each element is visited at most twice (once during iteration and once during reversal).\nArray: [1, 2, 4, 5, 7]\nk = 3\nChunk 1: [1, 2, 4] → [4, 2, 1]\n\nChunk 2 (edge case): [5, 7] → [7, 5]\n\n\n\nResult: [4, 2, 1, 7, 5]",
    "category": "github",
    "translations": {
      "zh": {
        "title": "按组反转数组",
        "summary": "关于按指定大小k分块反转数组元素的教程，包含实现细节、O(n)时间复杂度分析以及展示算法在样本数据上表现的实践例子。"
      },
      "fr": {
        "title": "Inverser un tableau par groupes",
        "summary": "Tutoriel sur l'inversion des éléments du tableau par tranches d'une taille spécifiée k, avec détails de mise en œuvre, analyse de la complexité temporelle O(n), et exemples pratiques montrant le comportement de l'algorithme sur les données d'exemple."
      },
      "de": {
        "title": "Array in Gruppen umkehren",
        "summary": "Anleitung zum Umkehren von Array-Elementen in Chunks einer angegebenen Größe k, mit Implementierungsdetails, O(n)-Zeitkomplexitätsanalyse und praktischen Beispielen, die das Verhalten des Algorithmus bei Beispieldaten zeigen."
      },
      "es": {
        "title": "Invertir matriz en grupos",
        "summary": "Tutorial sobre la inversión de elementos de matriz en fragmentos de un tamaño especificado k, con detalles de implementación, análisis de complejidad de tiempo O(n), y ejemplos prácticos que muestran el comportamiento del algoritmo en datos de muestra."
      }
    }
  },
  {
    "title": "16 Patterns for Crossing the WebAssembly Boundary (And the One That Wants to Kill Them All)",
    "slug": "webassembly-boundary-crossing-patterns",
    "url": "https://dev.to/rafacalderon/16-patterns-for-crossing-the-webassembly-boundary-and-the-one-that-wants-to-kill-them-all-5kb",
    "source": "DEV Community",
    "date": "2026-03-01T06:06:22.000Z",
    "summary": "Comprehensive catalog of 16 patterns for managing data exchange across WebAssembly/JavaScript boundaries, organizing strategies into primitives, memory management, and flow architecture to optimize performance costs of WASM integration.",
    "content": "WebAssembly is fast. We all know that by now. What almost nobody talks about is the hidden toll you pay every time you try to talk to it.\nThe moment your JavaScript code needs to pass a measly string to a WASM module, or your WASM tries to touch a DOM node, you slam face-first into the boundary — a literal wall between two worlds with fundamentally opposed type systems, memory models, and execution paradigms. On one side, JS breathes UTF-16 strings, garbage-collected live objects, and async promises. On the other, WASM is spartan: it only understands numeric primitives like i32 or f64, raw linear memory, and strictly synchronous execution.\nCrossing this boundary is never free. Every interaction has a price, and depending on the strategy you choose to pay it, that cost can range from mathematically negligible to a painful \"why on earth did I bother compiling this to WASM?\"\nWhat you're about to read is the definitive catalog of every known pattern for crossing this boundary, from the most trivial to the most exotic. To make sense of it all, I've organized them into three fundamental blocks based on the exact question they answer:\nBlock 1 — The Primitives: What things can actually cross the boundary and how do they do it?\nBlock 2 — Memory Strategies: How do you move heavy data efficiently without killing performance?\nBlock 3 — Flow Architectures: How do you orchestrate and design the conversation between both sides?\nAnd to close, we'll talk about the Component Model — the emerging standard that aspires to turn all of these patterns into museum pieces.\nWhat can cross the boundary, and how?\nBefore optimizing anything, you need to understand what can actually travel between the two worlds. WebAssembly's binary interface (ABI) is minimalist: numbers in, numbers out. Everything else — strings, objects, callbacks, DOM references — requires a translation layer.\nThe five patterns in this block are the foundation. Every advanced technique in the later blocks is built on top of",
    "category": "github",
    "translations": {
      "zh": {
        "title": "16种跨越WebAssembly边界的模式（以及那个想消灭它们的模式）",
        "summary": "管理WebAssembly/JavaScript边界上数据交换的16种模式的综合目录，将策略组织为基元、内存管理和流架构，以优化WASM集成的性能成本。"
      },
      "fr": {
        "title": "16 modèles pour traverser la limite WebAssembly (Et celui qui veut les tuer tous)",
        "summary": "Catalogue complet de 16 modèles pour gérer l'échange de données à travers les limites WebAssembly/JavaScript, organisant les stratégies en primitives, gestion de la mémoire et architecture de flux pour optimiser les coûts de performance de l'intégration WASM."
      },
      "de": {
        "title": "16 Muster zum Überschreiten der WebAssembly-Grenze (Und das eine, das sie alle töten will)",
        "summary": "Umfassender Katalog von 16 Mustern für die Verwaltung des Datenaustausches über WebAssembly/JavaScript-Grenzen hinweg, die Strategien in Primitive, Speicherverwaltung und Flow-Architektur organisieren, um die Leistungskosten der WASM-Integration zu optimieren."
      },
      "es": {
        "title": "16 patrones para cruzar el límite de WebAssembly (Y el que quiere matarlos a todos)",
        "summary": "Catálogo completo de 16 patrones para gestionar el intercambio de datos a través de los límites de WebAssembly/JavaScript, organizando estrategias en primitivas, gestión de memoria y arquitectura de flujo para optimizar los costos de rendimiento de la integración de WASM."
      }
    }
  },
  {
    "title": "FeatureDrop v3 — Your App Now Decides When and How to Show Features",
    "slug": "featuredrop-v3-client-side-adoption",
    "url": "https://dev.to/thegdsks/featuredrop-v3-your-app-now-decides-when-and-how-to-show-features-588o",
    "source": "DEV Community",
    "date": "2026-03-01T05:58:01.000Z",
    "summary": "FeatureDrop v3 is a 4 kB open-source client-side behavioral engine that automatically selects optimal feature announcement formats from session one, eliminating expensive server-side analytics while preserving user experience.",
    "content": "GitHub | Docs | npm | shadcn Components | Example App\nEvery product adoption tool — Pendo, Appcues, Chameleon — works the same way: collect weeks of server-side analytics, then tell you which users to nudge. They charge $250–$7,000/month for it.\nWe built a 4 kB client-side engine that makes the same decisions from session one. No server. No data collection. MIT licensed.\nThis is FeatureDrop v3. (v2 launch post here if you want the backstory — it blew up, so I kept building.)\nTL;DR:\nClient-side behavioral engine, 4 kB, zero servers\nPicks the right format (badge / toast / modal) per user automatically\nFree, MIT licensed, 479 tests, works with Next.js / Remix / Astro / Nuxt + shadcn\nEvery product adoption tool works the same way: you define a feature, pick a format (badge, modal, toast), set a date, and hope for the best.\nBut your power users don't need a modal. Your new users don't need a tiny badge. Someone who just dismissed three announcements doesn't want a fourth.\nPendo, Appcues, and Chameleon solve this with server-side analytics pipelines that take weeks to collect enough data.\nWe solved it with client-side analytics — in 4 kB, from the first session.\nBefore diving into the how — here's what v3 ships:\n\n\n\nMetric\nv2.7\nv3.0\n\n\n\n\nCore bundle (gzip)\n3.01 kB\n3.02 kB\n\n\nEngine bundle (gzip)\n—\n4.08 kB\n\n\nTests\n374\n479\n\n\nSubpath exports\n20\n26\n\n\nFramework integrations\n7\n11\n\n\nshadcn components\n0\n5\n\n\nCLI commands\n9\n10\n\n\n\nThe core didn't grow. Zero runtime dependencies. The engine is an opt-in 4 kB add-on. If you don't import it, you don't pay for it.\nnpm install featuredrop@3\n\nThe AdoptionEngine is a plugin that tracks behavior locally and makes smart decisions:\n┌─────────────────────────────────────────┐\n│           AdoptionEngine                │\n├──────────────┬──────────────────────────┤\n│ Behavior     │ Tracks sessions, dismiss │\n│ Tracker      │ patterns, engagement     │\n├──────────────┼──────────────────────────┤\n│ Timing       │ Cooldowns, fatigue       │\n│ Optimizer",
    "category": "github",
    "translations": {
      "zh": {
        "title": "FeatureDrop v3 — 你的应用现在决定何时以及如何展示功能",
        "summary": "FeatureDrop v3是一个4 kB的开源客户端行为引擎，从第一个会话开始自动选择最优的功能公告格式，消除昂贵的服务器端分析，同时保持用户体验。"
      },
      "fr": {
        "title": "FeatureDrop v3 — Votre application décide maintenant quand et comment montrer les fonctionnalités",
        "summary": "FeatureDrop v3 est un moteur comportemental côté client open-source de 4 kB qui sélectionne automatiquement les formats d'annonce de fonctionnalités optimaux dès la première session, éliminant les analyses coûteuses côté serveur tout en préservant l'expérience utilisateur."
      },
      "de": {
        "title": "FeatureDrop v3 — Ihre App entscheidet jetzt, wann und wie Funktionen angezeigt werden",
        "summary": "FeatureDrop v3 ist eine 4-kB-Open-Source-Client-seitige Behavioral Engine, die automatisch die optimalen Formate für Funktionsankündigungen ab der ersten Sitzung auswählt, teure serverseitige Analysen eliminiert und gleichzeitig die Benutzererfahrung bewahrt."
      },
      "es": {
        "title": "FeatureDrop v3 — Tu aplicación ahora decide cuándo y cómo mostrar características",
        "summary": "FeatureDrop v3 es un motor de comportamiento del lado del cliente de código abierto de 4 kB que selecciona automáticamente los formatos óptimos de anuncio de características desde la primera sesión, eliminando análisis costosos del lado del servidor mientras preserva la experiencia del usuario."
      }
    }
  },
  {
    "title": "Phase 3 - Deploying a Custom App to Azure Kubernetes Service Using Azure Container Registry",
    "slug": "deploying-app-aks-azure-container-registry",
    "url": "https://dev.to/pilgrim2go/phase-3-deploying-a-custom-app-to-azure-kubernetes-service-using-azure-container-registry-12mk",
    "source": "DEV Community",
    "date": "2026-03-01T05:56:24.000Z",
    "summary": "Production workflow guide for deploying containerized applications to AKS using Azure Container Registry, demonstrating cloud-native infrastructure patterns including ACR, AKS, managed identity, and public service exposure.",
    "content": "🚀\n\n\nIn this lab, we will:\nCreate an Azure Container Registry (ACR)\n\nBuild a custom Docker image in the cloud\nDeploy it to Azure Kubernetes Service (AKS)\n\nExpose it publicly using a LoadBalancer\nThis is a production-style workflow used by modern cloud-native teams.\nDeveloper → ACR → AKS → Service (LoadBalancer) → Public IP\n\nWe use:\nAzure Kubernetes Service (AKS)\nAzure Container Registry (ACR)\nManaged Identity for secure image pulls\nYou already created AKS:\naz aks create \\\n  --resource-group aks-east2-rg \\\n  --name aks-prod-east2 \\\n  --location eastus2 \\\n  --node-count 2 \\\n  --network-plugin azure \\\n  --enable-managed-identity \\\n  --enable-oidc-issuer \\\n  --enable-workload-identity \\\n  --enable-addons monitoring\n\nSet variables:\nRG=aks-east2-rg\nCLUSTER=aks-prod-east2\nACR_NAME=akseast2acr$RANDOM\nLOCATION=eastus2\n\nNew subscriptions often need manual provider registration:\naz provider register --namespace Microsoft.ContainerRegistry\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.Compute\n\nVerify:\naz provider list --query \"[?registrationState!='Registered']\"\n\naz acr create \\\n  --resource-group $RG \\\n  --name $ACR_NAME \\\n  --sku Standard \\\n  --location $LOCATION\n\nAttach ACR to AKS (important for image pull permissions):\naz aks update \\\n  --name $CLUSTER \\\n  --resource-group $RG \\\n  --attach-acr $ACR_NAME\n\nThis configures managed identity-based access.\nCreate project folder:\nmkdir hello-aks && cd hello-aks\n\napp.js\n\n\n\n\n\nconst http = require('http');\n\nconst server = http.createServer((req, res) => {\n  res.writeHead(200);\n  res.end(\"Hello from AKS + ACR 🚀\");\n});\n\nserver.listen(3000);\n\npackage.json\n\n\n\n\n\n{\n  \"name\": \"hello-aks\",\n  \"version\": \"1.0.0\",\n  \"main\": \"app.js\"\n}\n\nDockerfile\n\n\n\n\n\nFROM node:18-alpine\nWORKDIR /app\nCOPY . .\nEXPOSE 3000\nCMD [\"node\", \"app.js\"]\n\nSince Azure Cloud Shell doesn’t support Docker daemon, use:\naz acr build \\\n  --registry $ACR_NAME \\\n  --image h",
    "category": "github",
    "translations": {
      "zh": {
        "title": "第三阶段 - 使用Azure Container Registry将自定义应用部署到Azure Kubernetes Service",
        "summary": "使用Azure Container Registry将容器化应用部署到AKS的生产工作流指南,展示云原生基础设施模式,包括ACR、AKS、托管身份和公共服务暴露。"
      },
      "fr": {
        "title": "Phase 3 - Déploiement d'une application personnalisée sur Azure Kubernetes Service à l'aide d'Azure Container Registry",
        "summary": "Guide de flux de travail en production pour le déploiement d'applications conteneurisées sur AKS à l'aide d'Azure Container Registry, démontrant les modèles d'infrastructure cloud-native comprenant ACR, AKS, identité gérée et exposition de service public."
      },
      "de": {
        "title": "Phase 3 - Bereitstellung einer benutzerdefinierten Anwendung auf Azure Kubernetes Service mit Azure Container Registry",
        "summary": "Produktionsworkflow-Leitfaden für die Bereitstellung containerisierter Anwendungen auf AKS mit Azure Container Registry, demonstriert Cloud-Native-Infrastrukturmuster einschließlich ACR, AKS, verwalteter Identität und öffentlicher Service-Exposition."
      },
      "es": {
        "title": "Fase 3 - Implementación de una aplicación personalizada en Azure Kubernetes Service usando Azure Container Registry",
        "summary": "Guía de flujo de trabajo de producción para implementar aplicaciones contenerizadas en AKS usando Azure Container Registry, demostrando patrones de infraestructura cloud-native incluyendo ACR, AKS, identidad administrada y exposición de servicio público."
      }
    }
  },
  {
    "title": "🐾 Purrsona: An Extension to Help You Interact Without Overthinking",
    "slug": "purrsona-comment-refinement-extension",
    "url": "https://dev.to/bytethecarrot/purrsona-an-extension-to-help-you-interact-without-overthinking-35hl",
    "source": "DEV Community",
    "date": "2026-03-01T05:55:50.000Z",
    "summary": "Chrome extension using Google's Gemini 2.5 Flash to refine developer comments while preserving authentic voice through personalized style onboarding, reducing participation hesitation through AI-assisted writing enhancement.",
    "content": "This is a submission for the DEV Weekend Challenge: Community\nThis project is for developers who want to participate more but hesitate before they hit post.\nSometimes I read a great article, I have thoughts, I want to comment… and then I overthink it. Is this clear enough? Does this sound dumb? Is this too long? Is this too robotic? And then I either rewrite it three times or do not post at all 🥲\n\nThere are many AI writing tools, but most of them flatten your voice. They polish everything into the same generic tone. That did not help me. I did not want to sound like AI. I just wanted to sound like myself, just clearer and more confident.\nSo I built something for people like me who love the dev.to community and want to engage more but get stuck in their own heads.\nPurrsona is a Chrome extension that uses Google's Gemini AI to help developers transform their rough thoughts into polished comments while keeping their authentic voice.\nKey features:\nAI-powered comment refinement using Gemini 2.5 Flash\nTone options: Professional, Friendly, Curious, Technical, or Confident\nLength options: Short, Balanced, or Detailed\nRetro pixel cat-themed UI with satisfying sound effects\nStyle onboarding so the AI learns your voice instead of replacing it\n\n\n\nDuring onboarding, you answer two short questions:\nYour honest opinion about a tech topic\nAn explanation of a technical concept\nThese answers define your Purrsona so the AI preserves your writing rhythm instead of making everything sound robotic.\n\nYou set up your own Gemini API key.\nchrome.storage.\nThere is also a small personality layer built into the experience:\nButtons make a soft click sound\nWhile a request is processing, you hear a subtle purring sound 😆\nWhen refinement succeeds, you get a tiny meow\n\nI actually used Purrsona to post my first comment on this weekendchallenge submission 😄\n\n\nGitHub Repository: https://github.com/carr-o-t/purrsona\nWant to try it out? Here is how to load an unpacked extension in Chrome:\nClone the re",
    "category": "github",
    "translations": {
      "zh": {
        "title": "🐾 Purrsona: 帮助您无需过度思考就能互动的扩展程序",
        "summary": "Chrome扩展程序,使用Google的Gemini 2.5 Flash通过个性化风格入门来优化开发者评论,同时保留真实声音,通过AI辅助写作增强来减少参与犹豫。"
      },
      "fr": {
        "title": "🐾 Purrsona: Une extension pour vous aider à interagir sans trop réfléchir",
        "summary": "Extension Chrome utilisant Gemini 2.5 Flash de Google pour affiner les commentaires des développeurs tout en préservant la voix authentique grâce à l'intégration de style personnalisé, réduisant l'hésitation à participer grâce à l'amélioration de l'écriture assistée par IA."
      },
      "de": {
        "title": "🐾 Purrsona: Eine Erweiterung, um ohne Überdenken zu interagieren",
        "summary": "Chrome-Erweiterung mit Google's Gemini 2.5 Flash zur Verfeinerung von Entwicklerkommentaren unter Beibehaltung der authentischen Stimme durch personalisiertes Style-Onboarding, wodurch Zögerlichkeit bei der Teilnahme durch KI-unterstützte Schreibverbesserung reduziert wird."
      },
      "es": {
        "title": "🐾 Purrsona: Una extensión para ayudarte a interactuar sin pensar demasiado",
        "summary": "Extensión de Chrome que utiliza Gemini 2.5 Flash de Google para refinar comentarios de desarrolladores mientras preserva la voz auténtica a través de incorporación de estilo personalizado, reduciendo la vacilación para participar mediante mejora de escritura asistida por IA."
      }
    }
  },
  {
    "title": "Phase 5 - AKS with Azure DNS + NGINX Ingress + cert-manager",
    "slug": "aks-azure-dns-nginx-cert-manager",
    "url": "https://dev.to/pilgrim2go/phase-4-aks-with-azure-dns-nginx-ingress-cert-manager-i9",
    "source": "DEV Community",
    "date": "2026-03-01T05:55:38.000Z",
    "summary": "Production HTTPS setup guide for AKS combining Azure DNS, NGINX Ingress Controller, cert-manager, and Let's Encrypt, including DNS configuration, TLS automation, and certificate validation processes.",
    "content": "In this lab, we built a production-grade HTTPS setup for applications running on:\nAzure Kubernetes Service\nAzure DNS\nNGINX Ingress Controller\ncert-manager\nLet's Encrypt\nWe exposed:\nhello.az.innopy.dev\napi.az.innopy.dev\nAnd secured them with valid public TLS certificates.\nThis guide includes:\n✅ Azure DNS zone setup\nUser (HTTPS)\n   ↓\nAzure Public IP\n   ↓\nAzure Load Balancer\n   ↓\nNGINX Ingress Controller\n   ↓\nKubernetes Services\n   ↓\nPods\n\nFor certificate issuance:\ncert-manager\n   ↓\nLet's Encrypt (HTTP-01 challenge)\n   ↓\nTemporary solver ingress\n   ↓\nValidation\n   ↓\nTLS Secret stored in cluster\n\nIf you don’t already have the zone:\naz network dns zone create \\\n  --resource-group <dns-rg> \\\n  --name az.innopy.dev\n\nVerify:\naz network dns zone list -o table\n\nGet your NGINX public IP:\nkubectl get svc -n ingress-nginx\n\nSet variables:\nRG_DNS=<dns-rg>\nDNS_ZONE=az.innopy.dev\nINGRESS_IP=<public-ip>\n\nCreate A record for hello:\naz network dns record-set a create \\\n  --resource-group $RG_DNS \\\n  --zone-name $DNS_ZONE \\\n  --name hello \\\n  --ttl 300\n\naz network dns record-set a add-record \\\n  --resource-group $RG_DNS \\\n  --zone-name $DNS_ZONE \\\n  --record-set-name hello \\\n  --ipv4-address $INGRESS_IP\n\nRepeat for api:\naz network dns record-set a create \\\n  --resource-group $RG_DNS \\\n  --zone-name $DNS_ZONE \\\n  --name api \\\n  --ttl 300\n\naz network dns record-set a add-record \\\n  --resource-group $RG_DNS \\\n  --zone-name $DNS_ZONE \\\n  --record-set-name api \\\n  --ipv4-address $INGRESS_IP\n\nVerify propagation:\ndig +short hello.az.innopy.dev\ndig +short api.az.innopy.dev\n\nMust return your ingress public IP.\nInstall CRDs and controller (recommended via Helm in production).\nVerify:\nkubectl get pods -n cert-manager\n\nAll pods must be Running.\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-production\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: admin@innopy.dev\n    privateKeySecretRef:\n      name: letsencrypt-production-accou",
    "category": "github",
    "translations": {
      "zh": {
        "title": "第五阶段 - AKS + Azure DNS + NGINX Ingress + cert-manager",
        "summary": "AKS的生产HTTPS设置指南,结合Azure DNS、NGINX Ingress Controller、cert-manager和Let's Encrypt,包括DNS配置、TLS自动化和证书验证过程。"
      },
      "fr": {
        "title": "Phase 5 - AKS avec Azure DNS + NGINX Ingress + cert-manager",
        "summary": "Guide de configuration HTTPS en production pour AKS combinant Azure DNS, contrôleur NGINX Ingress, cert-manager et Let's Encrypt, incluant la configuration DNS, l'automatisation TLS et les processus de validation de certificat."
      },
      "de": {
        "title": "Phase 5 - AKS mit Azure DNS + NGINX Ingress + cert-manager",
        "summary": "Produktions-HTTPS-Setup-Leitfaden für AKS, der Azure DNS, NGINX Ingress Controller, cert-manager und Let's Encrypt kombiniert, einschließlich DNS-Konfiguration, TLS-Automatisierung und Zertifikatvalidierungsprozesse."
      },
      "es": {
        "title": "Fase 5 - AKS con Azure DNS + NGINX Ingress + cert-manager",
        "summary": "Guía de configuración HTTPS de producción para AKS que combina Azure DNS, controlador NGINX Ingress, cert-manager y Let's Encrypt, incluyendo configuración DNS, automatización TLS y procesos de validación de certificados."
      }
    }
  },
  {
    "title": "Phase 4 - Exposing Multiple Services on AKS Using NGINX Ingress",
    "slug": "aks-nginx-ingress-multiple-services",
    "url": "https://dev.to/pilgrim2go/phase-3-exposing-multiple-services-on-aks-using-nginx-ingress-141o",
    "source": "DEV Community",
    "date": "2026-03-01T05:55:23.000Z",
    "summary": "Guide to deploying NGINX Ingress on AKS for Layer 7 routing of multiple services through a single public IP, replacing expensive per-service LoadBalancers with efficient microservices-ready infrastructure.",
    "content": "In this lab, we moved beyond a simple LoadBalancer and implemented Layer 7 routing using NGINX Ingress on:\nAzure Kubernetes Service (AKS)\nNGINX Ingress Controller\nBy the end, we achieved:\nhttp://<PUBLIC-IP>/hello\nhttp://<PUBLIC-IP>/api\n\nUsing:\nOne public IP\nOne Ingress controller\nMultiple backend services\nWith a normal Service:\ntype: LoadBalancer\n\nYou get:\n1 Service = 1 Public IP\n\nThat becomes expensive and hard to manage.\nWith Ingress:\n1 Public IP\n   → Multiple routes\n   → Multiple services\n\nThis is how real microservices platforms operate.\n\nInternet\n   ↓\nAzure LoadBalancer (created by ingress)\n   ↓\nNGINX Ingress Controller\n   ↓\nhello-service\napi-service\n\nThe LoadBalancer is automatically created when installing the Ingress controller.\nWe deployed the official controller:\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml\n\nCheck readiness:\nkubectl get pods -n ingress-nginx\n\nVerify service:\nkubectl get svc -n ingress-nginx\n\nWait until:\ningress-nginx-controller   LoadBalancer   <EXTERNAL-IP>\n\nThat IP becomes your single entry point.\nDeployment:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello\n        image: <ACR_LOGIN_SERVER>/hello-aks:v1\n        ports:\n        - containerPort: 3000\n\nService:\napiVersion: v1\nkind: Service\nmetadata:\n  name: hello-service\nspec:\n  selector:\n    app: hello\n  ports:\n  - port: 80\n    targetPort: 3000\n\nDeployment:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: nginx\n        ports:\n        - containerPort: 80\n\nService:\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-service\nspec:\n  selector",
    "category": "github",
    "translations": {
      "zh": {
        "title": "第4阶段 - 在AKS上使用NGINX Ingress公开多个服务",
        "summary": "在AKS上部署NGINX Ingress的指南，用于通过单个公共IP进行多个服务的第7层路由，用高效的微服务就绪基础设施取代昂贵的按服务LoadBalancers。"
      },
      "fr": {
        "title": "Phase 4 - Exposition de plusieurs services sur AKS avec NGINX Ingress",
        "summary": "Guide de déploiement de NGINX Ingress sur AKS pour le routage de couche 7 de plusieurs services via une seule adresse IP publique, remplaçant les LoadBalancers coûteux par service par une infrastructure efficace et prête pour les microservices."
      },
      "de": {
        "title": "Phase 4 - Mehrere Services auf AKS mit NGINX Ingress verfügbar machen",
        "summary": "Leitfaden zur Bereitstellung von NGINX Ingress auf AKS für Layer-7-Routing von mehreren Services über eine einzelne öffentliche IP-Adresse, das teure Pro-Service-LoadBalancer durch effiziente, mikroservice-fähige Infrastruktur ersetzt."
      },
      "es": {
        "title": "Fase 4 - Exponer múltiples servicios en AKS usando NGINX Ingress",
        "summary": "Guía para desplegar NGINX Ingress en AKS para enrutamiento de capa 7 de múltiples servicios a través de una única IP pública, reemplazando costosos equilibradores de carga por servicio con una infraestructura eficiente y lista para microservicios."
      }
    }
  },
  {
    "title": "Phase 2: Deploying a Production-Ready AKS Cluster in East US 2 (Azure CNI + Managed Identity + Monitoring)",
    "slug": "production-aks-cluster-deployment",
    "url": "https://dev.to/pilgrim2go/phase-2-deploying-a-production-ready-aks-cluster-in-east-us-2-azure-cni-managed-identity--49bn",
    "source": "DEV Community",
    "date": "2026-03-01T05:55:08.000Z",
    "summary": "Step-by-step production AKS cluster deployment using Azure CNI networking, managed identity authentication, OIDC issuer, workload identity, and Azure Monitor for enterprise security and observability.",
    "content": "In Phase 1, we prepared:\nResource Group\nVirtual Network + Subnet\nManaged Identity\nProvider registrations\nNow we deploy a production-grade AKS cluster in eastus2 using best practices.\nThis is not a demo cluster.\nWe will create:\nAKS attached to existing VNet\nAzure CNI networking (not kubenet)\nManaged Identity (no service principal)\nOIDC issuer enabled\nWorkload Identity enabled\nAzure Monitor integration enabled\nSeparate system/user node pools (optional)\nIntegrated services:\nKubernetes\nAzure Virtual Network\nAzure Monitor\nLog Analytics\nAKS must be attached to an existing subnet.\nSUBNET_ID=$(az network vnet subnet show \\\n  --resource-group aks-east2-rg \\\n  --vnet-name aks-vnet \\\n  --name aks-subnet \\\n  --query id -o tsv)\n\nMI_ID=$(az identity show \\\n  --resource-group aks-east2-rg \\\n  --name aks-mi \\\n  --query id -o tsv)\n\nMI_PRINCIPAL_ID=$(az identity show \\\n  --resource-group aks-east2-rg \\\n  --name aks-mi \\\n  --query principalId -o tsv)\n\nAKS must manage IP allocations inside the subnet.\nAssign Network Contributor role:\naz role assignment create \\\n  --assignee $MI_PRINCIPAL_ID \\\n  --role \"Network Contributor\" \\\n  --scope $SUBNET_ID\n\nWithout this, Azure CNI will fail.\nThis is one of the most common production misconfigurations.\nNow we deploy.\naz aks create \\\n  --resource-group aks-east2-rg \\\n  --name aks-prod-east2 \\\n  --location eastus2 \\\n  --node-count 2 \\\n  --node-vm-size Standard_DS3_v2 \\\n  --network-plugin azure \\\n  --vnet-subnet-id $SUBNET_ID \\\n  --assign-identity $MI_ID \\\n  --enable-managed-identity \\\n  --enable-oidc-issuer \\\n  --enable-workload-identity \\\n  --enable-addons monitoring \\\n  --generate-ssh-keys\n\n\n\n\nFlag\nWhy It Matters\n\n\n\n\n--network-plugin azure\nEnables Azure CNI (real VNet IPs for pods)\n\n\n--vnet-subnet-id\nAttaches cluster to enterprise network\n\n\n--assign-identity\nUses managed identity\n\n\n--enable-oidc-issuer\nRequired for workload identity\n\n\n--enable-workload-identity\nModern cloud auth model\n\n\n--enable-addons monitoring\nEnables logs + metrics\n\n\n--node-vm",
    "category": "github",
    "translations": {
      "zh": {
        "title": "第2阶段：在美国东部2地区部署生产就绪的AKS集群（Azure CNI + 托管身份 + 监控）",
        "summary": "使用Azure CNI网络、托管身份认证、OIDC颁发者、工作负载身份和Azure Monitor进行分步生产AKS集群部署，以实现企业级安全性和可观测性。"
      },
      "fr": {
        "title": "Phase 2 : Déploiement d'un cluster AKS prêt pour la production dans l'Est des États-Unis 2 (Azure CNI + Identité gérée + Surveillance)",
        "summary": "Déploiement étape par étape du cluster AKS en production utilisant la mise en réseau Azure CNI, l'authentification par identité gérée, l'émetteur OIDC, l'identité de la charge de travail et Azure Monitor pour la sécurité et l'observabilité de l'entreprise."
      },
      "de": {
        "title": "Phase 2: Bereitstellung eines produktionsreifen AKS-Clusters in East US 2 (Azure CNI + Verwaltete Identität + Überwachung)",
        "summary": "Schrittweise Bereitstellung von AKS-Produktionsclustern mit Azure CNI-Netzwerk, verwalteter Identitätsauthentifizierung, OIDC-Aussteller, Workload-Identität und Azure Monitor für Unternehmenssicherheit und Observabilität."
      },
      "es": {
        "title": "Fase 2: Implementación de un clúster de AKS listo para producción en el Este de EE.UU. 2 (Azure CNI + Identidad administrada + Supervisión)",
        "summary": "Implementación paso a paso del clúster AKS en producción utilizando redes Azure CNI, autenticación de identidad administrada, emisor OIDC, identidad de carga de trabajo y Azure Monitor para seguridad y observabilidad empresariales."
      }
    }
  },
  {
    "title": "Phase 1: Preparing Azure CLI for Production AKS (Region: East US 2)",
    "slug": "azure-cli-aks-setup-foundation",
    "url": "https://dev.to/pilgrim2go/phase-1-preparing-azure-cli-for-production-aks-region-east-us-2-3ael",
    "source": "DEV Community",
    "date": "2026-03-01T05:54:29.000Z",
    "summary": "Foundational Azure CLI setup for production AKS, covering subscription configuration, VNet/subnet provisioning, managed identity creation, and resource provider registration—prerequisites for enterprise Kubernetes deployment.",
    "content": "Before deploying a production-grade AKS cluster, you must properly configure Azure CLI, subscription settings, networking, and identity.\nThis guide walks through the exact foundational steps used by platform engineers when building AKS in eastus2.\nBy the end of this phase, you will have:\nAzure CLI installed and configured\nSubscription properly selected\nDefault region set to eastus2\n\nResource Group created\nVirtual Network + Subnet created\nManaged Identity created\nRequired Azure resource providers registered\nThis sets the stage for production AKS deployment.\nOn Ubuntu / WSL:\ncurl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n\nVerify installation:\naz version\n\nYou are now ready to manage resources in:\nMicrosoft Azure\naz login\n\nThis authenticates via:\nMicrosoft Entra ID\nList available subscriptions:\naz account list --output table\n\nSet your working subscription:\naz account set --subscription \"<SUBSCRIPTION_ID>\"\n\nConfirm:\naz account show --output table\n\nInstead of specifying --location every time:\naz configure --defaults location=eastus2\n\nVerify available regions:\naz account list-locations --output table\n\nNow all new resources default to:\neastus2\n\nResource Groups logically organize infrastructure.\naz group create \\\n  --name aks-east2-rg\n\nVerify:\naz group show --name aks-east2-rg --output table\n\nWe prepare networking before creating AKS.\naz network vnet create \\\n  --resource-group aks-east2-rg \\\n  --name aks-vnet \\\n  --address-prefix 10.0.0.0/8 \\\n  --subnet-name aks-subnet \\\n  --subnet-prefix 10.240.0.0/16\n\nThis creates:\nVirtual Network\nSubnet dedicated for AKS\nThis integrates later with:\nAzure Virtual Network\nModern AKS uses Managed Identity instead of Service Principals.\naz identity create \\\n  --resource-group aks-east2-rg \\\n  --name aks-mi\n\nCapture:\nprincipalId\nclientId\nid\nThis identity will later allow AKS to manage networking and cloud resources securely.\nWhen enabling monitoring later, many users hit this error:\nMissingSubscriptionRegistration:\nMicrosoft.Operation",
    "category": "github",
    "translations": {
      "zh": {
        "title": "第1阶段：为生产AKS准备Azure CLI（地区：美国东部2）",
        "summary": "生产AKS的基础Azure CLI设置，涵盖订阅配置、VNet/子网配置、托管身份创建和资源提供程序注册——企业Kubernetes部署的先决条件。"
      },
      "fr": {
        "title": "Phase 1 : Préparation d'Azure CLI pour AKS en production (Région : Est des États-Unis 2)",
        "summary": "Configuration de base d'Azure CLI pour AKS en production, couvrant la configuration de l'abonnement, la configuration des VNet/sous-réseaux, la création d'identités gérées et l'enregistrement des fournisseurs de ressources—prérequis pour le déploiement Kubernetes d'entreprise."
      },
      "de": {
        "title": "Phase 1: Vorbereitung der Azure CLI für produktives AKS (Region: East US 2)",
        "summary": "Grundlegende Azure CLI-Einrichtung für produktives AKS mit Abonnementkonfiguration, VNet-/Subnetz-Bereitstellung, Verwaltung der Identitätserstellung und Registrierung von Ressourcenanbietern—Voraussetzungen für die Kubernetes-Unternehmensbereitstellung."
      },
      "es": {
        "title": "Fase 1: Preparación de la CLI de Azure para AKS en producción (Región: Este de EE.UU. 2)",
        "summary": "Configuración básica de Azure CLI para AKS en producción, que cubre la configuración de suscripción, el aprovisionamiento de VNet/subred, la creación de identidades administradas y el registro de proveedores de recursos—requisitos previos para la implementación de Kubernetes empresarial."
      }
    }
  },
  {
    "title": "Comparison Between Local Nmap Execution and Python Subprocess Execution",
    "slug": "nmap-python-subprocess-automation",
    "url": "https://dev.to/ganesh_hari_18/comparison-between-local-nmap-execution-and-python-subprocess-execution-pnp",
    "source": "DEV Community",
    "date": "2026-03-01T05:49:09.000Z",
    "summary": "Comparison of manual Nmap execution via command prompt versus programmatic execution using Python's subprocess, demonstrating automation benefits, output handling, and security tool integration into scalable workflows.",
    "content": "Automation is becoming a core skill in cybersecurity. While tools like Nmap are powerful on their own, integrating them into programmable workflows unlocks a completely new level of flexibility and scalability.\n\n\nIn this article, I compare two approaches to running Nmap:\nExecuting Nmap manually via Windows Command Prompt\nExecuting Nmap programmatically using Python’s subprocess module\nThe goal of this experiment is simple: understand how traditional command-line security tools can evolve into automation-ready components.\nThe objective of this experiment was to analyze and compare the output and behavior of Nmap when executed:\nDirectly through the Windows Command Prompt (Manual Method)\nProgrammatically using Python’s subprocess module (Automated Method)\nBy comparing both methods, we can better understand the benefits of automation and structured output handling.\nOperating System: Windows 10\nNmap Version: 7.97\nPython Version: 3.x\nExecution Method 1: Command Prompt (Manual)\nExecution Method 2: Python Script using subprocess\nnmap www.google.com\nHost: www.google.com\n\nIP Address: 142.250.67.4 / 142.250.67.196\nHost Status: Up\nLatency: ~0.17 seconds\nPorts Scanned: 1000 default TCP ports\nOpen Ports Identified\nPort    State   Service\n80  Open    HTTP\n443 Open    HTTPS\n2222    Open    EtherNetIP\n\nApproximately 25.39 seconds\n997 ports were filtered (no response)\nDefault Nmap scans the top 1000 TCP ports\nOutput is displayed directly in the terminal\nThe result format is plain text with no structured data\nThis method is straightforward and effective for quick, manual inspections.\nApproach\nIn this method, Python’s subprocess.run() was used to execute the same Nmap command programmatically.\nHost: WWW.GOOGLE.COM\nIP Address: 142.250.67.196\nHost Status: Up\nLatency: ~0.018 seconds\nPorts Scanned: 1000 default TCP ports\nPort    State   Service\n80  Open    HTTP\n443 Open    HTTPS\n2222    Open    EtherNetIP\n\n\nApproximately 15.34 seconds\n996 filtered ports + 1 net-unreachable\nOutput was captu",
    "category": "github",
    "translations": {
      "zh": {
        "title": "本地Nmap执行与Python子进程执行的比较",
        "summary": "比较通过命令提示符手动执行Nmap与使用Python子进程进行编程执行，演示自动化优势、输出处理以及安全工具集成到可扩展工作流的方法。"
      },
      "fr": {
        "title": "Comparaison entre l'exécution locale de Nmap et l'exécution Python Subprocess",
        "summary": "Comparaison de l'exécution manuelle de Nmap via l'invite de commande par rapport à l'exécution programmatique utilisant subprocess de Python, démontrant les avantages de l'automatisation, la gestion des résultats et l'intégration des outils de sécurité dans des workflows évolutifs."
      },
      "de": {
        "title": "Vergleich zwischen lokaler Nmap-Ausführung und Python-Subprocess-Ausführung",
        "summary": "Vergleich der manuellen Nmap-Ausführung über die Eingabeaufforderung im Vergleich zur programmgesteuerten Ausführung mit Pythons Subprocess, wobei die Vorteile der Automatisierung, die Ausgabeverarbeitung und die Integration von Sicherheitstools in skalierbare Workflows demonstriert werden."
      },
      "es": {
        "title": "Comparación entre la ejecución local de Nmap y la ejecución Python Subprocess",
        "summary": "Comparación de la ejecución manual de Nmap a través del símbolo del sistema frente a la ejecución programática usando subprocess de Python, demostrando beneficios de automatización, manejo de salida e integración de herramientas de seguridad en flujos de trabajo escalables."
      }
    }
  },
  {
    "title": "I built a desktop app that orchestrates Claude, GPT, Gemini and local Ollama in a 3-phase pipeline",
    "slug": "helix-ai-studio-multi-model-pipeline",
    "url": "https://dev.to/tsunamayo7/i-built-a-desktop-app-that-orchestrates-claude-gpt-gemini-and-local-ollama-in-a-3-phase-pipeline-1ml7",
    "source": "DEV Community",
    "date": "2026-03-01T05:46:02.000Z",
    "summary": "Helix AI Studio is an open-source desktop app coordinating Claude, GPT, Gemini, and local Ollama models in sequential pipelines where each model's output feeds into the next phase for complementary AI strengths.",
    "content": "I've been building desktop AI tools for a while, and one frustration kept coming up: every AI model has different strengths, but using them together was always manual work — copy-paste between apps, switch tabs, lose context.\nSo I built Helix AI Studio — an open-source desktop app that lets Claude, GPT, Gemini, and local Ollama models work together in a coordinated pipeline.\nGitHub: https://github.com/tsunamayo7/helix-ai-studio\nInstead of sending one prompt to one model, Helix routes your request through multiple AI models in sequence. Each model handles what it's best at:\nYour prompt\n    ↓\nPhase 1: Claude (analysis & reasoning)\n    ↓\nPhase 2: GPT / Gemini (alternative perspective)\n    ↓\nPhase 3: Local Ollama model (offline processing / privacy)\n    ↓\nFinal synthesized response\n\nYou configure which models run in which phases, and the output of each phase feeds into the next.\nDesktop GUI (PyQt6)\nThree chat tabs: cloudAI (Claude/GPT/Gemini), localAI (Ollama), mixAI (the pipeline)\nDark-themed native Windows app\nReal-time streaming responses\nBuilt-in Web UI (React + FastAPI)\nAccess from mobile or other devices on your LAN\nWebSocket-based streaming — same experience as the desktop\nJWT authentication\nLocal LLM Support\nOllama integration via httpx async calls\nModel switching without restart\nWorks fully offline\nRAG Memory\nSQLite-based conversation storage\nRetrieval-augmented context for follow-up questions\n\n\n\nLayer\nTech\n\n\n\n\nDesktop GUI\nPyQt6\n\n\nWeb backend\nFastAPI + Uvicorn + WebSocket\n\n\nWeb frontend\nReact + Tailwind CSS\n\n\nLocal LLMs\nOllama\n\n\nCloud AIs\nAnthropic SDK, OpenAI SDK, Google Generative AI\n\n\nDB\nSQLite\n\n\n\nDifferent models genuinely excel at different things. In my testing:\nClaude is great at structured reasoning and nuanced writing\nGPT handles coding tasks and tool use well\n\n\nGemini has strong multimodal and factual retrieval\nLocal models (Mistral, Llama, Gemma) keep sensitive data on-device\nBy pipelining them, you get complementary strengths rather than betting eve",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我构建了一个在3阶段管道中协调Claude、GPT、Gemini和本地Ollama的桌面应用",
        "summary": "Helix AI Studio是一个开源桌面应用程序，在顺序管道中协调Claude、GPT、Gemini和本地Ollama模型，其中每个模型的输出输入到下一阶段，以实现互补的AI优势。"
      },
      "fr": {
        "title": "J'ai construit une application de bureau qui orchestre Claude, GPT, Gemini et Ollama local dans un pipeline à 3 phases",
        "summary": "Helix AI Studio est une application de bureau open-source coordonnant les modèles Claude, GPT, Gemini et Ollama local dans des pipelines séquentiels où la sortie de chaque modèle alimente la phase suivante pour des forces d'IA complémentaires."
      },
      "de": {
        "title": "Ich habe eine Desktop-App erstellt, die Claude, GPT, Gemini und lokales Ollama in einer 3-Phasen-Pipeline orchestriert",
        "summary": "Helix AI Studio ist eine Open-Source-Desktop-App, die Claude-, GPT-, Gemini- und lokale Ollama-Modelle in sequentiellen Pipelines koordiniert, wobei die Ausgabe jedes Modells in die nächste Phase fließt, um komplementäre KI-Stärken zu ermöglichen."
      },
      "es": {
        "title": "Construí una aplicación de escritorio que orquesta Claude, GPT, Gemini y Ollama local en un pipeline de 3 fases",
        "summary": "Helix AI Studio es una aplicación de escritorio de código abierto que coordina los modelos Claude, GPT, Gemini y Ollama local en canales secuenciales donde la salida de cada modelo se introduce en la siguiente fase para obtener fortalezas de IA complementarias."
      }
    }
  },
  {
    "title": "Comunidade Viva —Plataforma de Engajamento Urbano",
    "slug": "comunidade-viva-urban-engagement",
    "url": "https://dev.to/diogobaguiar/comunidade-viva-plataforma-de-engajamento-urbano-3jph",
    "source": "DEV Community",
    "date": "2026-03-01T05:45:08.000Z",
    "summary": "Urban community engagement platform enabling collaborative problem-solving through geospatial incident reporting, local event management, gamification systems, and proximity-based alerts for real-time civic participation.",
    "content": "A Comunidade\n\n\nA comunidade foco deste projeto é composta por residentes de centros urbanos. O sistema atua como um catalisador tecnológico para conectar indivíduos que compartilham o mesmo espaço geográfico, viabilizando a resolução colaborativa de problemas estruturais e de segurança. \nO objetivo arquitetural é transformar a vizinhança em um ecossistema inteligente, descentralizado e altamente participativo.\nDesenvolvi uma Plataforma de Engajamento Comunitário em Tempo Real, projetada com foco em alta disponibilidade e separação de responsabilidades. O domínio da aplicação expõe as seguintes funcionalidades principais:\n  Mapeamento Geoespacial de Incidentes: Interface interativa para o reporte de problemas e alertas.\n  Orquestração de Eventos: Módulo para a criação e gestão de atividades locais.\n  Motor de Gamificação: Sistema de recompensas e badges para incentivar a cidadania ativa.\n  Notificações Baseadas em Contexto: Alertas críticos orientados pela proximidade geográfica.\n\nAcessar Plataforma em Produção\n\n\n\n  \n  \n  Repositório (Code)\n\n\nO código-fonte está versionado no GitHub e foi implementado aplicando princípios de Arquitetura de Software e estruturação modular.\n / \n        desafio-weeken-challenge\n      \n    \n🏙️ Comunidade Viva — Plataforma de Engajamento Urbano\nStatus do Projeto: 🚀 Em desenvolvimento para o DEV Weekend Challenge 2026.\n📑 Índice\n\n\n\n🎯 Visão Geral\n✨ Funcionalidades\n🛠️ Arquitetura e Tecnologias\n💻 Demonstração\n🚀 Instalação\n⚖️ Licença\n🎯 Visão Geral\n\n\nA Comunidade Viva nasceu para combater o isolamento social em centros urbanos. Onde antes havia apenas vizinhos, agora existe um ecossistema inteligente e participativo.\n\"Transformando a vizinhança em uma rede de colaboração em tempo real.\"\n✨ Funcionalidades\n\n\n\n\n📍 Mapeamento de Incidentes\n\nInterface interativa para reporte de problemas (buracos, segurança, iluminação).\n📅 Gestão de Eventos\n\nCriação de mutirões, feiras e atividades locais.\n🎮 Motor de Gamificação\n\nSistema de ranking e insígn",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Comunidade Viva —城市参与平台",
        "summary": "城市社区参与平台，通过地理空间事件报告、本地事件管理、游戏化系统和基于邻近度的警报，实现协作问题解决和实时公民参与。"
      },
      "fr": {
        "title": "Comunidade Viva —Plateforme d'engagement urbain",
        "summary": "Plateforme d'engagement communautaire urbain permettant la résolution collaborative des problèmes grâce au signalement des incidents géospatiaux, à la gestion des événements locaux, aux systèmes de gamification et aux alertes basées sur la proximité pour une participation civique en temps réel."
      },
      "de": {
        "title": "Comunidade Viva —Plattform für städtisches Engagement",
        "summary": "Plattform für städtisches Gemeinschaftsengagement, die kollaborative Problemlösung durch räumliche Vorfallsmeldung, lokales Veranstaltungsmanagement, Gamification-Systeme und näherungsbasierte Benachrichtigungen für die Teilhabe an Bürgern in Echtzeit ermöglicht."
      },
      "es": {
        "title": "Comunidade Viva —Plataforma de compromiso urbano",
        "summary": "Plataforma de compromiso comunitario urbano que permite la resolución colaborativa de problemas a través de informes de incidentes geoespaciales, gestión de eventos locales, sistemas de gamificación y alertas basadas en proximidad para la participación cívica en tiempo real."
      }
    }
  },
  {
    "title": "Microgpt",
    "slug": "microgpt-karpathy",
    "url": "http://karpathy.github.io/2026/02/12/microgpt/",
    "source": "Hacker News",
    "date": "2026-03-01T01:39:26.000Z",
    "summary": "Karpathy's technical article on Microgpt discussing efficient GPT implementations, generating substantial discussion with 385 points and 73 comments on Hacker News.",
    "content": "Article URL: http://karpathy.github.io/2026/02/12/microgpt/\nComments URL: https://news.ycombinator.com/item?id=47202708\nPoints: 385\n# Comments: 73",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Microgpt",
        "summary": "Karpathy的关于Microgpt的技术文章讨论了高效的GPT实现,在黑客新闻上获得了385个赞和73条评论的广泛讨论。"
      },
      "fr": {
        "title": "Microgpt",
        "summary": "L'article technique de Karpathy sur Microgpt discute les implémentations GPT efficaces, générant une discussion substantielle avec 385 points et 73 commentaires sur Hacker News."
      },
      "de": {
        "title": "Microgpt",
        "summary": "Karpathys technischer Artikel zu Microgpt erörtert effiziente GPT-Implementierungen und erzeugt eine umfangreiche Diskussion mit 385 Punkten und 73 Kommentaren auf Hacker News."
      },
      "es": {
        "title": "Microgpt",
        "summary": "El artículo técnico de Karpathy sobre Microgpt discute implementaciones GPT eficientes, generando una discusión sustancial con 385 puntos y 73 comentarios en Hacker News."
      }
    }
  },
  {
    "title": "How I Unified 3 Fragmented Medical APIs Into a Single Python SDK",
    "slug": "medkit-unified-medical-apis-sdk",
    "url": "https://dev.to/interestng/how-i-unified-3-fragmented-medical-apis-into-a-single-python-sdk-5di7",
    "source": "DEV Community",
    "date": "2026-03-01T00:28:00.000Z",
    "summary": "MedKit is a unified Python SDK that consolidates fragmented medical data APIs (OpenFDA, PubMed, ClinicalTrials.gov) into a single platform with features like clinical synthesis and drug interaction detection. The tool aims to reduce the time healthcare developers spend on data cleaning and integration, allowing them to focus on analysis and application logic.",
    "content": "I built MedKit because medical data is notoriously difficult to work with. If you want to correlate a drug's FDA label with its latest clinical trial phases and related research papers, you usually have to juggle three different APIs, handle idiosyncratic JSON schemas, and deal with inconsistent identifier types.\nMedKit is a unified Python SDK that transforms these fragmented sources (OpenFDA, PubMed, and ClinicalTrials.gov) into a single, programmable platform.\nKey Features:\nUnified Client: One MedKit() client to rule them all. No more multiple API keys or manual correlation.\nClinical Synthesis (med.conclude()): Aggregates data to give a \"snapshot\" verdict on a drug or condition, including an evidence strength score (0.0–1.0).\nInteraction Engine: catch drug-drug contraindications using cross-label mentions (brand vs generic).\nMedical Relationship Graph: Visualize connections between drugs, trials, and research papers as a knowledge graph.\nIntelligence Layer: Natural language routing (med.ask()) to query data in plain English.\nWhy Use It? Most healthcare developers spend 80% of their time just cleaning and joining data. MedKit handles the plumbing (caching, schema normalization, relationship mapping) so you can focus on the analysis or the application logic.\nTech Stack: Python (Sync/Async), Disk/Memory caching, and a provider-based architecture for easy extensibility.\nI'd love to get your thoughts on the med.conclude() synthesis logic, other features and what other providers (e.g., pharmacogenomics) you'd find useful.\nhttps://github.com/interestng/medkit PyPI: pip install medkit-sdk\nI really appreciate any support towards this post, and stars/follows on my github repo!\nLooking forward to your feedback!",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我如何将3个碎片化的医学API统一到一个Python SDK中",
        "summary": "MedKit是一个统一的Python SDK，整合了碎片化的医学数据API（OpenFDA、PubMed、ClinicalTrials.gov）到一个平台，具有临床综合和药物相互作用检测等功能。该工具旨在减少医疗保健开发人员在数据清理和集成上花费的时间，让他们能够专注于分析和应用逻辑。"
      },
      "fr": {
        "title": "Comment j'ai unifié 3 API médicales fragmentées dans un seul SDK Python",
        "summary": "MedKit est un SDK Python unifié qui consolide les API de données médicales fragmentées (OpenFDA, PubMed, ClinicalTrials.gov) en une seule plateforme avec des fonctionnalités telles que la synthèse clinique et la détection des interactions médicamenteuses. L'outil vise à réduire le temps que les développeurs de soins de santé consacrent au nettoyage et à l'intégration des données, leur permettant de se concentrer sur l'analyse et la logique des applications."
      },
      "de": {
        "title": "Wie ich 3 fragmentierte medizinische APIs in ein einzelnes Python SDK vereinigt habe",
        "summary": "MedKit ist ein einheitliches Python SDK, das fragmentierte medizinische Daten-APIs (OpenFDA, PubMed, ClinicalTrials.gov) in einer einzigen Plattform mit Funktionen wie klinischer Synthese und Erkennung von Arzneimittelwechselwirkungen konsolidiert. Das Tool zielt darauf ab, die Zeit zu reduzieren, die Gesundheitsentwickler für Datenbereinigung und Integration aufwenden, und ermöglicht ihnen, sich auf Analyse und Anwendungslogik zu konzentrieren."
      },
      "es": {
        "title": "Cómo unifiqué 3 API médicas fragmentadas en un único SDK de Python",
        "summary": "MedKit es un SDK de Python unificado que consolida las API de datos médicos fragmentadas (OpenFDA, PubMed, ClinicalTrials.gov) en una única plataforma con características como síntesis clínica y detección de interacciones medicamentosas. La herramienta tiene como objetivo reducir el tiempo que los desarrolladores de atención médica dedican a la limpieza e integración de datos, permitiéndoles enfocarse en el análisis y la lógica de la aplicación."
      }
    }
  },
  {
    "title": "I Built a macOS App to Stop Links From Opening in the Wrong Chrome Profile",
    "slug": "linkprism-chrome-profile-link-router",
    "url": "https://dev.to/badaverse/i-built-a-macos-app-to-stop-links-from-opening-in-the-wrong-chrome-profile-3e1f",
    "source": "DEV Community",
    "date": "2026-03-01T00:24:24.000Z",
    "summary": "LinkPrism is a macOS menu bar application that routes browser links to the correct Chrome profile based on user-defined rules, with a companion Chrome extension for in-browser link handling. The tool eliminates manual context switching by automatically directing links to specified profiles, reducing daily cognitive friction.",
    "content": "I use multiple Chrome profiles.\nEvery morning I open Slack, click a Notion link, and watch it open in my personal profile. I copy the URL, switch to the work profile, paste, hit enter. Then I click a GitHub link in email — same thing, wrong profile. Copy, switch, paste.\nI've been doing this dozens of times a day for years. At some point I stopped counting and started building.\nRoute links from outside Chrome (Slack, email) and links clicked inside Chrome\nSet a rule once and forget about it — no profile picker every time\nSupport wildcards and regex, not just exact domains\nI wanted something that covered the full loop. So I decided to build it myself.\nLinkPrism is a macOS menu bar app. You set it as your default browser, add a few rules, and that's it.\nnotion.so        → Work\ngithub.com       → Personal\n*.atlassian.net  → Work\ndocs.google.com  → Ask every time\n\nRules can be exact host matches, wildcards, or regex. For domains you use across profiles, there's an \"Ask\" mode — a profile picker pops up and you can check \"Don't ask again for this URL.\"\nThis handles links from Slack, email, Telegram — anything outside Chrome. But there's a catch.\nLinks clicked inside Chrome never hit the OS default browser handler. Chrome handles them internally. The app can't see them.\nSo I built a companion Chrome extension (Manifest V3) that:\nSyncs rules from the macOS app via a local HTTP server on 127.0.0.1:19384\n\nDetects which Chrome profile it's running in using chrome.identity\n\nMatches rules client-side on every navigation\nReroutes only when the current profile is wrong — no unnecessary redirects\nNow the full loop is closed. External links go through the app. In-browser links go through the extension. No gaps.\nAfter using it for a while, I realized the annoyance wasn't just the extra clicks. It was the context switching. Every wrong-profile link pulled me out of whatever I was focused on. Fixing this tiny friction removed a surprising amount of daily cognitive noise.\nIf you juggle C",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我构建了一个macOS应用来阻止链接在错误的Chrome配置文件中打开",
        "summary": "LinkPrism是一个macOS菜单栏应用程序，根据用户定义的规则将浏览器链接路由到正确的Chrome配置文件，并配有一个配套的Chrome扩展程序用于浏览器内的链接处理。该工具通过自动将链接定向到指定的配置文件来消除手动上下文切换，减少日常的认知摩擦。"
      },
      "fr": {
        "title": "J'ai créé une application macOS pour empêcher les liens de s'ouvrir dans le mauvais profil Chrome",
        "summary": "LinkPrism est une application de barre de menu macOS qui achemine les liens du navigateur vers le profil Chrome correct en fonction de règles définies par l'utilisateur, avec une extension Chrome complémentaire pour la gestion des liens dans le navigateur. L'outil élimine le changement de contexte manuel en dirigeant automatiquement les liens vers les profils spécifiés, réduisant la friction cognitive quotidienne."
      },
      "de": {
        "title": "Ich habe eine macOS-App erstellt, um zu verhindern, dass Links im falschen Chrome-Profil geöffnet werden",
        "summary": "LinkPrism ist eine macOS-Menüleisten-Anwendung, die Browser-Links basierend auf benutzerdefinierten Regeln zum korrekten Chrome-Profil leitet, mit einer begleitenden Chrome-Erweiterung für die Link-Verarbeitung im Browser. Das Tool eliminiert manuelle Kontextwechsel durch automatisches Weiterleiten von Links zu angegebenen Profilen und reduziert tägliche kognitive Reibung."
      },
      "es": {
        "title": "Construí una aplicación macOS para evitar que los enlaces se abran en el perfil de Chrome incorrecto",
        "summary": "LinkPrism es una aplicación de barra de menú de macOS que enruta enlaces del navegador al perfil de Chrome correcto según reglas definidas por el usuario, con una extensión de Chrome complementaria para el manejo de enlaces en el navegador. La herramienta elimina el cambio de contexto manual al dirigir automáticamente los enlaces a perfiles especificados, reduciendo la fricción cognitiva diaria."
      }
    }
  },
  {
    "title": "Happy Birthday, Lettuce! 🥬✨ Two Years of Helping Us “Let You Get Started”",
    "slug": "lettuce-owasp-onboarding-tool-anniversary",
    "url": "https://dev.to/owaspblt/happy-birthday-lettuce-two-years-of-helping-us-let-you-get-started-mek",
    "source": "DEV Community",
    "date": "2026-03-01T00:22:52.000Z",
    "summary": "The Lettuce project marked its two-year anniversary as a Slack-based onboarding tool that has helped nearly 6,000 newcomers navigate the OWASP ecosystem through structured guidance and project discovery. The tool transformed the barrier to entry for OWASP contributors by meeting them on Slack instead of requiring navigation through complex documentation.",
    "content": "Happy Birthday, Lettuce! 🥬✨\n\n\n\n  \n  \n  Two Years of Helping Us “Let You Get Started”\n\n\nTwo years ago today, a simple question echoed through the OWASP Slack channels — a question that continues to surface year after year:\n“Where do I begin?”\nFor newcomers, the OWASP ecosystem is inspiring — but vast. With countless repositories, extensive documentation, and a diverse range of project pages, it’s easy to feel overwhelmed before writing a single line of code.\nThat moment of uncertainty sparked the creation of BLT-Lettuce. Today, we celebrate the project that transformed an intimidating wall of information into a welcoming front door.\nLettuce began not with elaborate architecture, but with a practical realization. Through conversations between Donnie Brown and Jason, a clear insight emerged: the best way to support newcomers was to meet them exactly where they already were — on Slack.\nOn February 29, 2024, the first prototype commit landed with a focused mission: create a guided pathway for the steady wave of students and curious developers joining initiatives like Google Summer of Code.\nThe name reflects that mission perfectly:\nLettuce → “Let us get started.”\nLettuce didn’t launch with fanfare or a marketing campaign. It was a quiet utility designed to do one thing exceptionally well: provide orientation.\nSince its first organic Slack post in June 2024, Lettuce has supported nearly 6,000 newcomers in navigating OWASP with confidence.\nIt offered a structured, hierarchical guide through the ecosystem, enabling contributors to:\nDiscover projects aligned with their interests\nUnderstand contribution pathways without decoding the entire organization\nMove from “lost” to “confident” in a single conversation\nImportantly, Lettuce also takes into account each project’s Slack member count to suggest channels that are active, balanced, and welcoming. By guiding newcomers toward communities with healthy engagement — rather than overcrowded or inactive spaces — it helps ensure conv",
    "category": "github",
    "translations": {
      "zh": {
        "title": "生日快乐，Lettuce！🥬✨ 两年来帮助我们\"让你快速开始\"",
        "summary": "Lettuce项目迎来了两周年纪念，作为一个基于Slack的入职工具，它已经帮助近6000名新手通过结构化指导和项目发现来导航OWASP生态系统。该工具通过在Slack上与贡献者见面而不是要求他们通过复杂文档导航，改变了OWASP贡献者的进入障碍。"
      },
      "fr": {
        "title": "Joyeux anniversaire, Lettuce ! 🥬✨ Deux ans pour nous aider à \"vous faire commencer\"",
        "summary": "Le projet Lettuce a marqué son deuxième anniversaire en tant qu'outil d'intégration basé sur Slack qui a aidé près de 6 000 nouveaux venus à naviguer dans l'écosystème OWASP grâce à des conseils structurés et la découverte de projets. L'outil a transformé la barrière à l'entrée pour les contributeurs OWASP en les rencontrant sur Slack au lieu de leur demander de naviguer dans une documentation complexe."
      },
      "de": {
        "title": "Alles Gute zum Geburtstag, Lettuce! 🥬✨ Zwei Jahre, um uns „damit zu beginnen\" zu helfen",
        "summary": "Das Lettuce-Projekt feierte sein zweijähriges Jubiläum als Slack-basiertes Onboarding-Tool, das fast 6.000 Neuankömmlinge dabei geholfen hat, das OWASP-Ökosystem durch strukturierte Anleitung und Projektentdeckung zu navigieren. Das Tool veränderte die Eintrittsbarriere für OWASP-Mitwirkende, indem es sie auf Slack traf, anstatt sie durch komplexe Dokumentation zu navigieren."
      },
      "es": {
        "title": "¡Feliz cumpleaños, Lettuce! 🥬✨ Dos años ayudándonos a \"que comiences\"",
        "summary": "El proyecto Lettuce marcó su segundo aniversario como una herramienta de incorporación basada en Slack que ha ayudado a casi 6.000 recién llegados a navegar por el ecosistema de OWASP a través de orientación estructurada y descubrimiento de proyectos. La herramienta transformó la barrera de entrada para los colaboradores de OWASP al encontrarse con ellos en Slack en lugar de requerir que navegaran a través de documentación compleja."
      }
    }
  },
  {
    "title": "I built a Claude AI sidebar extension for the browser — here’s how",
    "slug": "claude-ai-browser-sidebar-extension",
    "url": "https://dev.to/nexio-labs/i-built-a-claude-ai-sidebar-extension-for-the-browser-heres-how-36d4",
    "source": "DEV Community",
    "date": "2026-03-01T00:13:44.000Z",
    "summary": "A Chrome extension that embeds a Claude-powered sidebar into webpages, enabling summarization, key point extraction, and contextual chat without tab switching. The tool uses Haiku for fast responses and Sonnet for longer outputs with direct API calls through a special CORS header.",
    "content": "I spend a lot of time reading docs, articles, and research papers. The constant tab-switching to ask Claude about what I'm reading was killing my flow.\nSo I built a Chrome extension that injects a Claude-powered sidebar into every webpage.\nSummarize — one click to get the full page summarized\nKey points — bullet-point extraction of the most important info\nExplain selection — highlight any text, get an explanation\nImprove selection — highlight text you've written, get a rewrite\nChat — full conversational mode with the page content as context\nAlt+A toggles it open/closed from anywhere.\nManifest V3 Chrome extension\nShadow DOM for UI isolation (no style conflicts with host pages)\nService worker background script for Claude API calls\nanthropic-dangerous-direct-browser-access header for direct API calls\nClaude Haiku for speed, Sonnet for longer outputs\nCalling Claude API directly from a browser extension causes a CORS error unless you add the anthropic-dangerous-direct-browser-access: true header. Took me a while to find that in the docs.\nI packaged it up for $9 if you want to skip building it yourself: Nexio AI Extension on Gumroad\nThe zip includes the full source code so you can load it as an unpacked extension or learn from it.\nHappy to answer questions about the implementation in the comments!",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我为浏览器构建了Claude AI侧边栏扩展——方法如下",
        "summary": "一个Chrome扩展程序，将Claude驱动的侧边栏嵌入网页中，支持摘要、关键点提取和上下文聊天，无需切换选项卡。该工具使用Haiku实现快速响应，使用Sonnet处理更长的输出，通过特殊的CORS头直接调用API。"
      },
      "fr": {
        "title": "J'ai créé une extension de barre latérale Claude AI pour le navigateur — voici comment",
        "summary": "Une extension Chrome qui intègre une barre latérale alimentée par Claude dans les pages Web, permettant le résumé, l'extraction des points clés et la discussion contextuelle sans changer d'onglet. L'outil utilise Haiku pour des réponses rapides et Sonnet pour des résultats plus longs avec des appels API directs via un en-tête CORS spécial."
      },
      "de": {
        "title": "Ich habe eine Claude-KI-Seitenleisten-Erweiterung für den Browser erstellt — so geht's",
        "summary": "Eine Chrome-Erweiterung, die eine von Claude angetriebene Seitenleiste in Webseiten einbettet und Zusammenfassungen, Extraktion wichtiger Punkte und kontextabhängiges Chatten ohne Registerkartenwechsel ermöglicht. Das Tool nutzt Haiku für schnelle Antworten und Sonnet für längere Ausgaben mit direkten API-Aufrufen über einen speziellen CORS-Header."
      },
      "es": {
        "title": "Construí una extensión de barra lateral Claude AI para el navegador — así es cómo",
        "summary": "Una extensión de Chrome que integra una barra lateral impulsada por Claude en páginas web, permitiendo resumen, extracción de puntos clave y chat contextual sin cambiar de pestaña. La herramienta utiliza Haiku para respuestas rápidas y Sonnet para salidas más largas con llamadas API directas a través de un encabezado CORS especial."
      }
    }
  },
  {
    "title": "Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideasand Publications, and Bibliography for Explainab",
    "slug": "explainability-human-ai-systems-literature-review",
    "url": "https://dev.to/paperium/explanation-in-human-ai-systems-a-literature-meta-review-synopsis-of-key-ideasand-publications-lnb",
    "source": "DEV Community",
    "date": "2026-03-01T00:10:07.000Z",
    "summary": "A meta-review synthesizing literature on explainability in human-AI systems, examining key ideas and publications relevant to interpretable AI and human-AI interaction design. The work provides researchers and practitioners with a comprehensive bibliography and synopsis of current knowledge in the field.",
    "content": "{{ $json.postContent }}",
    "category": "github",
    "translations": {
      "zh": {
        "title": "人类与AI系统中的解释：文献元审查、关键思想和出版物综述及可解释性参考书目",
        "summary": "一份关于人类与AI系统中可解释性的文献元审查，审视与可解释AI和人类与AI交互设计相关的关键思想和出版物。该工作为研究人员和从业者提供了该领域当前知识的全面参考书目和摘要。"
      },
      "fr": {
        "title": "L'explication dans les systèmes homme-IA : une méta-revue de la littérature, synopsis des idées clés et publications, et bibliographie pour l'explicabilité",
        "summary": "Une méta-revue synthétisant la littérature sur l'explicabilité dans les systèmes homme-IA, examinant les idées et publications clés pertinentes pour l'IA interprétable et la conception de l'interaction homme-IA. Le travail fournit aux chercheurs et praticiens une bibliographie complète et un synopsis des connaissances actuelles dans le domaine."
      },
      "de": {
        "title": "Erklärung in menschlich-KI-Systemen: Eine Literatur-Metaübersicht, Zusammenfassung von Schlüsselideen und Publikationen sowie Bibliographie für Erklärbarkeit",
        "summary": "Eine Metaübersicht, die Literatur zur Erklärbarkeit in menschlich-KI-Systemen zusammenfasst und Schlüsselideen und Publikationen untersucht, die für interpretierbare KI und Mensch-KI-Interaktionsdesign relevant sind. Das Werk bietet Forschern und Praktikern eine umfassende Bibliographie und Zusammenfassung des aktuellen Wissensstands in diesem Bereich."
      },
      "es": {
        "title": "Explicación en sistemas humano-IA: una metarrevisión de la literatura, sinopsis de ideas y publicaciones clave, y bibliografía para la explicabilidad",
        "summary": "Una metarrevisión que sintetiza la literatura sobre explicabilidad en sistemas humano-IA, examinando ideas y publicaciones clave relevantes para la IA interpretable y el diseño de interacción humano-IA. El trabajo proporciona a investigadores y profesionales una bibliografía completa y una sinopsis del conocimiento actual en el campo."
      }
    }
  },
  {
    "title": "Day 27 of #100DaysOfCode — REST API",
    "slug": "100daysofcode-rest-api-fundamentals",
    "url": "https://dev.to/m_saad_ahmad/day-27-of-100daysofcode-rest-api-37l7",
    "source": "DEV Community",
    "date": "2026-03-01T00:06:55.000Z",
    "summary": "Part of a coding challenge, this entry explains foundational REST API concepts including HTTP methods (GET, POST, PUT, PATCH, DELETE), resources, endpoints, and query parameters. It covers the request-response cycle and proper REST naming conventions for building scalable APIs.",
    "content": "Whether you realize it or not, you’ve already been using REST APIs every time an app sends a request and receives a response.\nToday, on Day 27, I focused on truly understanding how the request–response cycle works behind the scenes.\nThink of a REST API like a waiter in a restaurant:\nYou (the client/app) request food.\nThe kitchen (server/database) prepares it.\nThe waiter (REST API) takes your request, delivers it, and brings the result back.\nYou never go into the kitchen.\nIs REST?\n\n\nREST (Representational State Transfer) is a set of rules that allow two applications to communicate over the internet.\nThe client uses standard HTTP methods to talk to a server and fetch or change data.\nHere are the most common ones:\n\n\n\nMethod\nPurpose\n\n\n\n\nGET\nRetrieve data\n\n\nPOST\nCreate new data\n\n\nPUT\nReplace an entire existing resource\n\n\nPATCH\nUpdate part of an existing resource\n\n\nDELETE\nDelete a resource\n\n\n\nWhen your weather app loads, it might send a request like this:\nGET https://api.weather.com/city=karachi\n\nThe server responds with JSON data:\n{\n  \"city\": \"Karachi\",\n  \"temperature\": \"31°C\",\n  \"condition\": \"Sunny\"\n}\n\nYour app displays the weather — thanks to the API.\nA resource is basically any piece of data your API deals with.\nusers\nposts\nproducts\norders\nEach resource has a unique URL (called an endpoint).\nFor example:\n/users\n/posts\n/products\n\nREST focuses on nouns, not verbs.\nGet all users\nGET /users\n\nCreate a user\nPOST /users\n\nGet a single user\nGET /users/:id\n\nUpdate a user\nPUT /users/:id\nPATCH /users/:id\n\nDelete a user\nDELETE /users/:id\n\nPOST /createUser\nGET /getAllUsers\nDELETE /deleteUser\n\nThese use verbs in the URL — which breaks REST conventions.\nQuery params allow filtering, searching or customizing results.\nExamples:\nGET /users?role=admin\nGET /products?limit=10&page=2\nGET /posts?sort=latest\n\nA Request contains:\n\n\n\n\nparams → values inside the URL (e.g., /users/:id)\nquery → filtering/pagination (e.g., ?page=2)\nbody → data for POST/PUT/PATCH requests\nheaders → metadata (auth to",
    "category": "github",
    "translations": {
      "zh": {
        "title": "#100DaysOfCode第27天——REST API",
        "summary": "作为编码挑战的一部分，本条目解释了基础REST API概念，包括HTTP方法（GET、POST、PUT、PATCH、DELETE）、资源、端点和查询参数。它涵盖请求-响应周期和构建可扩展API的适当REST命名约定。"
      },
      "fr": {
        "title": "Jour 27 du #100DaysOfCode — API REST",
        "summary": "Dans le cadre d'un défi de codage, cette entrée explique les concepts fondamentaux des API REST, notamment les méthodes HTTP (GET, POST, PUT, PATCH, DELETE), les ressources, les points de terminaison et les paramètres de requête. Elle couvre le cycle demande-réponse et les conventions de nommage REST appropriées pour construire des API évolutives."
      },
      "de": {
        "title": "Tag 27 von #100DaysOfCode — REST API",
        "summary": "Als Teil einer Coding-Herausforderung erklärt dieser Eintrag grundlegende REST-API-Konzepte einschließlich HTTP-Methoden (GET, POST, PUT, PATCH, DELETE), Ressourcen, Endpunkte und Abfrageparameter. Es behandelt den Request-Response-Zyklus und ordnungsgemäße REST-Benennungskonventionen für die Erstellung skalierbarer APIs."
      },
      "es": {
        "title": "Día 27 de #100DaysOfCode — API REST",
        "summary": "Como parte de un desafío de codificación, esta entrada explica conceptos fundamentales de API REST incluyendo métodos HTTP (GET, POST, PUT, PATCH, DELETE), recursos, puntos finales y parámetros de consulta. Cubre el ciclo de solicitud-respuesta y las convenciones de nomenclatura REST adecuadas para construir API escalables."
      }
    }
  }
]