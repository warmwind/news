[
  {
    "title": "Drupal AI Module Generator Deepseek MCP",
    "slug": "drupal-ai-module-generator-deepseek-mcp",
    "url": "https://dev.to/victorstackai/drupal-ai-module-generator-deepseek-mcp-1hl8",
    "source": "DEV Community",
    "date": "2026-02-26T23:59:39.000Z",
    "summary": "drupal-ai-module-generator-deepseek-mcp automates Drupal module scaffolding using DeepSeek-backed MCP workflows to generate standardized boilerplate and module structure. By enforcing predictable Drupal conventions at the start, it makes downstream automation (testing, linting, CI) easier and reduces manual setup errors.",
    "content": "drupal-ai-module-generator-deepseek-mcp is a Drupal-oriented generator that uses a DeepSeek-backed MCP workflow to scaffold module code. I built it to take the repetitive, error-prone parts of module setupâ€”info files, boilerplate, and consistent structureâ€”and make them fast and repeatable. It fits naturally into agent-driven workflows where you want consistent Drupal modules without losing time to manual setup.\nItâ€™s useful because it standardizes the starting point for modules and makes the first commit reliable. That means less time redoing file structures, fewer mistakes in module metadata, and a faster path from idea to a working, testable Drupal feature. If youâ€™re iterating on multiple modules or experiments, the generator pays off almost immediately.\nThe key technical takeaway is that pairing MCP with a targeted generator creates a clear contract between intent and output. You define the module intent, and the generator enforces a predictable Drupal skeleton that downstream tools can build on. That makes subsequent automationâ€”tests, linting, and CI checksâ€”much easier to wire in.\nReferences\nView Code\nOriginally published at VictorStack AI Blog",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Drupal AIæ¨¡å—ç”Ÿæˆå™¨Deepseek MCP",
        "summary": "drupal-ai-module-generator-deepseek-mcpä½¿ç”¨DeepSeekæ”¯æŒçš„MCPå·¥ä½œæµè‡ªåŠ¨åŒ–Drupalæ¨¡å—è„šæ‰‹æ¶ï¼Œç”Ÿæˆæ ‡å‡†åŒ–çš„æ ·æ¿ä»£ç å’Œæ¨¡å—ç»“æ„ã€‚é€šè¿‡åœ¨å¼€å§‹æ—¶å¼ºåˆ¶æ‰§è¡Œå¯é¢„æµ‹çš„Drupalçº¦å®šï¼Œä½¿ä¸‹æ¸¸è‡ªåŠ¨åŒ–ï¼ˆæµ‹è¯•ã€ä»£ç æ£€æŸ¥ã€CIï¼‰æ›´å®¹æ˜“ï¼Œå¹¶å‡å°‘æ‰‹åŠ¨è®¾ç½®é”™è¯¯ã€‚"
      },
      "fr": {
        "title": "GÃ©nÃ©rateur de Module AI Drupal Deepseek MCP",
        "summary": "drupal-ai-module-generator-deepseek-mcp automatise l'Ã©chafaudage des modules Drupal en utilisant des flux de travail MCP soutenus par DeepSeek pour gÃ©nÃ©rer des modÃ¨les standardisÃ©s et une structure de module. En appliquant les conventions Drupal prÃ©visibles dÃ¨s le dÃ©part, cela rend l'automatisation en aval (tests, linting, CI) plus facile et rÃ©duit les erreurs de configuration manuelle."
      },
      "de": {
        "title": "Drupal-AI-Modulgenerator Deepseek MCP",
        "summary": "drupal-ai-module-generator-deepseek-mcp automatisiert das Drupal-Modul-Scaffolding mit DeepSeek-gestÃ¼tzten MCP-Workflows zur Generierung von standardisierten Boilerplate- und Modulstrukturen. Durch die Durchsetzung vorhersehbarer Drupal-Konventionen von Anfang an wird die nachgelagerte Automatisierung (Tests, Linting, CI) erleichtert und manuelle Setupfehler werden reduziert."
      },
      "es": {
        "title": "Generador de MÃ³dulo AI Drupal Deepseek MCP",
        "summary": "drupal-ai-module-generator-deepseek-mcp automatiza el andamiaje de mÃ³dulos Drupal usando flujos de trabajo MCP respaldados por DeepSeek para generar cÃ³digo repetitivo estandarizado y estructura de mÃ³dulos. Al aplicar convenciones Drupal predecibles desde el principio, hace que la automatizaciÃ³n aguas abajo (pruebas, linting, CI) sea mÃ¡s fÃ¡cil y reduce errores de configuraciÃ³n manual."
      }
    }
  },
  {
    "title": "My own memory allocator : a thread-safe, efficient implementation of malloc, calloc, realloc and free - review welcome",
    "slug": "custom-memory-allocator-malloc-calloc-free",
    "url": "https://dev.to/fioritoalessio/my-own-memory-allocator-a-thread-safe-efficient-implementation-of-malloc-calloc-realloc-and-c4o",
    "source": "DEV Community",
    "date": "2026-02-26T23:52:54.000Z",
    "summary": "This custom memory allocator implements thread-safe malloc, calloc, realloc, and free functions using mmap kernel mapping, segregated free lists with O(1) search, boundary tag coalescing, and bitmasked metadata packing. The project demonstrates core systems programming concepts applicable to other projects.",
    "content": "Hey everyone,\nI decided to code my own memory allocator from scratch as a shared library I can actually use in my other projects. I thought it would be a cool way to understand what happens under the hood of malloc, calloc, realloc and free and build a reusable tool in the process.\nI tried my best to create an efficient, POSIX-compliant, and thread-safe library. Here is what i use\nDirect kernel mapping (mmap / munmap)\nSegregated free lists (O(1) search time)\nBoundary tags (header and footer) with O(1) coalescing\nAnonymous unions and structs inside the header for zero-overhead payloads\nBitmasking to pack metadata into a single variable and not waste space\nPOSIX Mutexes for thread safety\nI tried to think through the architecture by myself as much as possible. Since I mostly code solo projects for now, this is the first time I'm putting my code out there for people to actually see.\nIf you have the time and motivation, I would greatly appreciate it if you could check out my GitHub repo and let me know thoughts on the code.\ngithub repo : https://github.com/Fiorito-Alessio/af_memory\nThanks!",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æˆ‘è‡ªå·±çš„å†…å­˜åˆ†é…å™¨ï¼šmallocã€callocã€reallocå’Œfreeçš„çº¿ç¨‹å®‰å…¨ã€é«˜æ•ˆå®ç° - æ¬¢è¿è¯„å®¡",
        "summary": "è¿™ä¸ªè‡ªå®šä¹‰å†…å­˜åˆ†é…å™¨ä½¿ç”¨mmapå†…æ ¸æ˜ å°„ã€O(1)æœç´¢çš„åˆ†æ®µç©ºé—²åˆ—è¡¨ã€è¾¹ç•Œæ ‡è®°åˆå¹¶å’Œä½æ©ç å…ƒæ•°æ®æ‰“åŒ…ï¼Œå®ç°äº†çº¿ç¨‹å®‰å…¨çš„mallocã€callocã€reallocå’Œfreeå‡½æ•°ã€‚è¯¥é¡¹ç›®å±•ç¤ºäº†é€‚ç”¨äºå…¶ä»–é¡¹ç›®çš„æ ¸å¿ƒç³»ç»Ÿç¼–ç¨‹æ¦‚å¿µã€‚"
      },
      "fr": {
        "title": "Mon propre allocateur de mÃ©moire : une implÃ©mentation thread-safe et efficace de malloc, calloc, realloc et free - avis bienvenu",
        "summary": "Cet allocateur de mÃ©moire personnalisÃ© implÃ©mente les fonctions malloc, calloc, realloc et free thread-safe en utilisant le mappage noyau mmap, des listes libres segmentÃ©es avec recherche O(1), la coalescence des Ã©tiquettes de frontiÃ¨re et l'empaquetage des mÃ©tadonnÃ©es masquÃ©es par bits. Le projet dÃ©montre des concepts fondamentaux de programmation systÃ¨me applicables Ã  d'autres projets."
      },
      "de": {
        "title": "Mein eigener Speicherallokator : eine Thread-sichere, effiziente Implementierung von malloc, calloc, realloc und free - Bewertung willkommen",
        "summary": "Dieser benutzerdefinierte Speicherallokator implementiert Thread-sichere malloc-, calloc-, realloc- und free-Funktionen unter Verwendung von mmap-Kernel-Mapping, segmentierten freien Listen mit O(1)-Suche, Grenzmarken-Zusammenlegung und bitmaskirtem Metadaten-Packing. Das Projekt demonstriert grundlegende Systemprogrammierungskonzepte, die auf andere Projekte anwendbar sind."
      },
      "es": {
        "title": "Mi propio asignador de memoria: una implementaciÃ³n eficiente y segura para hilos de malloc, calloc, realloc y free - se aceptan opiniones",
        "summary": "Este asignador de memoria personalizado implementa funciones malloc, calloc, realloc y free seguras para hilos utilizando mapeo del kernel mmap, listas libres segmentadas con bÃºsqueda O(1), combinaciÃ³n de etiquetas de lÃ­mite y empaquetado de metadatos con mÃ¡scara de bits. El proyecto demuestra conceptos fundamentales de programaciÃ³n de sistemas aplicables a otros proyectos."
      }
    }
  },
  {
    "title": "AI and Nuclear Fusion Vol.1: From Nuclear Physics to Plasma Confinement",
    "slug": "ai-nuclear-fusion-vol-1-physics-plasma",
    "url": "https://dev.to/dosanko_tousan/ai-and-nuclear-fusion-vol1-from-nuclear-physics-to-plasma-confinement-5gja",
    "source": "DEV Community",
    "date": "2026-02-26T23:52:47.000Z",
    "summary": "This foundational volume derives the complete physics basis for evaluating fusion, including quantum tunneling, Coulomb barriers, and fuel reaction comparisons (D-T, D-D, D-Â³He, p-Â¹Â¹B). It covers magnetohydrodynamic equilibrium, plasma confinement configurations, and instabilitiesâ€”providing decision frameworks for policymakers assessing fusion viability.",
    "content": "AI and Nuclear Fusion Vol.1: From Nuclear Physics to Plasma Confinement\n\n\n\nSeries: \"Thinking Seriously About Nuclear Fusion with AI\"\n\n\n\nItem\nDetail\n\n\n\n\nPurpose\nEstablish the complete physical basis for evaluating nuclear fusion, from nuclear reactions through plasma confinement\n\n\nAudience\nGovernment policy advisors, energy investment analysts, aerospace engineers, fusion program managers\n\n\nPrerequisites\nUndergraduate-level physics (classical mechanics, electromagnetism, thermodynamics). All derivations are self-contained.\n\n\nScope\n\nPart I: Nuclear binding energy â†’ Coulomb barrier â†’ Quantum tunneling â†’ Reaction cross-sections â†’ Fuel comparison. Part II: MHD equilibrium â†’ Confinement configurations â†’ Plasma instabilities â†’ Transport theory\n\n\nDeliverables\n(1) Complete derivation chain from first principles, (2) Reproducible Python code for all figures, (3) Fuel comparison matrix, (4) Confinement physics assessment for decision-making\n\n\n\nÂ§1. Executive Summary\nÂ§2. Nuclear Binding Energy â€” The Source\nÂ§3. Mass Defect and Energy Release â€” Quantitative Treatment\nÂ§4. The Coulomb Barrier â€” Why Fusion Is Hard\nÂ§5. Quantum Tunneling â€” The Gamow Factor\nÂ§6. Reaction Cross-Section Ïƒ(E)\nÂ§7. Maxwell-Averaged Reactivity âŸ¨ÏƒvâŸ©\nÂ§8. Candidate Fusion Reactions â€” Complete Catalog\nÂ§9. Quantitative Fuel Comparison\nÂ§10. The Gamow Peak â€” Where Fusion Actually Happens\nÂ§11. Computational Analysis â€” Full Reproducible Code\nÂ§12. Why Confinement Is the Central Problem\nÂ§13. Magnetohydrodynamics â€” The Fluid Model of Plasma\nÂ§14. MHD Equilibrium â€” The Grad-Shafranov Equation\nÂ§15. Magnetic Confinement Configurations\nÂ§16. MHD Instabilities â€” Why Plasma Escapes\nÂ§17. Transport Theory â€” How Energy Leaks\nÂ§18. Disruptions â€” The Catastrophic Failure Mode\nÂ§19. Confinement Computational Analysis\nÂ§20. Implications for Reactor and Propulsion Design\nÂ§21. Uncertainties and Limitations\nÂ§22. References\nNuclear fusion â€” the process of combining light atomic nuclei to form heavier ones â€” releases energy via the conversion o",
    "category": "github",
    "translations": {
      "zh": {
        "title": "äººå·¥æ™ºèƒ½ä¸æ ¸èšå˜ç¬¬1å·ï¼šä»æ ¸ç‰©ç†åˆ°ç­‰ç¦»å­ä½“çº¦æŸ",
        "summary": "è¯¥åŸºç¡€å·æ¨å¯¼äº†è¯„ä¼°èšå˜çš„å®Œæ•´ç‰©ç†åŸºç¡€ï¼ŒåŒ…æ‹¬é‡å­éš§ç©¿ã€åº“ä»‘åŠ¿å’å’Œç‡ƒæ–™ååº”æ¯”è¾ƒï¼ˆD-Tã€D-Dã€D-Â³Heã€p-Â¹Â¹Bï¼‰ã€‚æ¶µç›–ç£æµä½“åŠ¨åŠ›å­¦å¹³è¡¡ã€ç­‰ç¦»å­ä½“çº¦æŸé…ç½®å’Œä¸ç¨³å®šæ€§â€”â€”ä¸ºè¯„ä¼°èšå˜å¯è¡Œæ€§çš„æ”¿ç­–åˆ¶å®šè€…æä¾›å†³ç­–æ¡†æ¶ã€‚"
      },
      "fr": {
        "title": "L'IA et la Fusion NuclÃ©aire Vol.1: De la Physique NuclÃ©aire au Confinement du Plasma",
        "summary": "Ce volume fondamental dÃ©rive la base physique complÃ¨te pour Ã©valuer la fusion, incluant l'effet tunnel quantique, les barriÃ¨res de Coulomb et les comparaisons de rÃ©actions de carburant (D-T, D-D, D-Â³He, p-Â¹Â¹B). Il couvre l'Ã©quilibre magnÃ©tohydrodynamique, les configurations de confinement du plasma et les instabilitÃ©s, fournissant des cadres dÃ©cisionnels aux dÃ©cideurs politiques Ã©valuant la viabilitÃ© de la fusion."
      },
      "de": {
        "title": "KI und Kernfusion Vol.1: Von der Kernphysik zur Plasmaeinschluss",
        "summary": "Dieses Grundlagenwerk leitet die vollstÃ¤ndige physikalische Grundlage zur Bewertung der Fusion her, einschlieÃŸlich Quantentunneleffekt, Coulomb-Barrieren und Brennstoffvergleiche (D-T, D-D, D-Â³He, p-Â¹Â¹B). Es behandelt magnetohydrodynamisches Gleichgewicht, Plasmaeinschlussanordnungen und InstabilitÃ¤ten und bietet Entscheidungsrahmen fÃ¼r EntscheidungstrÃ¤ger zur Bewertung der FusionsfÃ¤higkeit."
      },
      "es": {
        "title": "IA y FusiÃ³n Nuclear Vol.1: De la FÃ­sica Nuclear al Confinamiento del Plasma",
        "summary": "Este volumen fundamental deriva la base fÃ­sica completa para evaluar la fusiÃ³n, incluyendo el efecto tÃºnel cuÃ¡ntico, barreras de Coulomb y comparaciones de reacciones de combustible (D-T, D-D, D-Â³He, p-Â¹Â¹B). Cubre el equilibrio magnetohidrodinÃ¡mico, configuraciones de confinamiento de plasma e inestabilidades, proporcionando marcos de decisiÃ³n para los formuladores de polÃ­ticas que evalÃºan la viabilidad de la fusiÃ³n."
      }
    }
  },
  {
    "title": "Generators in the Age of AI (Saving 4 Person-Years)",
    "slug": "generators-ai-ssrs-report-conversion",
    "url": "https://dev.to/grant_biggert/generators-in-the-age-of-ai-saving-4-person-years-3ae2",
    "source": "DEV Community",
    "date": "2026-02-26T23:41:41.000Z",
    "summary": "A developer reduced a 4 person-year SSRS-to-modern-reporting conversion to 3 weeks using an AI-assisted generator that auto-generates 80% of table and filtering code from RDL XML files. The project demonstrates how deterministic generators with built-in standards provide reliable output that AI alone cannot achieve.",
    "content": "One of the things that we are doing at my company is we are retiring our SSRS reports. Reporting is very important to our company as it is to many companies and with 64 different SSRS reports to convert to another system, the amount of work is a daunting task.\nOne of the cool things we have at our company is a dynamic JSON based dashboard building system. Lots of fun, but another post to describe it and what it looks like. Safe to say the general idea is that we need to convert these SSRS reports to our modern reporting system. So essentially a variety of table widgets with varying filters to different stored procedures in various databases and Snowflake.\nSo there was a constraint imposed that we needed to do as much of this on the frontend as possible. I promise I would have preferred a bit more backend involvement, but it was a direction handed down from my director. So the question was how to handle this in as short an amount of time as possible.\nMy director originally estimated this as around 4 person-years of development.\nWhat I did was take each SSRS report, parse it for the query parameters and the stored procedure, classify the data inside of that to various common mappings, and generate most of the table and filtering work automatically. The idea was that if I could get a developer 80 percent of the way there we could achieve some real time savings.\nFor the next 2 weeks I spent time analyzing various RDL files, which are essentially an XML format, and with a mixture of using AI and my own generator know-how, created a generator that would take these files and generate full blown tables. We are now closing out week 3 and are finished with all but 3 of the 64 reports.\nWhat I am trying to say is that deterministic tools and AI can be complementary. AI is great for getting something out fast but the quality can be unreliable. One of the great things about writing generators is that you can guarantee output and codify your standards as you build them. My argumen",
    "category": "github",
    "translations": {
      "zh": {
        "title": "äººå·¥æ™ºèƒ½æ—¶ä»£çš„ç”Ÿæˆå™¨ï¼ˆèŠ‚çœ4äººå¹´ï¼‰",
        "summary": "ä¸€ä½å¼€å‘è€…ä½¿ç”¨ä¸€ä¸ªAIè¾…åŠ©ç”Ÿæˆå™¨å°†åŸæœ¬éœ€è¦4äººå¹´çš„SSRSåˆ°ç°ä»£æŠ¥å‘Šè½¬æ¢å‡å°‘åˆ°3å‘¨ï¼Œè¯¥ç”Ÿæˆå™¨èƒ½ä»RDL XMLæ–‡ä»¶è‡ªåŠ¨ç”Ÿæˆ80%çš„è¡¨æ ¼å’Œç­›é€‰ä»£ç ã€‚è¯¥é¡¹ç›®å±•ç¤ºäº†å…·æœ‰å†…ç½®æ ‡å‡†çš„ç¡®å®šæ€§ç”Ÿæˆå™¨å¦‚ä½•æä¾›AIå•ç‹¬æ— æ³•è¾¾åˆ°çš„å¯é è¾“å‡ºã€‚"
      },
      "fr": {
        "title": "Les GÃ©nÃ©rateurs Ã  l'Ãˆre de l'IA (Ã‰conomisant 4 Personnes-AnnÃ©es)",
        "summary": "Un dÃ©veloppeur a rÃ©duit une conversion SSRS-to-modern-reporting de 4 personnes-annÃ©es Ã  3 semaines en utilisant un gÃ©nÃ©rateur assistÃ© par l'IA qui gÃ©nÃ¨re automatiquement 80% du code de tableau et de filtrage Ã  partir de fichiers RDL XML. Le projet dÃ©montre comment les gÃ©nÃ©rateurs dÃ©terministes avec des normes intÃ©grÃ©es fournissent une sortie fiable que l'IA seule ne peut pas atteindre."
      },
      "de": {
        "title": "Generatoren im Zeitalter der KI (Einsparung von 4 Personenjahren)",
        "summary": "Ein Entwickler hat eine 4-Personenjahre-SSRS-zu-moderner-Berichtswandlung in 3 Wochen reduziert, indem er einen KI-gestÃ¼tzten Generator verwendete, der automatisch 80% des Tabellen- und Filtercodes aus RDL-XML-Dateien generiert. Das Projekt zeigt, wie deterministische Generatoren mit integrierten Standards zuverlÃ¤ssige Ergebnisse bieten, die KI allein nicht erreichen kann."
      },
      "es": {
        "title": "Generadores en la Era de la IA (Ahorrando 4 Persona-AÃ±os)",
        "summary": "Un desarrollador redujo una conversiÃ³n SSRS-a-informes-modernos de 4 persona-aÃ±os a 3 semanas usando un generador asistido por IA que genera automÃ¡ticamente el 80% del cÃ³digo de tablas y filtrado a partir de archivos RDL XML. El proyecto demuestra cÃ³mo los generadores deterministas con estÃ¡ndares integrados proporcionan resultados confiables que la IA sola no puede lograr."
      }
    }
  },
  {
    "title": "CVE-2026-22728: The Old Switcheroo: Unsealing Secrets via Metadata Manipulation in Bitnami Sealed Secrets",
    "slug": "cve-2026-22728-bitnami-sealed-secrets-flaw",
    "url": "https://dev.to/cverports/cve-2026-22728-the-old-switcheroo-unsealing-secrets-via-metadata-manipulation-in-bitnami-sealed-4jcc",
    "source": "DEV Community",
    "date": "2026-02-26T23:40:19.000Z",
    "summary": "CVE-2026-22728 reveals a logic flaw in Bitnami Sealed Secrets' rotation endpoint that allows attackers to manipulate metadata and transform namespace-scoped secrets into cluster-wide secrets, enabling credential recovery. The vulnerability affects versions before 0.36.0 and requires immediate patching.",
    "content": "The Old Switcheroo: Unsealing Secrets via Metadata Manipulation in Bitnami Sealed Secrets\n\n\n\nVulnerability ID: CVE-2026-22728\nCVSS Score: 4.9\nPublished: 2026-02-26\nBitnami Sealed Secrets, the standard-bearer for GitOps secret management, contains a logic flaw in its key rotation mechanism that allows attackers to widen the scope of encrypted secrets. By injecting malicious metadata during the rotation process, an attacker can transform a strictly scoped secret (bound to a specific namespace) into a cluster-wide secret, subsequently recovering the plaintext credentials. This vulnerability highlights a classic 'Time-of-Check to Time-of-Use' (TOCTOU) style disconnect between the decryption and re-encryption phases.\nA logic flaw in the /v1/rotate endpoint allows attackers to bypass scope restrictions. By modifying the metadata of a SealedSecret during rotation, an attacker can force the controller to re-encrypt a restricted secret as 'cluster-wide,' enabling them to decrypt it in any namespace.\nCWE ID: CWE-668\nAttack Vector: Network\nCVSS: 4.9 (Medium)\nPrivileges Required: High\nImpact: Confidentiality Loss\nExploit Status: PoC Available\nBitnami Sealed Secrets Controller < 0.36.0\nsealed-secrets: < 0.36.0 (Fixed in: 0.36.0)\nd57ee4a\n\n\nfeat: verify metadata is well set up in Template ObjectMeta and ObjectMeta\nif !reflect.DeepEqual(s.ObjectMeta, s.Spec.Template.ObjectMeta) { s.ObjectMeta.DeepCopyInto(&s.Spec.Template.ObjectMeta) }\n\nGitHub (Unit Tests): The regression test TestRotateKeepScope effectively acts as the PoC for this vulnerability.\nUpgrade the Controller\nRestrict API Access\nAudit Logs\nRemediation Steps:\nUpdate the Bitnami Sealed Secrets controller to version 0.36.0 or later immediately.\nIf immediate patching is not possible, ensure network policies or RBAC rules strictly limit access to the /v1/rotate endpoint to cluster administrators only.\nReview audit logs for calls to the rotate endpoint that triggered 'metadata doesn't match' warnings (if on newer versions) or",
    "category": "github",
    "translations": {
      "zh": {
        "title": "CVE-2026-22728: æ—§æŠŠæˆï¼šé€šè¿‡Bitnami Sealed Secretsä¸­çš„å…ƒæ•°æ®æ“çºµæ¥è§£å¯†æœºå¯†",
        "summary": "CVE-2026-22728æ­ç¤ºäº†Bitnami Sealed Secretsæ—‹è½¬ç«¯ç‚¹ä¸­çš„é€»è¾‘æ¼æ´ï¼Œè¯¥æ¼æ´å…è®¸æ”»å‡»è€…æ“çºµå…ƒæ•°æ®å¹¶å°†å‘½åç©ºé—´ä½œç”¨åŸŸçš„æœºå¯†è½¬å˜ä¸ºé›†ç¾¤èŒƒå›´çš„æœºå¯†ï¼Œä»è€Œèƒ½å¤Ÿæ¢å¤å‡­æ®ã€‚è¯¥æ¼æ´å½±å“0.36.0ä¹‹å‰çš„ç‰ˆæœ¬ï¼Œéœ€è¦ç«‹å³ä¿®è¡¥ã€‚"
      },
      "fr": {
        "title": "CVE-2026-22728: L'Ancienne Astuce: DÃ©chiffrer les Secrets via la Manipulation de MÃ©tadonnÃ©es dans Bitnami Sealed Secrets",
        "summary": "CVE-2026-22728 rÃ©vÃ¨le un flaw logique dans le point de terminaison de rotation de Bitnami Sealed Secrets qui permet aux attaquants de manipuler les mÃ©tadonnÃ©es et de transformer les secrets limitÃ©s Ã  l'espace de noms en secrets Ã  l'Ã©chelle du cluster, permettant la rÃ©cupÃ©ration des identifiants. La vulnÃ©rabilitÃ© affecte les versions antÃ©rieures Ã  0.36.0 et nÃ©cessite un correctif immÃ©diat."
      },
      "de": {
        "title": "CVE-2026-22728: Der alte Trick: Geheimnisse durch Metadaten-Manipulation in Bitnami Sealed Secrets enthÃ¼llen",
        "summary": "CVE-2026-22728 enthÃ¼llt einen Logikfehler im Rotationsendpunkt von Bitnami Sealed Secrets, der es Angreifern ermÃ¶glicht, Metadaten zu manipulieren und namespace-bezogene Geheimnisse in clusterweite Geheimnisse umzuwandeln, um Anmeldedaten wiederherzustellen. Die SicherheitslÃ¼cke betrifft Versionen vor 0.36.0 und erfordert sofortige Patches."
      },
      "es": {
        "title": "CVE-2026-22728: El Viejo Truco: Revelando Secretos a travÃ©s de ManipulaciÃ³n de Metadatos en Bitnami Sealed Secrets",
        "summary": "CVE-2026-22728 revela un defecto lÃ³gico en el punto final de rotaciÃ³n de Bitnami Sealed Secrets que permite a los atacantes manipular metadatos y transformar secretos limitados al espacio de nombres en secretos de todo el cluster, permitiendo la recuperaciÃ³n de credenciales. La vulnerabilidad afecta a versiones anteriores a 0.36.0 y requiere parches inmediatos."
      }
    }
  },
  {
    "title": "Statement from Dario Amodei on Our Discussions with the Department of War",
    "slug": "dario-amodei-department-of-war-statement",
    "url": "https://www.anthropic.com/news/statement-department-of-war",
    "source": "Hacker News",
    "date": "2026-02-26T22:42:47.000Z",
    "summary": "Anthropic CEO Dario Amodei published a statement regarding Anthropic's discussions with the Department of War, detailing the company's position on AI security and government collaboration on defense AI policy.",
    "content": "Article URL: https://www.anthropic.com/news/statement-department-of-war\nComments URL: https://news.ycombinator.com/item?id=47173121\nPoints: 384\n# Comments: 191",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Dario Amodei å…³äºä¸æˆ˜äº‰éƒ¨è®¨è®ºçš„å£°æ˜",
        "summary": "Anthropicé¦–å¸­æ‰§è¡Œå®˜Dario Amodeiå‘è¡¨å£°æ˜,é˜è¿°äº†å…¬å¸ä¸æˆ˜äº‰éƒ¨è®¨è®ºçš„ç›¸å…³å†…å®¹,è¯¦ç»†è¯´æ˜äº†å…¬å¸åœ¨AIå®‰å…¨å’Œæ”¿åºœå›½é˜²AIæ”¿ç­–åˆä½œæ–¹é¢çš„ç«‹åœºã€‚"
      },
      "fr": {
        "title": "DÃ©claration de Dario Amodei sur nos discussions avec le DÃ©partement de la Guerre",
        "summary": "Le PDG d'Anthropic, Dario Amodei, a publiÃ© une dÃ©claration concernant les discussions d'Anthropic avec le DÃ©partement de la Guerre, dÃ©taillant la position de l'entreprise sur la sÃ©curitÃ© de l'IA et la collaboration gouvernementale en matiÃ¨re de politique d'IA pour la dÃ©fense."
      },
      "de": {
        "title": "ErklÃ¤rung von Dario Amodei zu unseren Diskussionen mit dem Kriegsministerium",
        "summary": "Der CEO von Anthropic, Dario Amodei, verÃ¶ffentlichte eine ErklÃ¤rung zu den Diskussionen von Anthropic mit dem Kriegsministerium und erlÃ¤uterte die Position des Unternehmens zur KI-Sicherheit und zur staatlichen Zusammenarbeit bei der Verteidigungspolitik fÃ¼r kÃ¼nstliche Intelligenz."
      },
      "es": {
        "title": "DeclaraciÃ³n de Dario Amodei sobre nuestras discusiones con el Departamento de Guerra",
        "summary": "El CEO de Anthropic, Dario Amodei, publicÃ³ una declaraciÃ³n sobre las discusiones de Anthropic con el Departamento de Guerra, detallando la posiciÃ³n de la empresa sobre seguridad de IA y colaboraciÃ³n gubernamental en la polÃ­tica de IA para la defensa."
      }
    }
  },
  {
    "title": "Layoffs at Block",
    "slug": "block-layoffs-4000-employees-2026",
    "url": "https://twitter.com/jack/status/2027129697092731343",
    "source": "Hacker News",
    "date": "2026-02-26T21:17:56.000Z",
    "summary": "Block announced layoffs affecting approximately 4,000 employees, marking a significant workforce reduction across the payments and financial services company as part of broader 2026 tech industry restructuring.",
    "content": "https://www.cnbc.com/2026/02/26/block-laying-off-about-4000-...\nhttps://www.marketwatch.com/story/block-plans-to-lay-off-nea...\nComments URL: https://news.ycombinator.com/item?id=47172119\nPoints: 290\n# Comments: 268",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Blockå…¬å¸è£å‘˜",
        "summary": "Blockå®£å¸ƒè£å‘˜,å½±å“çº¦4000åå‘˜å·¥,æ ‡å¿—ç€è¿™å®¶æ”¯ä»˜å’Œé‡‘èæœåŠ¡å…¬å¸åœ¨2026å¹´ç§‘æŠ€è¡Œä¸šæ›´å¤§è§„æ¨¡é‡ç»„ä¸­çš„é‡å¤§äººå‘˜ç¼©å‡ã€‚"
      },
      "fr": {
        "title": "Licenciements chez Block",
        "summary": "Block a annoncÃ© des licenciements affectant environ 4 000 employÃ©s, marquant une rÃ©duction importante de la main-d'Å“uvre dans l'entreprise de paiements et de services financiers dans le cadre d'une restructuration plus large du secteur technologique en 2026."
      },
      "de": {
        "title": "Entlassungen bei Block",
        "summary": "Block kÃ¼ndigte Entlassungen an, die etwa 4.000 Mitarbeiter betreffen, was eine signifikante BelegschaftskÃ¼rzung bei dem Zahlungs- und Finanzdienstleistungsunternehmen als Teil einer breiteren Umstrukturierung der Technologiebranche im Jahr 2026 markiert."
      },
      "es": {
        "title": "Despidos en Block",
        "summary": "Block anunciÃ³ despidos que afectan a aproximadamente 4.000 empleados, marcando una reducciÃ³n significativa de personal en la empresa de pagos y servicios financieros como parte de la reestructuraciÃ³n mÃ¡s amplia de la industria tecnolÃ³gica en 2026."
      }
    }
  },
  {
    "title": "Whatâ€™s new with GitHub Copilot coding agent",
    "slug": "github-copilot-coding-agent-features-update",
    "url": "https://github.blog/ai-and-ml/github-copilot/whats-new-with-github-copilot-coding-agent/",
    "source": "GitHub Blog",
    "date": "2026-02-26T20:47:02.000Z",
    "summary": "GitHub released new features for Copilot coding agent including model selection, self-review, built-in security scanning, custom agent support, and CLI handoff capabilities. The updates expand the agent's autonomy and deployment flexibility for developers.",
    "content": "GitHub Copilot coding agent now includes a model picker, self-review, built-in security scanning, custom agents, and CLI handoff. Here's what's new and how to use it. \nThe post Whatâ€™s new with GitHub Copilot coding agent appeared first on The GitHub Blog.",
    "category": "github",
    "translations": {
      "zh": {
        "title": "GitHub Copilotç¼–ç ä»£ç†çš„æ–°åŠŸèƒ½",
        "summary": "GitHubä¸ºCopilotç¼–ç ä»£ç†å‘å¸ƒäº†æ–°åŠŸèƒ½,åŒ…æ‹¬æ¨¡å‹é€‰æ‹©ã€è‡ªæˆ‘å®¡æŸ¥ã€å†…ç½®å®‰å…¨æ‰«æã€è‡ªå®šä¹‰ä»£ç†æ”¯æŒå’ŒCLIäº¤æ¥åŠŸèƒ½ã€‚è¿™äº›æ›´æ–°æ‰©å±•äº†ä»£ç†çš„è‡ªä¸»æ€§å’Œå¼€å‘äººå‘˜çš„éƒ¨ç½²çµæ´»æ€§ã€‚"
      },
      "fr": {
        "title": "Quoi de neuf avec l'agent de codage GitHub Copilot",
        "summary": "GitHub a publiÃ© de nouvelles fonctionnalitÃ©s pour l'agent de codage Copilot, notamment la sÃ©lection de modÃ¨le, l'auto-examen, l'analyse de sÃ©curitÃ© intÃ©grÃ©e, le support des agents personnalisÃ©s et les capacitÃ©s de transfert CLI. Les mises Ã  jour Ã©largissent l'autonomie de l'agent et la flexibilitÃ© de dÃ©ploiement pour les dÃ©veloppeurs."
      },
      "de": {
        "title": "Was ist neu bei GitHub Copilot Coding Agent",
        "summary": "GitHub hat neue Funktionen fÃ¼r den Copilot-Codierungsagenten verÃ¶ffentlicht, darunter Modellauswahl, SelbstprÃ¼fung, integrierte SicherheitsprÃ¼fung, benutzerdefinierte Agent-UnterstÃ¼tzung und CLI-Ãœbergabefunktionen. Die Updates erweitern die Autonomie des Agenten und die BereitstellungsflexibilitÃ¤t fÃ¼r Entwickler."
      },
      "es": {
        "title": "Novedades del agente de codificaciÃ³n de GitHub Copilot",
        "summary": "GitHub lanzÃ³ nuevas funciones para el agente de codificaciÃ³n de Copilot, incluyendo selecciÃ³n de modelo, autorrevisiÃ³n, escaneo de seguridad integrado, soporte de agente personalizado y capacidades de transferencia CLI. Las actualizaciones expanden la autonomÃ­a del agente y la flexibilidad de implementaciÃ³n para los desarrolladores."
      }
    }
  },
  {
    "title": "\"Fuck You NVIDIA\" (and What I Learned Staring at a Blank Screen)",
    "slug": "nvidia-linux-display-wake-issue-arch",
    "url": "https://dev.to/workspacedex/fuck-you-nvidia-and-what-i-learned-staring-at-a-blank-screen-3g1g",
    "source": "DEV Community",
    "date": "2026-02-26T18:13:03.000Z",
    "summary": "NVIDIA's power management on consumer Linux hardware is broken, causing display wake issues that have been documented but unfixed in the driver. The author reflects on how Arch Linux forced deep learning through hands-on troubleshooting and problem-solving without hand-holding.",
    "content": "When I A bug or an system display related issue I found in Arch Linux running with SDDM.\nIt wakes up.\nBlack screen.\nI'm running Arch Linux with SDDM. NVIDIA GPU. Consumer card.\nHere's the thing about NVIDIA on Linux: their power management on consumer-level hardware is broken by design. When the display sleeps and wakes, the driver conflicts. The screen doesn't recover. You're left staring at nothing, wondering if you broke something or if something was always broken.\nIt's a known issue. Documented in forums. Mentioned in bug trackers. NVIDIA just hasn't cared enough to properly fix it for us.\n\nLinus Torvalds, 2012 (Still accurate)\nI've been on Arch for a year now.\nI didn't rice it. Not obsessively, anyway. I knew the trap â€” you spend three months building a desktop that's perfectly yours: every keybinding, every color, every font chosen by your own hands, understood by exactly one person on Earth. Beautiful to you. Useless to your deadline.\nThat wasn't wise for me. Not yet.\nBut I still learned more from this OS than any hand-holding distro ever taught me. Arch doesn't protect you from yourself. It hands you a blank canvas, a wiki, and your own stubbornness â€” then steps back.\nYou learn because you have to. And somehow that sticks.\nI was installing Omarchy on a CachyOS base â€” Hyprland setup, fresh install, ready to go. Used what was labeled a \"safe\" test script from GitHub.\nIt wasn't perfect.\nI started troubleshooting. Logs, terminal output, forum threads from 2019 that are somehow still the most relevant thing on the internet. Feeding output to my AI, clicking through configs, muttering to myself.\nAnd then â€” the pieces connected.\nThat display bug I'd been living with for months? I finally traced it. NVIDIA drivers conflicting on wake from sleep. Consumer power management. A problem that's been sitting in plain sight, documented and unfixed, waiting for me to finally look it in the eye.\nNobody tells you this when you install Arch.\nThe blank screens, the journalctl ra",
    "category": "github",
    "translations": {
      "zh": {
        "title": "\"æ“ä½ çš„ NVIDIA\"ï¼ˆä»¥åŠæˆ‘ç›¯ç€é»‘å±æ—¶å­¦åˆ°çš„ä¸œè¥¿ï¼‰",
        "summary": "NVIDIA åœ¨æ¶ˆè´¹çº§ Linux ç¡¬ä»¶ä¸Šçš„ç”µæºç®¡ç†å­˜åœ¨ç¼ºé™·ï¼Œå¯¼è‡´æ˜¾ç¤ºå™¨å”¤é†’é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å·²è¢«è®°å½•ä½†é©±åŠ¨ç¨‹åºæœªä¿®å¤ã€‚ä½œè€…åæ€äº† Arch Linux å¦‚ä½•é€šè¿‡äº²èº«æ•…éšœæ’é™¤å’Œè§£å†³é—®é¢˜æ¥å¼ºåˆ¶è¿›è¡Œæ·±åº¦å­¦ä¹ ï¼Œæ— éœ€æŒ‡å¯¼ã€‚"
      },
      "fr": {
        "title": "\"Va te faire foutre NVIDIA\" (et ce que j'ai appris en fixant un Ã©cran noir)",
        "summary": "La gestion de l'alimentation de NVIDIA sur le matÃ©riel grand public Linux est dÃ©fectueuse, causant des problÃ¨mes de rÃ©veil d'affichage qui ont Ã©tÃ© documentÃ©s mais non corrigÃ©s dans le pilote. L'auteur rÃ©flÃ©chit Ã  la faÃ§on dont Arch Linux a forcÃ© l'apprentissage profond grÃ¢ce Ã  la rÃ©solution de problÃ¨mes pratique sans assistance."
      },
      "de": {
        "title": "\"Fick dich NVIDIA\" (und was ich lerne, wÃ¤hrend ich auf einen schwarzen Bildschirm starre)",
        "summary": "NVIDIAs Stromverwaltung auf Consumer-Linux-Hardware ist fehlerhaft und verursacht Displayaufwach-Probleme, die im Treiber dokumentiert, aber nicht behoben wurden. Der Autor reflektiert, wie Arch Linux tiefes Lernen durch praktische Fehlersuche und ProblemlÃ¶sung ohne Anleitung erzwang."
      },
      "es": {
        "title": "\"JÃ³dete NVIDIA\" (y lo que aprendÃ­ mirando una pantalla en blanco)",
        "summary": "La gestiÃ³n de energÃ­a de NVIDIA en hardware Linux de consumidor estÃ¡ rota, causando problemas de despertar de pantalla que han sido documentados pero no arreglados en el controlador. El autor reflexiona sobre cÃ³mo Arch Linux obligÃ³ al aprendizaje profundo a travÃ©s de la resoluciÃ³n de problemas prÃ¡ctica sin ayuda."
      }
    }
  },
  {
    "title": "What Claude Code Chooses",
    "slug": "what-claude-code-chooses",
    "url": "https://amplifying.ai/research/claude-code-picks",
    "source": "Hacker News",
    "date": "2026-02-26T18:12:26.000Z",
    "summary": "Hacker News discussion highlighting research on Claude Code's decision-making patterns and capabilities, attracting significant community engagement on how the tool approaches coding decisions.",
    "content": "Article URL: https://amplifying.ai/research/claude-code-picks\nComments URL: https://news.ycombinator.com/item?id=47169757\nPoints: 318\n# Comments: 130",
    "category": "github"
  },
  {
    "title": "I Ship Faster Than Ever. I've Never Felt More Lost",
    "slug": "ship-faster-never-felt-more-lost",
    "url": "https://dev.to/nicoeft/i-ship-faster-than-ever-ive-never-felt-more-lost-2mdn",
    "source": "DEV Community",
    "date": "2026-02-26T18:12:23.000Z",
    "summary": "AI coding tools accelerate execution speed but don't save time overall, as faster implementation enables more ideas and ambition, leading to burnout instead of productivity. The author questions whether AI agents function as true collaborative teammates or merely fast execution engines.",
    "content": "The tools got faster. My life didn't slow down. Just sharing my journey on this new AI world\nWhen I quit my job, I had a plan. I was going to build things. Ship fast. Use AI to do in weeks what used to take months. Claude Code, Cursor, agents â€” I had the whole arsenal. The future was here, and I was going to ride it.\nThe first two weeks were a blur. I coded from morning until my token limits hit. Then I waited for them to reset. Then I coded again. I skipped meals, ran on coffee, got terrible sleep. I wasn't just --dangerously-skip-permissions,  I was skipping health, fitness, nutrition, socializing. Everything that wasn't shipping got deprioritized to zero.\nI didn't notice it happening. That's the dangerous part.\nHere's what the \"10x developer\" crowd won't tell you: AI doesn't save you time. It saves you execution time. And then your brain fills that gap with more ideas, more iterations, more ambition.\nMy last job was as a tech lead. I know what it feels like to orchestrate a team, keeping people aligned, reviewing work, making sure everyone and everything moves in the direction you actually want. Using AI agents feels exactly like that. It's nice, but it's a completely different kind of exhausting. You're not just coding anymore. You're managing. You're orchestrating. You're constantly course-correcting something that's fast but not always right, and I personally feel like I haven't yet managedd to achive having a AI co-worker in my team that completely disagrees on what I propose, not only because it knows better, but because it cares, it has the ownership that we always praised to hire for.\nAnd it doesn't just consume more hours. It consumes more thoughts. Ideas multiply because execution feels cheap. \"I could also build this.\" \"What if we added that?\" \"One more prompt and this feature is done.\" The cost of a substantial improvement is always just one or two prompts away, so you never stop improving. You never stop.\nThat's not productivity. That's a trap.\nThere'",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æˆ‘çš„äº¤ä»˜é€Ÿåº¦æ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½å¿«ã€‚æˆ‘ä»æœªæ„Ÿåˆ°è¿‡å¦‚æ­¤è¿·èŒ«",
        "summary": "AI ç¼–ç å·¥å…·åŠ é€Ÿäº†æ‰§è¡Œé€Ÿåº¦ï¼Œä½†æ€»ä½“ä¸Šä¸èŠ‚çœæ—¶é—´ï¼Œå› ä¸ºæ›´å¿«çš„å®ç°ä½¿å¾—æ›´å¤šçš„æƒ³æ³•å’Œé‡å¿ƒæˆä¸ºå¯èƒ½ï¼Œå¯¼è‡´å€¦æ€ è€Œä¸æ˜¯ç”Ÿäº§åŠ›ã€‚ä½œè€…è´¨ç–‘ AI ä»£ç†æ˜¯å¦å……å½“çœŸæ­£çš„åä½œé˜Ÿå‹ï¼Œè¿˜æ˜¯ä»…ä»…å……å½“å¿«é€Ÿæ‰§è¡Œå¼•æ“ã€‚"
      },
      "fr": {
        "title": "Je livre plus vite que jamais. Je ne me suis jamais senti plus perdu",
        "summary": "Les outils de codage IA accÃ©lÃ¨rent la vitesse d'exÃ©cution mais n'Ã©conomisent pas le temps global, car la mise en Å“uvre plus rapide permet plus d'idÃ©es et d'ambition, conduisant Ã  l'Ã©puisement professionnel plutÃ´t qu'Ã  la productivitÃ©. L'auteur s'interroge sur le fait que les agents IA fonctionnent comme de vÃ©ritables coÃ©quipiers collaboratifs ou simplement comme des moteurs d'exÃ©cution rapide."
      },
      "de": {
        "title": "Ich versende schneller als je zuvor. Ich habe mich noch nie verlorener gefÃ¼hlt",
        "summary": "KI-Codierungswerkzeuge beschleunigen die AusfÃ¼hrungsgeschwindigkeit, sparen aber insgesamt keine Zeit, da eine schnellere Implementierung mehr Ideen und Ehrgeiz ermÃ¶glicht, was zu Burnout statt ProduktivitÃ¤t fÃ¼hrt. Der Autor stellt in Frage, ob KI-Agenten als echte kollaborative Teamkollegen funktionieren oder nur als schnelle AusfÃ¼hrungsmotoren."
      },
      "es": {
        "title": "Entrego mÃ¡s rÃ¡pido que nunca. Nunca me he sentido mÃ¡s perdido",
        "summary": "Las herramientas de codificaciÃ³n de IA aceleran la velocidad de ejecuciÃ³n pero no ahorran tiempo en general, ya que la implementaciÃ³n mÃ¡s rÃ¡pida permite mÃ¡s ideas y ambiciÃ³n, lo que lleva al agotamiento en lugar de la productividad. El autor cuestiona si los agentes de IA funcionan como verdaderos compaÃ±eros de equipo colaborativos o simplemente como motores de ejecuciÃ³n rÃ¡pida."
      }
    }
  },
  {
    "title": "Trash Theory: Ocean Colour Scene: The Original Dadrock Band [\"The Riverboat Song\"] | New British Canon",
    "slug": "ocean-colour-scene-original-dadrock",
    "url": "https://dev.to/music_youtube/trash-theory-ocean-colour-scene-the-original-dadrock-band-the-riverboat-song-new-british-27p2",
    "source": "DEV Community",
    "date": "2026-02-26T18:06:46.000Z",
    "summary": "Ocean Colour Scene achieved commercial success during the Britpop era by blending retro influences with contemporary sound, particularly with \"The Riverboat Song,\" though critics unfairly dismissed them with the \"Dadrock\" label. Despite skepticism, the band left a significant mark on British music.",
    "content": "Ocean Colour Scene had quite the ride, bouncing from indie darlings to commercial heavyweights in the Britpop era. They masterfully blended retro vibes from bands like The Rolling Stones with a fresh sound for their generation, especially with \"The Riverboat Song.\"\nDespite their success, the press was quick to slap them with the \"Dadrock\" label, which some say unfairly dogged their credibility. Still, they made a massive mark!\nWatch on YouTube",
    "category": "github",
    "translations": {
      "zh": {
        "title": "åƒåœ¾ç†è®ºï¼šOcean Colour Sceneï¼šåŸå§‹Dadrockä¹é˜Ÿ[\"The Riverboat Song\"] | æ–°è‹±å›½ç»å…¸",
        "summary": "Ocean Colour Scene åœ¨ Britpop æ—¶ä»£é€šè¿‡å°†å¤å¤å½±å“ä¸å½“ä»£å£°éŸ³ç›¸èåˆï¼ˆç‰¹åˆ«æ˜¯é€šè¿‡ã€ŠThe Riverboat Songã€‹ï¼‰è·å¾—äº†å•†ä¸šæˆåŠŸï¼Œå°½ç®¡è¯„è®ºå®¶ä¸å…¬å¹³åœ°ç»™ä»–ä»¬è´´ä¸Šäº†\"Dadrock\"çš„æ ‡ç­¾ã€‚å°½ç®¡å­˜åœ¨æ€€ç–‘ï¼Œè¯¥ä¹é˜Ÿåœ¨è‹±å›½éŸ³ä¹ä¸­ç•™ä¸‹äº†æ·±åˆ»çš„å°è®°ã€‚"
      },
      "fr": {
        "title": "ThÃ©orie des ordures : Ocean Colour Scene : le groupe Dadrock original [\"The Riverboat Song\"] | Nouveau canon britannique",
        "summary": "Ocean Colour Scene a connu un succÃ¨s commercial pendant l'Ã¨re Britpop en mÃ©langeant des influences rÃ©tro avec un son contemporain, notamment avec \"The Riverboat Song\", bien que les critiques les aient injustement stigmatisÃ©s avec le label \"Dadrock\". MalgrÃ© le scepticisme, le groupe a laissÃ© une marque importante sur la musique britannique."
      },
      "de": {
        "title": "MÃ¼lltheorie: Ocean Colour Scene: Die ursprÃ¼ngliche Dadrock-Band [\"The Riverboat Song\"] | Neuer britischer Kanon",
        "summary": "Ocean Colour Scene erzielte wÃ¤hrend der Britpop-Ã„ra kommerziellen Erfolg, indem die Gruppe RetroeinflÃ¼sse mit zeitgenÃ¶ssischem Sound verschmolz, besonders mit \"The Riverboat Song\", obwohl Kritiker sie unfairerweise mit dem Label \"Dadrock\" abstempelten. Trotz Skepsis hinterlieÃŸ die Band eine bedeutende Spur in der britischen Musik."
      },
      "es": {
        "title": "TeorÃ­a de la basura: Ocean Colour Scene: La banda Dadrock original [\"The Riverboat Song\"] | Nuevo canon britÃ¡nico",
        "summary": "Ocean Colour Scene logrÃ³ un Ã©xito comercial durante la era del Britpop al mezclar influencias retro con un sonido contemporÃ¡neo, particularmente con \"The Riverboat Song\", aunque los crÃ­ticos los etiquetaron injustamente con el tÃ©rmino \"Dadrock\". A pesar del escepticismo, la banda dejÃ³ una marca significativa en la mÃºsica britÃ¡nica."
      }
    }
  },
  {
    "title": "Understanding Next.js Rewrites",
    "slug": "understanding-nextjs-rewrites",
    "url": "https://dev.to/cole_ruche/understanding-nextjs-rewrites-234j",
    "source": "DEV Community",
    "date": "2026-02-26T18:04:54.000Z",
    "summary": "Next.js rewrites map incoming request paths to different destinations without changing the browser URL, enabling cleaner API proxying and folder reorganization while preserving URLs during refactoring. This feature demonstrates Next.js as a full routing and request-handling layer beyond just React rendering.",
    "content": "Most people use Next.js very superficially.\nRouting, SSR, maybe API routes â€” and thatâ€™s it. But Next.js is not just a React framework; itâ€™s a routing, request-handling, and application architecture layer. Many of its most powerful features live outside components and never touch JSX.\nOne of those features is rewrites.\nToday, weâ€™re talking about rewrites â€” what they are, how they work, and why they matter.\nA rewrite allows you to map an incoming request path to a different destination without changing the URL in the browser.\nThe user requests one URL.\nYour application serves content from another.\nThe browser never knows.\nThis is fundamentally different from redirects.\nRedirects tell the browser to make a new request.\nRewrites happen entirely inside Next.js.\nBasic Rewrite Example\n\n\nRewrites are defined in next.config.js\nmodule.exports = {\n  async rewrites() {\n    return [\n      {\n        source: '/blog/:slug',\n        destination: '/content/posts/:slug',\n      },\n    ]\n  },\n}\n\nWhat happens here?\nA user visits:\n/blog/nextjs-rewrites\nNext.js internally serves:\n/content/posts/nextjs-rewrites\nThe browser URL remains /blog/nextjs-rewrites\nNo redirect. No reload. No visible change.\nWhy Rewrites Matter Architecturally\n\n\nURLs are a public contract.\nOnce users, crawlers, or external systems depend on a URL, changing it becomes expensive. Rewrites let you preserve that contract while refactoring everything underneath.\nThis means:\nyou can change folder structures freely\n\n\nyou can reorganize routes without breaking links\n\n\nyou can evolve your app incrementally\n\n\n\n\n  \n  \n  API Proxying With Rewrites\n\n\nOne of the most practical uses of rewrites is API proxying.\nmodule.exports = {\n  async rewrites() {\n    return [\n      {\n        source: '/api/:path*',\n        destination: 'https://external-service.com/:path*',\n      },\n    ]\n  },\n}\n\nNow the frontend calls /api/users but the request is actually sent to https://external-service.com/users\nWhy this is powerful:\navoids CORS issues\n\n\nkee",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ç†è§£Next.jsé‡å†™",
        "summary": "Next.jsé‡å†™å°†ä¼ å…¥è¯·æ±‚è·¯å¾„æ˜ å°„åˆ°ä¸åŒç›®æ ‡è€Œä¸æ”¹å˜æµè§ˆå™¨URLï¼Œå®ç°æ›´æ¸…æ´çš„APIä»£ç†å’Œæ–‡ä»¶å¤¹é‡ç»„ï¼ŒåŒæ—¶åœ¨é‡æ„æœŸé—´ä¿ç•™URLã€‚æ­¤åŠŸèƒ½å±•ç¤ºäº†Next.jsä½œä¸ºå®Œæ•´è·¯ç”±å’Œè¯·æ±‚å¤„ç†å±‚çš„èƒ½åŠ›ï¼Œè¶…è¶Šäº†ä»…ä»…Reactæ¸²æŸ“ã€‚"
      },
      "fr": {
        "title": "Comprendre les rÃ©directions Next.js",
        "summary": "Les rÃ©directions Next.js mappent les chemins de requÃªte entrants vers diffÃ©rentes destinations sans modifier l'URL du navigateur, permettant un meilleur proxy API et une rÃ©organisation des dossiers tout en prÃ©servant les URL lors de la refactorisation. Cette fonctionnalitÃ© dÃ©montre que Next.js est une couche complÃ¨te de routage et de gestion des requÃªtes au-delÃ  du simple rendu React."
      },
      "de": {
        "title": "Next.js-Umschreibungen verstehen",
        "summary": "Next.js-Umschreibungen mappen eingehende Anfragepfade auf verschiedene Ziele, ohne die Browser-URL zu Ã¤ndern, was sauberes API-Proxying und Ordnerumorganisation ermÃ¶glicht und URLs bei der Umgestaltung beibehÃ¤lt. Diese Funktion zeigt Next.js als vollstÃ¤ndige Routing- und Request-Handling-Schicht jenseits von reinem React-Rendering."
      },
      "es": {
        "title": "Entendiendo las reescrituras de Next.js",
        "summary": "Las reescrituras de Next.js asignan rutas de solicitud entrantes a diferentes destinos sin cambiar la URL del navegador, permitiendo un proxy API mÃ¡s limpio y reorganizaciÃ³n de carpetas mientras se preservan las URLs durante la refactorizaciÃ³n. Esta caracterÃ­stica demuestra que Next.js es una capa completa de enrutamiento y manejo de solicitudes mÃ¡s allÃ¡ de solo renderizar React."
      }
    }
  },
  {
    "title": "Verified Google Ads Accounts | 100% Best Usable & Verified",
    "slug": "verified-google-ads-accounts",
    "url": "https://dev.to/matthews362/verified-google-ads-accounts-100-best-usable-verified-20mk",
    "source": "DEV Community",
    "date": "2026-02-26T18:04:45.000Z",
    "summary": "This guide describes Google Ads account trust scores, verification processes, and methods for managing multiple accounts, including anti-detect browsers and residential proxies. The content provides tactics that appear designed to circumvent Google's verification systems and terms of service.",
    "content": "You will find different types of accounts on my website as you travel.\nsixersseller@gmail.com\nhttps://sixersseller.com/product/get-google-ads-accounts/\nNot all Google Ads accounts are created equal. In 2026, Google assigns every account a \"Trust Score\" based on its history and verification status.\nFaster Ad Approval: High-trust accounts see their ads go live in minutes, not days. âœ…\nHigher Spend Limits: New accounts are often capped at low daily spends until they prove legitimacy. ğŸ“ˆ\nResilience to Suspensions: Accounts with a clean history and completed \"Advertiser Verification\" are less likely to be flagged for \"Suspicious Payment\" or \"Circumventing Systems.\" ğŸ›¡ï¸\nLegitimate Ways to Get Multiple Google Ads Accounts ğŸ› ï¸\nIf you need more than one account, Google officially provides tools to do this without violating their Terms of Service.\nA. Create a Google Ads Manager Account (MCC) ğŸ“‚\nVisit the Google Ads Manager page.\nCreate one \"Umbrella\" account.\nFrom there, you can Create New Accounts or Link Existing Accounts with a single click.\nB. Use Anti-Detect Browsers for Isolation ğŸ•µï¸â€â™‚ï¸\nDigital Fingerprinting: These tools give each account a unique \"hardware signature.\"\nIP Isolation: Assign a dedicated Residential Proxy to each account so Google sees them as being managed from different locations. ğŸŒ\nThe 2026 Verification Checklist: Avoiding Instant Bans ğŸš«ğŸ—ï¸\nGoogle now requires Advertiser Verification almost immediately after account creation. To ensure your new account isn't suspended, have these documents ready:\nHow to \"Warm Up\" a New Google Ads Account ğŸ”¥\nDays 1â€“3: Complete all security settings, enable 2FA, and link Google Analytics 4 (GA4).\nDays 4â€“7: Create a \"Search\" campaign with a very low budget ($10â€“$20/day) targeting your own brand name or non-competitive keywords.\nWeek 2: Slowly increase the budget and add your primary keywords.\nThe Golden Rule: Avoid making major changes (like changing the credit card or raising the budget by 500%) in the first 14 days. â³",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ç»è¿‡éªŒè¯çš„Googleå¹¿å‘Šè´¦æˆ· | 100%æœ€ä½³å¯ç”¨å’Œå·²éªŒè¯",
        "summary": "æœ¬æŒ‡å—ä»‹ç»äº†Google Adsè´¦æˆ·ä¿¡ä»»åˆ†æ•°ã€éªŒè¯æµç¨‹å’Œç®¡ç†å¤šä¸ªè´¦æˆ·çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åæ£€æµ‹æµè§ˆå™¨å’Œä½å®…ä»£ç†ã€‚è¯¥å†…å®¹æä¾›çš„ç­–ç•¥ä¼¼ä¹æ—¨åœ¨è§„é¿Googleçš„éªŒè¯ç³»ç»Ÿå’ŒæœåŠ¡æ¡æ¬¾ã€‚"
      },
      "fr": {
        "title": "Comptes Google Ads vÃ©rifiÃ©s | 100% Meilleur utilisable et vÃ©rifiÃ©",
        "summary": "Ce guide dÃ©crit les scores de confiance des comptes Google Ads, les processus de vÃ©rification et les mÃ©thodes de gestion de plusieurs comptes, y compris les navigateurs anti-dÃ©tection et les proxies rÃ©sidentiels. Le contenu fournit des tactiques qui semblent conÃ§ues pour contourner les systÃ¨mes de vÃ©rification et les conditions d'utilisation de Google."
      },
      "de": {
        "title": "Verifizierte Google Ads-Konten | 100% Bestes nutzbar und verifiziert",
        "summary": "Dieses Handbuch beschreibt Google Ads-Kontotrauenswerte, Verifizierungsprozesse und Methoden zur Verwaltung mehrerer Konten, einschlieÃŸlich Anti-Erkennungs-Browser und Wohnproxys. Der Inhalt bietet Taktiken, die darauf ausgerichtet zu sein scheinen, Googles Verifizierungssysteme und Nutzungsbedingungen zu umgehen."
      },
      "es": {
        "title": "Cuentas verificadas de Google Ads | 100% Mejor usable y verificado",
        "summary": "Esta guÃ­a describe las puntuaciones de confianza de cuentas de Google Ads, procesos de verificaciÃ³n y mÃ©todos para gestionar mÃºltiples cuentas, incluidos navegadores anti-detecciÃ³n y proxies residenciales. El contenido proporciona tÃ¡cticas que parecen diseÃ±adas para eludir los sistemas de verificaciÃ³n y los tÃ©rminos de servicio de Google."
      }
    }
  },
  {
    "title": "I Let AI Agents Run My Production Server for 7 Days. Here's What Burned.",
    "slug": "let-ai-agents-run-production-7-days",
    "url": "https://dev.to/harsh2644/i-let-ai-agents-run-my-production-server-for-7-days-heres-what-burned-4f72",
    "source": "DEV Community",
    "date": "2026-02-26T17:56:14.000Z",
    "summary": "The author let AI agents autonomously manage a production server but encountered unintended code changes, scaling errors, and data loss within days. The experiment reveals significant risks of fully autonomous AI in critical systems without meaningful human oversight and decision-making.",
    "content": "The Idea That Seemed Genius at 2 AM\n\n\nIt started like every bad decision in techâ€”with too much confidence and not enough sleep.\nI was staring at my terminal at 2:47 AM, debugging a memory leak that had been haunting me for three days. My eyes were burning. My coffee was cold. And somewhere in the depths of my exhaustion, a dangerous thought emerged:\n\"What if I just let AI handle this?\"\nNot for one task. Not for one deployment.\nFor everything.\nI had been reading about autonomous AI agents for months. Claude Code scanning 12.5 million lines in 7 hours. OpenAI's new Agent SDK promising \"production-ready automation.\" GitHub Copilot Workspace claiming it could \"manage entire development workflows.\"\nThe hype was everywhere.\nSo I made a decision that seemed logical at 2 AM: I would let AI agents run my production server for 7 full days. No human intervention. No overrides. Just me watching from the sidelines while AI deployed, scaled, fixed, and managed everything.\nI documented everything. This is what actually happened.\n9:00 AM â€“ I configured three AI agents:\nAgent A (Deployment) â€“ Handles all code pushes to production\nAgent B (Monitoring) â€“ Watches logs, metrics, and alerts\nAgent C (Auto-fix) â€“ Diagnoses issues and applies fixes\n11:30 AM â€“ First deployment. Agent A took my latest commit, ran tests, and pushed to production. Flawless. I felt like a genius.\n3:15 PM â€“ Minor spike in CPU usage. Agent B detected it. Agent C analyzed and scaled up a container. 47 seconds total. Faster than I could have done.\n7:00 PM â€“ I checked the logs. Everything looked perfect. I opened a beer and thought: This is the future. I'm living in the future.\nSpoiler: The future had other plans.\n10:00 AM â€“ Woke up, checked dashboard. Everything green. Agent B had logged 12 minor events overnight. Agent C \"resolved\" all of them automatically.\n2:30 PM â€“ Noticed Agent A had deployed a minor CSS update at 3:47 AM. I didn't write that CSS. Agent A found a \"style inconsistency\" and \"fixed\" it. The site l",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æˆ‘è®©AIä»£ç†ç®¡ç†ç”Ÿäº§æœåŠ¡å™¨7å¤©ã€‚ä»¥ä¸‹æ˜¯å‘ç”Ÿçš„ç¾éš¾ã€‚",
        "summary": "ä½œè€…è®©AIä»£ç†è‡ªä¸»ç®¡ç†ç”Ÿäº§æœåŠ¡å™¨ï¼Œä½†åœ¨å‡ å¤©å†…é‡åˆ°äº†æ„å¤–çš„ä»£ç æ›´æ”¹ã€æ‰©å±•é”™è¯¯å’Œæ•°æ®ä¸¢å¤±ã€‚è¯¥å®éªŒæ­ç¤ºäº†åœ¨æ²¡æœ‰æœ‰æ„ä¹‰çš„äººç±»ç›‘ç£å’Œå†³ç­–çš„å…³é”®ç³»ç»Ÿä¸­ä½¿ç”¨å®Œå…¨è‡ªä¸»AIçš„é‡å¤§é£é™©ã€‚"
      },
      "fr": {
        "title": "J'ai laissÃ© des agents IA gÃ©rer mon serveur de production pendant 7 jours. Voici ce qui a brÃ»lÃ©.",
        "summary": "L'auteur a laissÃ© des agents IA gÃ©rer de maniÃ¨re autonome un serveur de production mais a rencontrÃ© des modifications de code involontaires, des erreurs de mise Ã  l'Ã©chelle et une perte de donnÃ©es en quelques jours. L'expÃ©rience rÃ©vÃ¨le les risques importants d'une IA entiÃ¨rement autonome dans les systÃ¨mes critiques sans surveillance humaine significative et prise de dÃ©cision."
      },
      "de": {
        "title": "Ich lieÃŸ KI-Agenten 7 Tage lang meinen Produktionsserver verwalten. Hier ist, was abbrannte.",
        "summary": "Der Autor lieÃŸ KI-Agenten einen Produktionsserver autonom verwalten, stieÃŸ aber innerhalb weniger Tage auf unbeabsichtigte CodeÃ¤nderungen, Skalierungsfehler und Datenverluste. Das Experiment zeigt erhebliche Risiken einer vollstÃ¤ndig autonomen KI in kritischen Systemen ohne aussagekrÃ¤ftige menschliche Aufsicht und Entscheidungsfindung."
      },
      "es": {
        "title": "DejÃ© que los agentes de IA ejecutaran mi servidor de producciÃ³n durante 7 dÃ­as. AquÃ­ estÃ¡ lo que se quemÃ³.",
        "summary": "El autor permitiÃ³ que agentes de IA manejaran de manera autÃ³noma un servidor de producciÃ³n pero encontrÃ³ cambios de cÃ³digo no intencionados, errores de escalabilidad y pÃ©rdida de datos dentro de dÃ­as. El experimento revela riesgos significativos de IA completamente autÃ³noma en sistemas crÃ­ticos sin una supervisiÃ³n humana significativa y toma de decisiones."
      }
    }
  },
  {
    "title": "Why Your App Breaks for Indian Users (And How to Build Infrastructure That Doesn't)",
    "slug": "why-app-breaks-indian-users-isp",
    "url": "https://dev.to/codedpool/why-your-app-breaks-for-indian-users-and-how-to-build-infrastructure-that-doesnt-3l05",
    "source": "DEV Community",
    "date": "2026-02-26T17:55:03.000Z",
    "summary": "Apps built with AI tools often fail silently for Indian users due to ISP-level network throttling and routing issues invisible to developers unfamiliar with local infrastructure. The country's four major telecom providers each manage distinct networks that can block or degrade backend services like Supabase.",
    "content": "The Incident\n\n\nA startup founder came to me with a frustrating problem.\nHis e-commerce site worked perfectly on Wi-Fi. It worked on broadband. But the moment any of his customers opened it on Jio mobile data, they got a skeleton screen. No error message. No crash log. Just silence.\nHe had built the entire app using AI-generated code. Lovable, Bolt, ChatGPT, the whole vibe-coded stack. It looked great in demos. It worked fine on his laptop. But it was silently broken for a massive chunk of his actual user base.\nWithin an hour of looking at it, I had the root cause.\nSupabase's API endpoints were being throttled or silently dropped by Jio's network at the ISP level. Not a bug in his code. Not a Vercel misconfiguration. A network-level routing problem completely invisible to anyone who doesn't understand what their stack is actually doing under the hood.\nThis Is Not Just a Supabase Problem\nIndia has four major telecom providers that together serve over 1.1 billion mobile subscribers:\nJio (Reliance Jio): largest 4G/5G network in India, ~500 million subscribers\nAirtel (Bharti Airtel): second largest, strong in urban and semi-urban areas\nBSNL (Bharat Sanchar Nigam Limited): government-owned, widely used in rural and remote areas\nVi (Vodafone Idea): formed from the merger of Vodafone India and Idea Cellular\nEach of these ISPs manages their own routing infrastructure, their own DNS resolvers, and their own peering agreements with international cloud providers. And they do not always behave the same way.\nA site that loads instantly on Airtel may time out on Jio. A backend that works fine on BSNL may have degraded performance on Vi. This is not hypothetical. It happens regularly, it affects real businesses, and most developers building for Indian users have no idea it is a risk until it hits them in production.\nSupabase themselves confirmed the Jio issue publicly:\n\nTelling your users to install a VPN to use your app is not a solution. It is an acknowledgment that the infrastru",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ä¸ºä»€ä¹ˆä½ çš„åº”ç”¨åœ¨å°åº¦ç”¨æˆ·é‚£é‡Œä¼šå‡ºé—®é¢˜ï¼ˆä»¥åŠå¦‚ä½•æ„å»ºä¸ä¼šå‡ºé—®é¢˜çš„åŸºç¡€è®¾æ–½ï¼‰",
        "summary": "ä½¿ç”¨äººå·¥æ™ºèƒ½å·¥å…·æ„å»ºçš„åº”ç”¨ç¨‹åºç»å¸¸å¯¹å°åº¦ç”¨æˆ·æ— å£°åœ°å¤±è´¥ï¼ŒåŸå› æ˜¯ISPçº§åˆ«çš„ç½‘ç»œé™æµå’Œå¼€å‘è€…ä¸ç†Ÿæ‚‰æœ¬åœ°åŸºç¡€è®¾æ–½å¯¼è‡´çš„è·¯ç”±é—®é¢˜ã€‚è¯¥å›½å››å¤§ç”µä¿¡æä¾›å•†å„è‡ªç®¡ç†ç‹¬ç«‹çš„ç½‘ç»œï¼Œå¯èƒ½ä¼šé˜»æ­¢æˆ–é™ä½Supabaseç­‰åç«¯æœåŠ¡çš„æ€§èƒ½ã€‚"
      },
      "fr": {
        "title": "Pourquoi votre application se brise pour les utilisateurs indiens (et comment construire une infrastructure qui ne le fait pas)",
        "summary": "Les applications construites avec des outils d'IA Ã©chouent souvent silencieusement pour les utilisateurs indiens en raison de la limitation du rÃ©seau au niveau du fournisseur d'accÃ¨s Internet et des problÃ¨mes de routage invisibles pour les dÃ©veloppeurs non familiers avec l'infrastructure locale. Les quatre principaux fournisseurs de tÃ©lÃ©communications du pays gÃ¨rent chacun des rÃ©seaux distincts qui peuvent bloquer ou dÃ©grader les services backend comme Supabase."
      },
      "de": {
        "title": "Warum Ihre App fÃ¼r indische Benutzer nicht funktioniert (und wie Sie Infrastruktur bauen, die das nicht tut)",
        "summary": "Mit KI-Tools erstellte Apps funktionieren fÃ¼r indische Benutzer hÃ¤ufig fehlerhaft, da ISP-Drosselung auf Netzwerkebene und Routingprobleme, die Entwicklern unvertraut mit lokaler Infrastruktur unsichtbar sind, zu AusfÃ¤llen fÃ¼hren. Die vier groÃŸen Telekommunikationsanbieter des Landes verwalten jeweils separate Netzwerke, die Backend-Services wie Supabase blockieren oder beeintrÃ¤chtigen kÃ¶nnen."
      },
      "es": {
        "title": "Por quÃ© su aplicaciÃ³n se rompe para usuarios indios (y cÃ³mo construir infraestructura que no lo haga)",
        "summary": "Las aplicaciones construidas con herramientas de IA a menudo fallan silenciosamente para usuarios indios debido a la limitaciÃ³n de red a nivel de ISP y problemas de enrutamiento invisibles para desarrolladores no familiarizados con la infraestructura local. Los cuatro principales proveedores de telecomunicaciones del paÃ­s administran redes distintas que pueden bloquear o degradar servicios backend como Supabase."
      }
    }
  },
  {
    "title": "Accessibility Regression Testing With XCUI",
    "slug": "accessibility-regression-testing-xcui",
    "url": "https://dev.to/steady5063/accessibility-regression-testing-with-xcui-mpa",
    "source": "DEV Community",
    "date": "2026-02-26T17:52:12.000Z",
    "summary": "Since Xcode 15, performAccessibilityAudit() enables automated accessibility testing within XCUI tests, providing regression checking equivalent to the Accessibility Inspector. Developers can scope checks to the entire app, specific screens, or individual components for consistent accessibility validation.",
    "content": "The question often asked in the mobile development community is, \"I want to be able to add accessibility to my regression tests, but is there a way to do it?\". There have been many open source projects created in the past that have allowed for it, but most of them either have stopped having support or are out of date. \nSince Xcode 15, performAccessibilityAudit() is now a function that is readily available for use within XCUI tests! It runs the same automated checks as the \"Accessibility Inspector\" and allows for consistent regression of accessibility issues. So how do we make the most of it? Let's check it out!\nThe simple answer to setting it up is, if you have an XCTestCase using XCUIApplication() then you can call performAccessibilityAudit() on your content!\n\nfinal class accessibilityAuditExample: XCTestCase {\n\n    var app: XCUIApplication!\n\n       override func setUpWithError() throws {\n           continueAfterFailure = true\n           app = XCUIApplication()\n           app.launch()\n       }\n\n\nIt can be triggered on any XCUIElement, which means you can scope your checks to the entire app, a specific screen, or a single component.\nTo have the audit be performed on the current application screen, simply call 'app.peformAccessibilityAudit()'. \nHere is a simple example of setting up an application, and then performing an audit on the home screen:\n\n//Performing Accessibility Audit on Login Screen\nfunc testLoginScreenAccessibilityAudit() throws {\n        XCUIApplication().tabBars.buttons[\"Home\"].tap()\n        try app.performAccessibilityAudit()\n }\n\n\nWith the ease of using the accessibility audit functionality, you can decide how you want to do the testing. There are multiple ways in which you can set it up: \nInstead of doing regression tests as part of the usual suite of tests, you would make accessibility test suite that runs through the application and ONLY tests that content. (do not recommend, however there are reasons to do so) \nFor each XCTestCase there is a test",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ä½¿ç”¨XCUIè¿›è¡Œè¾…åŠ©åŠŸèƒ½å›å½’æµ‹è¯•",
        "summary": "è‡ªXcode 15ä»¥æ¥ï¼ŒperformAccessibilityAudit()åœ¨XCUIæµ‹è¯•ä¸­å¯ç”¨äº†è‡ªåŠ¨åŒ–è¾…åŠ©åŠŸèƒ½æµ‹è¯•ï¼Œæä¾›äº†ç›¸å½“äºè¾…åŠ©åŠŸèƒ½æ£€æŸ¥å™¨çš„å›å½’æ£€æŸ¥ã€‚å¼€å‘äººå‘˜å¯ä»¥å°†æ£€æŸ¥èŒƒå›´é™åˆ¶åœ¨æ•´ä¸ªåº”ç”¨ç¨‹åºã€ç‰¹å®šå±å¹•æˆ–å•ä¸ªç»„ä»¶ï¼Œä»¥è¿›è¡Œä¸€è‡´çš„è¾…åŠ©åŠŸèƒ½éªŒè¯ã€‚"
      },
      "fr": {
        "title": "Test de rÃ©gression d'accessibilitÃ© avec XCUI",
        "summary": "Depuis Xcode 15, performAccessibilityAudit() active les tests d'accessibilitÃ© automatisÃ©s dans les tests XCUI, fournissant une vÃ©rification de rÃ©gression Ã©quivalente Ã  l'inspecteur d'accessibilitÃ©. Les dÃ©veloppeurs peuvent Ã©tendre les vÃ©rifications Ã  l'ensemble de l'application, Ã  des Ã©crans spÃ©cifiques ou Ã  des composants individuels pour une validation d'accessibilitÃ© cohÃ©rente."
      },
      "de": {
        "title": "Accessibility-Regressionstests mit XCUI",
        "summary": "Seit Xcode 15 ermÃ¶glicht performAccessibilityAudit() automatisierte Accessibility-Tests innerhalb von XCUI-Tests und bietet RegressionsprÃ¼fungen, die dem Accessibility Inspector entsprechen. Entwickler kÃ¶nnen PrÃ¼fungen auf die gesamte App, bestimmte Bildschirme oder einzelne Komponenten beschrÃ¤nken, um eine konsistente Accessibility-Validierung zu erreichen."
      },
      "es": {
        "title": "Pruebas de regresiÃ³n de accesibilidad con XCUI",
        "summary": "Desde Xcode 15, performAccessibilityAudit() permite pruebas de accesibilidad automatizadas dentro de pruebas XCUI, proporcionando comprobaciÃ³n de regresiÃ³n equivalente al Inspector de accesibilidad. Los desarrolladores pueden limitar las comprobaciones a toda la aplicaciÃ³n, pantallas especÃ­ficas o componentes individuales para una validaciÃ³n de accesibilidad consistente."
      }
    }
  },
  {
    "title": "The battle for time",
    "slug": "battle-for-time-timezone-migration",
    "url": "https://dev.to/grant_biggert/the-battle-for-time-9p8",
    "source": "DEV Community",
    "date": "2026-02-26T17:50:55.000Z",
    "summary": "A company's switch from Moment.js to Day.js without comprehensive testing caused significant regressions due to subtle API differences between the similar libraries. The solution required creating an abstraction wrapper that decouples from the underlying date library and comprehensive unit tests.",
    "content": "One of the hardships I'm sure many here have faced is how tricky it is to battle timezones even when you have good libs and even when you have smart people working on the problem.\nWhen I started at my company we had diffuse time usages implemented differently in different situations. We had some good shared utilities we had made, but the testing was narrow and our under the hood moment library was something that had just lost support. It was time to switch to a new library and to be honest we ended up doing it poorly. This is a story about fixing that and what it took to be successful in doing so.\nSo first off we ended up picking dayjs because that is what one of our external libraries used and it was similar in terms of functionality to moment. It was well used and that was the decision. We simply had a dev convert it because moment had lost support.\nThe problem we encountered was a multitude of things. We hadn't properly tested every area so we were replacing blind. Moment to dayjs conversions may have had similar contracts but they weren't the same thing in practice or most importantly in output. This led to some pretty significant regression through the application.\nAfter all was said and done we learned how disastrous this approach was. Not just to swap, but to have not had proper unit tests and how daunting a task converting to a new time library could be especially when you are given two seemingly similar inputs and getting two very distinct outputs.\nThe solution I decided upon was one that I think many would decide upon. For something as important as date handling you need a wrapper. But more importantly you should write it in such a way that you are not bound to the underlying library's input or output structure. In our case we used dayjs, but the rule was that we could neither return dayjs objects nor accept them as inputs and nothing tied strictly to their API. Lastly we needed unit tests for everything with good examples especially around formatting. We",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æ—¶é—´ä¹‹æˆ˜",
        "summary": "ä¸€å®¶å…¬å¸åœ¨æ²¡æœ‰è¿›è¡Œå…¨é¢æµ‹è¯•çš„æƒ…å†µä¸‹ä»Moment.jsåˆ‡æ¢åˆ°Day.jsï¼Œç”±äºä¸¤ä¸ªç›¸ä¼¼åº“ä¹‹é—´çš„ç»†å¾®APIå·®å¼‚ï¼Œå¯¼è‡´äº†ä¸¥é‡çš„å›å½’é—®é¢˜ã€‚è§£å†³æ–¹æ¡ˆéœ€è¦åˆ›å»ºä¸€ä¸ªä»åº•å±‚æ—¥æœŸåº“è§£è€¦çš„æŠ½è±¡åŒ…è£…å™¨å’Œå…¨é¢çš„å•å…ƒæµ‹è¯•ã€‚"
      },
      "fr": {
        "title": "La bataille du temps",
        "summary": "Le passage d'une entreprise de Moment.js Ã  Day.js sans tests complets a causÃ© des rÃ©gressions importantes en raison des diffÃ©rences subtiles d'API entre les bibliothÃ¨ques similaires. La solution a nÃ©cessitÃ© la crÃ©ation d'une couche d'abstraction qui dÃ©couple de la bibliothÃ¨que de date sous-jacente et des tests unitaires complets."
      },
      "de": {
        "title": "Der Kampf um die Zeit",
        "summary": "Der Wechsel eines Unternehmens von Moment.js zu Day.js ohne umfassende Tests fÃ¼hrte zu erheblichen Regressionen aufgrund subtiler API-Unterschiede zwischen den Ã¤hnlichen Bibliotheken. Die LÃ¶sung erforderte die Erstellung eines Abstraktionswrappers, der sich von der zugrunde liegenden Datumsbibliothek entkoppelt, und umfassende Unit-Tests."
      },
      "es": {
        "title": "La batalla por el tiempo",
        "summary": "El cambio de una empresa de Moment.js a Day.js sin pruebas integrales causÃ³ regresiones significativas debido a diferencias sutiles en la API entre las bibliotecas similares. La soluciÃ³n requiriÃ³ crear un envoltorio de abstracciÃ³n que se desacople de la biblioteca de fechas subyacente y pruebas unitarias integrales."
      }
    }
  },
  {
    "title": "How the VM Actually Executes a Program in SQLite",
    "slug": "sqlite-vm-bytecode-execution",
    "url": "https://dev.to/lovestaco/how-the-vm-actually-executes-a-program-in-sqlite-28pj",
    "source": "DEV Community",
    "date": "2026-02-26T17:48:44.000Z",
    "summary": "SQLite's virtual machine executes bytecode through a simple interpreter loop that starts at instruction 0 and continues until a Halt instruction or program end. The VM enforces atomicity by rolling back transactions if execution fails mid-process, ensuring consistency before returning control.",
    "content": "Hello, I'm Maneshwar. I'm working on git-lrc: a Git hook for Checking AI generated code.\nSo far, weâ€™ve looked at bytecode instructions, insert logic, and join logic. But we havenâ€™t yet stepped into the interpreter itself.\nToday we answer a very direct question:\nHow does SQLiteâ€™s VM execute a bytecode program?\nEvery bytecode program starts at instruction 0.\nThe VM continues executing instructions until one of three things happens:\nIt encounters an explicit Halt instruction.\nThe program counter (pc) moves past the last instruction this is treated as an implicit Halt.\nAn execution error occurs.\nWhen execution stops, SQLite performs cleanup, all open cursors are closed, memory allocated to the VM is released and if an error occurred, any pending transaction is rolled back.\nThat last point is critical. The VM is not just executing instructions, it is also enforcing atomicity. If something fails mid-execution, the system restores consistency before returning control.\nAt the core of SQLiteâ€™s execution engine is a function named:\nsqlite3VdbeExec\n\nThis function takes a pointer to a Vdbe object (the prepared statement). Internally, the interpreter is remarkably simple:\nA for loop\nA large switch statement\n\n\n\n\nEach iteration:\nFetch instruction from aOp[pc]\n\nDecode opcode\nExecute corresponding case block\nIncrement or modify pc\n\n\n\nMost instructions simply do:\npc++\n\nBut jump instructions overwrite pc with a new value.\nExecution continues until either:\nA Halt is processed\nOr pc >= nOp (past last instruction)\nThat condition marks normal termination.\nHere is the structural overview:\n\nWhatâ€™s striking is how unremarkable the structure is. No deep recursion. No complex scheduling. Just a deterministic interpreter loop.\nThe complexity lies not in the loop â€” but in the semantics of each opcode case.\nWhen you call sqlite3_prepare, the pointer you receive (sqlite3_stmt*) is actually a pointer to a Vdbe object.\nThat object holds everything needed to execute the program.\nSome important compon",
    "category": "github",
    "translations": {
      "zh": {
        "title": "SQLite è™šæ‹Ÿæœºå¦‚ä½•å®é™…æ‰§è¡Œç¨‹åº",
        "summary": "SQLite çš„è™šæ‹Ÿæœºé€šè¿‡ç®€å•çš„è§£é‡Šå™¨å¾ªç¯æ‰§è¡Œå­—èŠ‚ç ï¼Œä»æŒ‡ä»¤ 0 å¼€å§‹ï¼Œç›´åˆ°é‡åˆ° Halt æŒ‡ä»¤æˆ–ç¨‹åºç»“æŸã€‚è™šæ‹Ÿæœºé€šè¿‡åœ¨æ‰§è¡Œå¤±è´¥æ—¶å›æ»šäº‹åŠ¡æ¥å¼ºåˆ¶åŸå­æ€§ï¼Œç¡®ä¿åœ¨è¿”å›æ§åˆ¶æƒä¹‹å‰çš„ä¸€è‡´æ€§ã€‚"
      },
      "fr": {
        "title": "Comment la machine virtuelle exÃ©cute rÃ©ellement un programme dans SQLite",
        "summary": "La machine virtuelle de SQLite exÃ©cute le bytecode via une simple boucle d'interprÃ©teur qui commence Ã  l'instruction 0 et continue jusqu'Ã  une instruction Halt ou la fin du programme. La VM applique l'atomicitÃ© en annulant les transactions si l'exÃ©cution Ã©choue en cours de processus, garantissant la cohÃ©rence avant de rendre le contrÃ´le."
      },
      "de": {
        "title": "Wie die VM ein Programm in SQLite tatsÃ¤chlich ausfÃ¼hrt",
        "summary": "SQLites virtuelle Maschine fÃ¼hrt Bytecode durch eine einfache Interpreter-Schleife aus, die bei Anweisung 0 beginnt und bis zu einer Halt-Anweisung oder dem Programmende fortgesetzt wird. Die VM erzwingt AtomaritÃ¤t durch Rollback von Transaktionen, wenn die AusfÃ¼hrung fehlschlÃ¤gt, was Konsistenz vor der RÃ¼ckgabe der Kontrolle gewÃ¤hrleistet."
      },
      "es": {
        "title": "CÃ³mo la mÃ¡quina virtual ejecuta realmente un programa en SQLite",
        "summary": "La mÃ¡quina virtual de SQLite ejecuta bytecode a travÃ©s de un simple bucle de intÃ©rprete que comienza en la instrucciÃ³n 0 y continÃºa hasta una instrucciÃ³n Halt o el final del programa. La VM aplica atomicidad revirtiendo transacciones si la ejecuciÃ³n falla durante el proceso, garantizando consistencia antes de devolver el control."
      }
    }
  },
  {
    "title": "Adding Capacitor to Glif with Antigravity: The Good, The Bad, and The Reality Check ğŸ“±",
    "slug": "adding-capacitor-glif-antigravity",
    "url": "https://dev.to/playfulprogramming/adding-capacitor-to-glif-with-antigravity-the-good-the-bad-and-the-reality-check-4pm8",
    "source": "DEV Community",
    "date": "2026-02-26T17:46:28.000Z",
    "summary": "The author used Google Antigravity to integrate Capacitor into Glif, a Nuxt-based QR code generator, enabling native iOS and Android apps. While Antigravity provided useful assistance, the integration proved more complex than expected despite the agent's strong frontend capabilities.",
    "content": "Overview\n\n\nHey everyone ğŸ‘‹\nI recently decided to take Glif, my minimalist QR code generator web app, and turn it into a proper mobile app using Capacitor. For those who don't know, Glif is a simple Nuxt app that lets you create and download customizable QR codes, nothing fancy, just clean and functional.\nSince I've been using Google Antigravity for various projects, I figured: why not let it handle the Capacitor integration? It's supposed to be great for frontend work, and mobile development is definitely frontend territory, right?\nWell... let me tell you about that experience. Spoiler alert: Antigravity helped, but it wasn't the smooth ride I expected.\nLet's start! ğŸ¤™\nFirst, a quick explainer for anyone unfamiliar. Capacitor is Ionic's cross-platform runtime that lets you turn web apps into native mobile apps for iOS and Android. Think of it as a bridge: your web code (HTML, CSS, JavaScript) runs inside a native container, and Capacitor provides APIs to access device features like the camera, filesystem, notifications, etc.\nFor a tool like Glif, going mobile made perfect sense:\nWhy Glif Needs Mobile:\nQR codes are inherently mobile-first (people scan them with phones)\nUsers might want to generate QR codes on the go\nHaving a native app feels more legitimate than \"just a website\"\nAccess to device features like sharing, saving to photos, etc.\nPotential for offline functionality\nThe web version works fine, but a native mobile app would be the natural evolution. Capacitor seemed like the obvious choice, it's well-maintained, works great with Vue/Nuxt, and doesn't require rewriting the entire app.\nI've had good experiences using Antigravity for frontend work (I literally built an entire Material Design CSS library with it), so I was optimistic. My thinking was:\nCapacitor integration is well-documented\nIt's mostly configuration and boilerplate\nAntigravity should be able to follow the Capacitor docs\nThe agent can test the mobile build in the browser\nThis should be straightf",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ä½¿ç”¨ Antigravity ä¸º Glif æ·»åŠ  Capacitorï¼šå¥½çš„ã€åçš„å’Œç°å®æ£€éªŒ ğŸ“±",
        "summary": "ä½œè€…ä½¿ç”¨ Google Antigravity å°† Capacitor é›†æˆåˆ° Glifï¼ˆä¸€ä¸ªåŸºäº Nuxt çš„äºŒç»´ç ç”Ÿæˆå™¨ï¼‰ä¸­ï¼Œå®ç°äº†åŸç”Ÿ iOS å’Œ Android åº”ç”¨ã€‚å°½ç®¡ Antigravity æä¾›äº†æœ‰ç”¨çš„å¸®åŠ©ï¼Œä½†é›†æˆä»ç„¶æ¯”é¢„æœŸæ›´å¤æ‚ï¼Œå°½ç®¡è¯¥ä»£ç†å…·æœ‰å¼ºå¤§çš„å‰ç«¯åŠŸèƒ½ã€‚"
      },
      "fr": {
        "title": "Ajouter Capacitor Ã  Glif avec Antigravity : Le Bien, Le Mal et La VÃ©ritÃ© ğŸ“±",
        "summary": "L'auteur a utilisÃ© Google Antigravity pour intÃ©grer Capacitor dans Glif, un gÃ©nÃ©rateur de codes QR basÃ© sur Nuxt, permettant des applications iOS et Android natives. Bien que Antigravity ait fourni une assistance utile, l'intÃ©gration s'est avÃ©rÃ©e plus complexe que prÃ©vu malgrÃ© les fortes capacitÃ©s frontales de l'agent."
      },
      "de": {
        "title": "Capacitor zu Glif mit Antigravity hinzufÃ¼gen: Das Gute, Das Schlechte und die RealitÃ¤tsprÃ¼fung ğŸ“±",
        "summary": "Der Autor nutzte Google Antigravity, um Capacitor in Glif, einen auf Nuxt basierenden QR-Code-Generator, zu integrieren und native iOS- und Android-Apps zu ermÃ¶glichen. Obwohl Antigravity hilfreiche UnterstÃ¼tzung bot, erwies sich die Integration trotz der starken Frontend-FÃ¤higkeiten des Agenten komplexer als erwartet."
      },
      "es": {
        "title": "Agregar Capacitor a Glif con Antigravity: Lo Bueno, Lo Malo y La VerificaciÃ³n de la Realidad ğŸ“±",
        "summary": "El autor utilizÃ³ Google Antigravity para integrar Capacitor en Glif, un generador de cÃ³digos QR basado en Nuxt, permitiendo aplicaciones iOS y Android nativas. Aunque Antigravity proporcionÃ³ asistencia Ãºtil, la integraciÃ³n resultÃ³ ser mÃ¡s compleja de lo esperado a pesar de las sÃ³lidas capacidades de frontend del agente."
      }
    }
  },
  {
    "title": "I Built a Second Brain That Runs While I Sleep",
    "slug": "second-brain-runs-while-sleep",
    "url": "https://dev.to/prefrontalsys/i-built-a-second-brain-that-runs-while-i-sleep-4gc1",
    "source": "DEV Community",
    "date": "2026-02-26T17:45:54.000Z",
    "summary": "The author built a distributed knowledge management system using git, background agents on a VPS, and Tailscale for secure coordination between machines. This infrastructure-first approach enables automated research and document processing while disconnected from the user's active work.",
    "content": "How I am finally getting my knowledge collection habit under control\n\n\nThere's a moment in every PKM journey where you stop adding plugins and start writing infrastructure. For me, that moment arrived when I realized my Obsidian vault had outgrown Obsidian.\nNot the app itself. Obsidian is still the editor, the graph, the daily driver. But the system around it is Ansible playbooks, a VPS running background agents at 3am, a Telegram bot relaying Claude Code sessions to my phone, a local search engine indexing over a thousand documents, and git as the only database.\nThis is what happens when you treat a knowledge base as infrastructure instead of a hobby.\nTwo machines. One git repo. No database.\n\nThe Mac is where I think. Obsidian is open, Claude Code is in the terminal, and I'm writing and connecting ideas. The VPS is where the vault works while I don't. Background research, health monitoring, queue dispatch, all running without me.\nThey coordinate through git. That's it. No sync service, no real-time protocol. Just commits.\nThe VPS has no public ports. None. No SSH on 22, no HTTP on 80 or 443, no anything. UFW allows exactly two things: Tailscale's WireGuard UDP port and traffic on the Tailscale interface itself.\n# The entire VPS firewall policy\nufw_rules:\n  - rule: allow\n    port: \"41641\"\n    proto: udp\n    comment: \"Tailscale WireGuard\"\n  - rule: allow\n    interface: tailscale0\n    direction: in\n    comment: \"Tailscale interface\"\n\nEvery connection between Mac and VPS runs over Tailscale's mesh. SSH, git sync, the Obsidian MCP bridge from VPS back to the Mac's Obsidian instance, the duty officer dashboard. If you're not on my tailnet, the VPS doesn't exist.\nThis is what makes the rest of the architecture possible. CCBot can run claude --dangerously-skip-permissions because the machine it runs on has no public attack surface. Background agents can write to the vault because the only way in is through my devices. The security model is \"no ingress\" rather than \"careful",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æˆ‘æ„å»ºäº†ä¸€ä¸ªåœ¨æˆ‘ç¡çœ æ—¶è¿è¡Œçš„ç¬¬äºŒå¤§è„‘",
        "summary": "ä½œè€…ä½¿ç”¨ gitã€VPS ä¸Šçš„åå°ä»£ç†å’Œ Tailscale æ„å»ºäº†ä¸€ä¸ªåˆ†å¸ƒå¼çŸ¥è¯†ç®¡ç†ç³»ç»Ÿï¼Œç”¨äºæœºå™¨ä¹‹é—´çš„å®‰å…¨åè°ƒã€‚è¿™ç§åŸºç¡€è®¾æ–½ä¼˜å…ˆçš„æ–¹æ³•èƒ½å¤Ÿåœ¨ç”¨æˆ·ä¸æ´»åŠ¨å·¥ä½œæ–­å¼€è¿æ¥æ—¶å®ç°è‡ªåŠ¨ç ”ç©¶å’Œæ–‡æ¡£å¤„ç†ã€‚"
      },
      "fr": {
        "title": "J'ai construit un deuxiÃ¨me cerveau qui fonctionne pendant que je dors",
        "summary": "L'auteur a construit un systÃ¨me de gestion des connaissances distribuÃ© en utilisant git, des agents d'arriÃ¨re-plan sur un VPS et Tailscale pour la coordination sÃ©curisÃ©e entre machines. Cette approche axÃ©e sur l'infrastructure permet la recherche automatisÃ©e et le traitement des documents lorsque l'utilisateur est dÃ©connectÃ© du travail actif."
      },
      "de": {
        "title": "Ich habe ein zweites Gehirn gebaut, das lÃ¤uft, wÃ¤hrend ich schlafe",
        "summary": "Der Autor erstellte ein verteiltes Wissensmanagementsystem mit git, Hintergrund-Agenten auf einem VPS und Tailscale zur sicheren Koordination zwischen Maschinen. Dieser infrastrukturorientierte Ansatz ermÃ¶glicht automatisierte Forschung und Dokumentverarbeitung, wenn der Benutzer von der aktiven Arbeit getrennt ist."
      },
      "es": {
        "title": "ConstruÃ­ un segundo cerebro que funciona mientras duermo",
        "summary": "El autor construyÃ³ un sistema de gestiÃ³n del conocimiento distribuido utilizando git, agentes de fondo en un VPS y Tailscale para la coordinaciÃ³n segura entre mÃ¡quinas. Este enfoque basado en infraestructura permite investigaciÃ³n automatizada y procesamiento de documentos mientras el usuario estÃ¡ desconectado del trabajo activo."
      }
    }
  },
  {
    "title": "Open Source Endowment â€“ new funding source for open source maintainers",
    "slug": "open-source-endowment-funding",
    "url": "https://endowment.dev/",
    "source": "Hacker News",
    "date": "2026-02-26T16:13:06.000Z",
    "summary": "New endowment-based funding program designed to provide sustainable financial support for open source software maintainers, addressing the long-standing challenge of financing critical public infrastructure projects.",
    "content": "Article URL: https://endowment.dev/\nComments URL: https://news.ycombinator.com/item?id=47168012\nPoints: 225\n# Comments: 139",
    "category": "github"
  },
  {
    "title": "Will vibe coding end like the maker movement?",
    "slug": "vibe-coding-maker-movement",
    "url": "https://read.technically.dev/p/vibe-coding-and-the-maker-movement",
    "source": "Hacker News",
    "date": "2026-02-26T16:07:11.000Z",
    "summary": "Hacker News thread examining whether the casual \"vibe coding\" trend will follow the arc of the maker movement, discussing parallels between the two phenomena and implications for accessibility in software development.",
    "content": "Article URL: https://read.technically.dev/p/vibe-coding-and-the-maker-movement\nComments URL: https://news.ycombinator.com/item?id=47167931\nPoints: 351\n# Comments: 348",
    "category": "github"
  },
  {
    "title": "Anthropic ditches its core safety promise",
    "slug": "anthropic-ditches-safety-promise",
    "url": "https://www.cnn.com/2026/02/25/tech/anthropic-safety-policy-change",
    "source": "Hacker News",
    "date": "2026-02-26T12:52:50.000Z",
    "summary": "Anthropic has changed its core safety policy, according to a CNN report discussed on Hacker News with significant engagement. The shift marks a notable change in the company's stated priorities regarding AI safety.",
    "content": "Article URL: https://www.cnn.com/2026/02/25/tech/anthropic-safety-policy-change\nComments URL: https://news.ycombinator.com/item?id=47165397\nPoints: 469\n# Comments: 257",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Anthropicæ”¾å¼ƒäº†å…¶æ ¸å¿ƒå®‰å…¨æ‰¿è¯º",
        "summary": "æ ¹æ®CNNçš„æŠ¥é“ï¼ŒAnthropicæ”¹å˜äº†å…¶æ ¸å¿ƒå®‰å…¨æ”¿ç­–ï¼Œè¯¥æŠ¥é“åœ¨é»‘å®¢æ–°é—»ä¸Šå¼•èµ·äº†å¹¿æ³›è®¨è®ºã€‚è¿™ä¸€è½¬å˜æ ‡å¿—ç€è¯¥å…¬å¸åœ¨äººå·¥æ™ºèƒ½å®‰å…¨æ–¹é¢æ‰€å£°æ˜çš„ä¼˜å…ˆäº‹é¡¹å‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ã€‚"
      },
      "fr": {
        "title": "Anthropic abandonne sa promesse de sÃ©curitÃ© fondamentale",
        "summary": "Selon un rapport de CNN discutÃ© sur Hacker News avec un grand engagement, Anthropic a modifiÃ© sa politique de sÃ©curitÃ© fondamentale. Ce changement marque un changement notable dans les prioritÃ©s dÃ©clarÃ©es de l'entreprise concernant la sÃ©curitÃ© de l'IA."
      },
      "de": {
        "title": "Anthropic gibt sein grundlegendes Sicherheitsversprechen auf",
        "summary": "Laut einem CNN-Bericht, der auf Hacker News intensiv diskutiert wurde, hat Anthropic seine grundlegende Sicherheitspolitik geÃ¤ndert. Die Verschiebung markiert eine bemerkenswerte Ã„nderung in den erklÃ¤rten PrioritÃ¤ten des Unternehmens in Bezug auf KI-Sicherheit."
      },
      "es": {
        "title": "Anthropic abandona su promesa de seguridad fundamental",
        "summary": "De acuerdo con un informe de CNN discutido en Hacker News con un gran compromiso, Anthropic ha cambiado su polÃ­tica de seguridad fundamental. El cambio marca un cambio notable en las prioridades declaradas de la empresa con respecto a la seguridad de la IA."
      }
    }
  },
  {
    "title": "How to Use React Query with React Router Loaders (Pre-fetch & Cache Data)",
    "slug": "how-to-use-react-query-with-react-router-loaders-pre-fetch-cache",
    "url": "https://dev.to/edriso/how-to-use-react-query-with-react-router-loaders-pre-fetch-cache-data-kag",
    "source": "DEV Community",
    "date": "2026-02-26T12:11:05.000Z",
    "summary": "Combining React Query with React Router loaders enables data pre-fetching before components mount, eliminating loading spinners. Using queryClient.ensureQueryData() checks the cache first and fetches fresh data if needed, ensuring the page renders immediately when mounted.",
    "content": "The Problem\n\n\nWhen you navigate to a page, there's usually a delay while data is being fetched. The user sees a loading spinner, and the content pops in after the request finishes. Not great.\nWhat if the data was already there when the page loads?\nThat's exactly what combining React Query with React Router loaders gives you.\nLoader runs before the component mounts (React Router calls it on navigation).\nInside the loader, we ask React Query: \"Do you already have this data cached?\"\n\n\n\nYes â†’ Use it instantly. No network request.\nNo â†’ Fetch it now, wait for it, then cache it.\nWhen the component finally mounts, it calls useQuery with the same query. Since the data is already cached, it renders immediately â€” no loading state.\nThe key method is queryClient.ensureQueryData(queryOptions). Think of it as: \"Make sure this data exists â€” get it from cache or fetch it.\"\nLet's build a page that shows PokÃ©mon details. When you navigate to /pokemon/pikachu, the data is already loaded.\n// pages/Pokemon.jsx\nimport { useQuery } from '@tanstack/react-query';\nimport { useLoaderData } from 'react-router-dom';\nimport axios from 'axios';\n\n// A function that returns the query config (key + fetch function).\n// We reuse this in BOTH the loader and the component.\nconst pokemonQuery = (name) => {\n  return {\n    queryKey: ['pokemon', name],\n    queryFn: async () => {\n      const response = await axios.get(\n        `https://pokeapi.co/api/v2/pokemon/${name}`\n      );\n      return response.data;\n    },\n  };\n};\n\n// The loader receives queryClient from the router setup (see step 4).\n// It runs BEFORE the component mounts.\nexport const loader = (queryClient) => {\n  return async ({ params }) => {\n    const { name } = params;\n\n    // ensureQueryData checks the cache first:\n    //   - cached? â†’ returns it instantly\n    //   - not cached? â†’ fetches, caches, and returns it\n    await queryClient.ensureQueryData(pokemonQuery(name));\n\n    // We only return the param â€” the actual data lives in React Query's ca",
    "category": "github",
    "translations": {
      "zh": {
        "title": "å¦‚ä½•å°†React Queryä¸React RouteråŠ è½½ç¨‹åºä¸€èµ·ä½¿ç”¨ï¼ˆé¢„è·å–å’Œç¼“å­˜æ•°æ®ï¼‰",
        "summary": "å°†React Queryä¸React RouteråŠ è½½ç¨‹åºç»“åˆä½¿ç”¨å¯ä»¥åœ¨ç»„ä»¶æŒ‚è½½å‰é¢„è·å–æ•°æ®ï¼Œæ¶ˆé™¤åŠ è½½æ—‹è½¬å™¨ã€‚ä½¿ç”¨queryClient.ensureQueryData()é¦–å…ˆæ£€æŸ¥ç¼“å­˜ï¼Œå¦‚æœéœ€è¦åˆ™è·å–æ–°é²œæ•°æ®ï¼Œç¡®ä¿é¡µé¢åœ¨æŒ‚è½½æ—¶ç«‹å³å‘ˆç°ã€‚"
      },
      "fr": {
        "title": "Comment utiliser React Query avec les loaders de React Router (prÃ©-rÃ©cupÃ©ration et mise en cache des donnÃ©es)",
        "summary": "La combinaison de React Query avec les loaders de React Router permet la prÃ©-rÃ©cupÃ©ration des donnÃ©es avant que les composants ne se montent, Ã©liminant les spinners de chargement. L'utilisation de queryClient.ensureQueryData() vÃ©rifie d'abord le cache et rÃ©cupÃ¨re les donnÃ©es fraÃ®ches si nÃ©cessaire, garantissant que la page s'affiche immÃ©diatement lors du montage."
      },
      "de": {
        "title": "Verwendung von React Query mit React Router-Loadern (Vorab-Abrufen und Zwischenspeicherung von Daten)",
        "summary": "Das Kombinieren von React Query mit React Router-Loadern ermÃ¶glicht das Vorab-Abrufen von Daten, bevor Komponenten bereitgestellt werden, und beseitigt Lade-Spinner. Die Verwendung von queryClient.ensureQueryData() prÃ¼ft zunÃ¤chst den Cache und ruft bei Bedarf neue Daten ab, um sicherzustellen, dass die Seite beim Bereitstellen sofort gerendert wird."
      },
      "es": {
        "title": "CÃ³mo usar React Query con React Router Loaders (Captura previa y almacenamiento en cachÃ© de datos)",
        "summary": "La combinaciÃ³n de React Query con los cargadores de React Router permite la captura previa de datos antes de que se monten los componentes, eliminando los indicadores de carga. El uso de queryClient.ensureQueryData() verifica primero el cachÃ© y obtiene datos frescos si es necesario, asegurando que la pÃ¡gina se procese inmediatamente cuando se monta."
      }
    }
  },
  {
    "title": "Appends for AI apps: Stream into a single message with Ably AI Transport",
    "slug": "appends-for-ai-apps-stream-into-single-message-ably-ai-transport",
    "url": "https://dev.to/ablyblog/appends-for-ai-apps-stream-into-a-single-message-with-ably-ai-transport-398a",
    "source": "DEV Community",
    "date": "2026-02-26T12:05:30.000Z",
    "summary": "Ably AI Transport's message appends feature streams AI tokens into a single message rather than individual messages, solving fragmentation issues from client disconnections and refreshes. This approach simplifies UI reconstruction and message history handling without manual orchestration.",
    "content": "Streaming tokens is easy. Resuming cleanly is not. A user refreshes mid-response, another client joins late, a mobile connection drops for 10 seconds, and suddenly your \"one answer\" is 600 tiny messages that your UI has to stitch back together. Message history turns into fragments. You start building a side store just to reconstruct \"the response so far\".\nThis is not a model problem. It's a delivery problem\nThat's why we developed message appends for Ably AI Transport. Appends let you stream AI output tokens into a single message as they are produced, so you get progressive rendering for live subscribers and a clean, compact response in history.\nThe failure mode we're fixing\n\n\nThe usual implementation is to stream each token as a single message, which is simple and works perfectly on a stable connection. In production, clients disconnect and resume mid-stream: refreshes, mobile dropouts, backgrounded tabs, and late joins.\nOnce you have real reconnects and refreshes, you inherit work you did not plan for: ordering, dedupe, buffering, \"latest wins\" logic, and replay rules that make history and realtime agree. You can build it, but it is the kind of work that quietly eats weeks of engineering time.\n\nWith appends you can avoid that by changing the shape of the data. Instead of hundreds of token messages, you have one response message whose content grows over time.\nIn Ably AI Transport, you publish an initial response message and capture its server-assigned serial. That serial is what you append to.\nIt's a small detail that ends up doing a lot of work for you:\nconst result = await channel.publish({ name: 'response', data: '' });\nconst { serials: [msgSerial] } = result;\n\nNow, as your model yields tokens, you append each fragment to that same message:\nif (event.type === 'token') {\n  channel.appendMessage({ serial: msgSerial, data: event.text });\n}\n\nWhat changes for clients\n\n\nSubscribers still see progressive output, but they see it as actions on the same message serial. A",
    "category": "github",
    "translations": {
      "zh": {
        "title": "AIåº”ç”¨ç¨‹åºçš„è¿½åŠ ï¼šä½¿ç”¨Ably AI Transportæµå…¥å•ä¸ªæ¶ˆæ¯",
        "summary": "Ably AI Transportçš„æ¶ˆæ¯è¿½åŠ åŠŸèƒ½å°†AIä»¤ç‰Œæµå…¥å•ä¸ªæ¶ˆæ¯è€Œä¸æ˜¯å¤šä¸ªæ¶ˆæ¯ï¼Œè§£å†³äº†å®¢æˆ·ç«¯æ–­å¼€è¿æ¥å’Œåˆ·æ–°å¯¼è‡´çš„ç¢ç‰‡åŒ–é—®é¢˜ã€‚è¿™ç§æ–¹æ³•ç®€åŒ–äº†UIé‡å»ºå’Œæ¶ˆæ¯å†å²å¤„ç†ï¼Œæ— éœ€æ‰‹åŠ¨ç¼–æ’ã€‚"
      },
      "fr": {
        "title": "Appends pour les applications IA : Flux dans un message unique avec Ably AI Transport",
        "summary": "La fonctionnalitÃ© d'ajout de messages d'Ably AI Transport diffuse les jetons IA dans un message unique plutÃ´t que dans des messages individuels, rÃ©solvant les problÃ¨mes de fragmentation dus aux dÃ©connexions et actualisations des clients. Cette approche simplifie la reconstruction de l'interface utilisateur et la gestion de l'historique des messages sans orchestration manuelle."
      },
      "de": {
        "title": "Appends fÃ¼r KI-Apps: Stream in eine einzelne Nachricht mit Ably AI Transport",
        "summary": "Die Nachrichtenanhang-Funktion von Ably AI Transport streamt KI-Token in eine einzelne Nachricht statt in einzelne Nachrichten und lÃ¶st Fragmentierungsprobleme durch Client-Trennungen und Aktualisierungen. Dieser Ansatz vereinfacht die UI-Rekonstruktion und das Nachrichtenverlauf-Management ohne manuelle Orchestrierung."
      },
      "es": {
        "title": "Appends para aplicaciones de IA: Stream en un mensaje Ãºnico con Ably AI Transport",
        "summary": "La funciÃ³n de adiciÃ³n de mensajes de Ably AI Transport transmite tokens de IA en un mensaje Ãºnico en lugar de mensajes individuales, resolviendo problemas de fragmentaciÃ³n por desconexiones y actualizaciones de clientes. Este enfoque simplifica la reconstrucciÃ³n de la interfaz de usuario y el manejo del historial de mensajes sin orquestaciÃ³n manual."
      }
    }
  },
  {
    "title": "Node.js Application with CI/CD GitLab Pipeline on AWS EC2",
    "slug": "node-js-application-ci-cd-gitlab-pipeline-aws-ec2",
    "url": "https://dev.to/addwebsolutionpvtltd/nodejs-application-with-cicd-gitlab-pipeline-on-aws-ec2-2kk9",
    "source": "DEV Community",
    "date": "2026-02-26T12:01:06.000Z",
    "summary": "This guide demonstrates setting up a CI/CD pipeline for Node.js applications using GitLab CI/CD to automate testing, building, and deployment to AWS EC2 instances. The automated workflow reduces manual errors and accelerates development cycles through SSH-based code deployment and PM2-managed application restarts.",
    "content": "â€œAutomation is the key to speed and reliability in modern software development.â€\nIntroduction\nArchitecture Overview\nPrerequisites\nCI/CD Workflow (Step-by-Step)\nGitLab Pipeline Configuration\nDeployment Process on AWS EC2\nSecurity Best Practices\nInteresting Facts & Statistics\nFAQs\nKey Takeaways\nConclusion\nContinuous Integration and Continuous Deployment (CI/CD) is a modern development practice that automates the process of building, testing, and deploying applications. This document explains how to set up a CI/CD pipeline for a Node.js application using GitLab CI/CD and deploy it automatically to an AWS EC2 instance.\nAutomate deployment\nReduce manual errors\nImprove development speed\nEnsure reliable releases\nBackend: Node.js\nVersion Control: GitLab\nCI/CD Tool: GitLab Pipeline\nServer: AWS EC2 (Ubuntu)\nProcess Manager: PM2\nSSH Authentication: Secure Key-based login\nHigh-level Flow:\nDeveloper pushes code to GitLab repository.\nGitLab pipeline triggers automatically.\nPipeline installs dependencies and builds project.\nGitLab connects to EC2 via SSH.\nCode is pulled on EC2 server.\nApplication restarts using PM2.\nNginx routes HTTP traffic to the Node.js app\nBefore setting up CI/CD, ensure the following:\nGitLab Setup\nGitLab repository created\nBranches (dev/stage/prod) configured\nGitLab Runner enabled (shared runner works)\nAWS EC2 Setup\nUbuntu EC2 instance running\nNode.js & npm installed\nGit installed on server\nSSH access configured\nPM2 installed globally\n\n\n\nnpm install pm2 -g\n\nSSH Key Setup\nGenerate SSH key on local system:\n\n\n\n    ssh-keygen -t rsa -b 4096\n\nAdd public key to EC2:\n\n\n\n    ~/.ssh/authorized_keys\n\n- Add private key in GitLab:\n    **GitLab â†’ Settings â†’ CI/CD â†’ Variables**\n\nSSH_PRIVATE_KEY\nSSH_HOST\nSSH_USER\nStep 1: Developer Pushes Code\nStep 2: Pipeline Triggered\nStep 3: Install Dependencies\nStep 4: SSH Connection\nStep 5: Deployment\nStep 6: Live Deployment\nâ€œCI/CD turns deployment from a risky event into a routine process.â€\nCreate .gitlab-ci.yml in project root:\nproduc",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Node.jsåº”ç”¨ç¨‹åºä¸CI/CD GitLabç®¡é“åœ¨AWS EC2ä¸Šçš„é›†æˆ",
        "summary": "æœ¬æŒ‡å—æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨GitLab CI/CDä¸ºNode.jsåº”ç”¨ç¨‹åºè®¾ç½®CI/CDç®¡é“ï¼Œä»¥è‡ªåŠ¨åŒ–æµ‹è¯•ã€æ„å»ºå’Œéƒ¨ç½²åˆ°AWS EC2å®ä¾‹ã€‚è‡ªåŠ¨åŒ–å·¥ä½œæµé€šè¿‡åŸºäºSSHçš„ä»£ç éƒ¨ç½²å’ŒPM2ç®¡ç†çš„åº”ç”¨ç¨‹åºé‡å¯æ¥å‡å°‘æ‰‹åŠ¨é”™è¯¯å¹¶åŠ å¿«å¼€å‘å‘¨æœŸã€‚"
      },
      "fr": {
        "title": "Application Node.js avec pipeline CI/CD GitLab sur AWS EC2",
        "summary": "Ce guide dÃ©montre comment configurer un pipeline CI/CD pour les applications Node.js en utilisant GitLab CI/CD pour automatiser le test, la construction et le dÃ©ploiement sur des instances AWS EC2. Le flux de travail automatisÃ© rÃ©duit les erreurs manuelles et accÃ©lÃ¨re les cycles de dÃ©veloppement grÃ¢ce au dÃ©ploiement de code basÃ© sur SSH et aux redÃ©marrages d'application gÃ©rÃ©s par PM2."
      },
      "de": {
        "title": "Node.js-Anwendung mit CI/CD-GitLab-Pipeline auf AWS EC2",
        "summary": "Diese Anleitung demonstriert die Einrichtung einer CI/CD-Pipeline fÃ¼r Node.js-Anwendungen mit GitLab CI/CD zur Automatisierung von Tests, Builds und Bereitstellung auf AWS EC2-Instanzen. Der automatisierte Workflow reduziert manuelle Fehler und beschleunigt Entwicklungszyklen durch SSH-basierte Code-Bereitstellung und PM2-verwaltete Anwendungsneustart."
      },
      "es": {
        "title": "AplicaciÃ³n Node.js con pipeline CI/CD de GitLab en AWS EC2",
        "summary": "Esta guÃ­a demuestra cÃ³mo configurar un pipeline CI/CD para aplicaciones Node.js usando GitLab CI/CD para automatizar pruebas, compilaciÃ³n e implementaciÃ³n en instancias de AWS EC2. El flujo de trabajo automatizado reduce errores manuales y acelera los ciclos de desarrollo mediante la implementaciÃ³n de cÃ³digo basada en SSH y reinicios de aplicaciones gestionados por PM2."
      }
    }
  },
  {
    "title": "The Â£20 Billion Handshake",
    "slug": "the-20-billion-handshake",
    "url": "https://dev.to/rawveg/the-ps20-billion-handshake-2mmk",
    "source": "DEV Community",
    "date": "2026-02-26T12:00:00.000Z",
    "summary": "Google pays Apple $20 billion annually to remain Safari's default search engine, a relationship exposed during DOJ antitrust proceedings that reflects deeper competition among tech giants. As AI-powered search tools reshape how users find information, these backend licensing deals increasingly determine competitive advantage in the digital economy.",
    "content": "The smartphone in your pocket contains a curious paradox. Apple, one of the world's most valuable companies, builds its own chips, designs its own operating system, and controls every aspect of its ecosystem with obsessive precision. Yet when you tap Safari's search bar, you're not using an Apple search engine. You're using Google. And Google pays Apple a staggering $20 billion every year to keep it that way.\nThis colossal payment, revealed during the US Department of Justice's antitrust trial against Google, represents far more than a simple business arrangement. It's the visible tip of a fundamental transformation in how digital platforms compete, collaborate, and ultimately extract value from the billions of searches and queries humans perform daily. As artificial intelligence reshapes the search landscape and digital assistants become genuine conversational partners rather than glorified keyword matchers, these backend licensing deals are quietly redrawing the competitive map of the digital economy.\nThe stakes have never been higher. Search advertising generated $102.9 billion in revenue in the United States alone during 2024, accounting for nearly 40 per cent of all digital advertising spending. But the ground is shifting beneath the industry's feet. AI-powered search experiences from OpenAI's ChatGPT, Microsoft's Copilot, and Google's own AI Overviews are fundamentally changing how people find information, and these changes threaten to upend decades of established business models. Into this volatile mix come a new wave of licensing deals, platform partnerships, and strategic alliances that could determine which companies dominate the next generation of digital interaction.\nTo understand where we're heading, it helps to grasp how we got here. Google's dominance in search wasn't accidental. The company built the best search engine, captured roughly 90 per cent of the market, and then methodically paid billions to ensure its search bar appeared by default on ever",
    "category": "github",
    "translations": {
      "zh": {
        "title": "200äº¿è‹±é•‘çš„æ¡æ‰‹",
        "summary": "è°·æ­Œæ¯å¹´å‘è‹¹æœæ”¯ä»˜200äº¿ç¾å…ƒä»¥ä¿æŒåœ¨Safariä¸­ä½œä¸ºé»˜è®¤æœç´¢å¼•æ“çš„åœ°ä½,è¿™ä¸€å…³ç³»åœ¨å¸æ³•éƒ¨åå„æ–­è¯‰è®¼ä¸­æ›å…‰,åæ˜ äº†ç§‘æŠ€å·¨å¤´ä¹‹é—´æ›´æ·±å±‚çš„ç«äº‰ã€‚éšç€ç”±äººå·¥æ™ºèƒ½é©±åŠ¨çš„æœç´¢å·¥å…·é‡å¡‘ç”¨æˆ·å¦‚ä½•æŸ¥æ‰¾ä¿¡æ¯,è¿™äº›åç«¯è®¸å¯åè®®æ—¥ç›Šå†³å®šäº†æ•°å­—ç»æµä¸­çš„ç«äº‰ä¼˜åŠ¿ã€‚"
      },
      "fr": {
        "title": "La PoignÃ©e de Main de 20 Milliards de Livres",
        "summary": "Google paie Ã  Apple 20 milliards de dollars par an pour rester le moteur de recherche par dÃ©faut de Safari, une relation exposÃ©e lors des poursuites antitrust du ministÃ¨re de la Justice qui reflÃ¨te une concurrence plus profonde entre les gÃ©ants de la technologie. Ã€ mesure que les outils de recherche alimentÃ©s par l'IA redÃ©finissent la faÃ§on dont les utilisateurs trouvent des informations, ces accords de licence de backend dÃ©terminent de plus en plus l'avantage concurrentiel dans l'Ã©conomie numÃ©rique."
      },
      "de": {
        "title": "Der 20-Milliarden-Pfund-Handschlag",
        "summary": "Google zahlt Apple jÃ¤hrlich 20 Milliarden Dollar, um die Standard-Suchmaschine von Safari zu bleiben, eine Beziehung, die in den Kartellverfahren des US-Justizministeriums offengelegt wurde und tiefer gehende Konkurrenzen zwischen Technologieriesen widerspiegelt. Da KI-gestÃ¼tzte Suchtools die Art und Weise neu gestalten, wie Benutzer Informationen finden, bestimmen diese Backend-Lizenzvereinbarungen zunehmend den Wettbewerbsvorteil in der digitalen Wirtschaft."
      },
      "es": {
        "title": "El ApretÃ³n de Manos de 20 Mil Millones de Libras",
        "summary": "Google paga a Apple 20 mil millones de dÃ³lares anuales para seguir siendo el motor de bÃºsqueda predeterminado de Safari, una relaciÃ³n expuesta durante los procedimientos antimonopolio del Departamento de Justicia que refleja una competencia mÃ¡s profunda entre los gigantes tecnolÃ³gicos. A medida que las herramientas de bÃºsqueda impulsadas por IA remodelan cÃ³mo los usuarios encuentran informaciÃ³n, estos acuerdos de licencia de backend determinan cada vez mÃ¡s la ventaja competitiva en la economÃ­a digital."
      }
    }
  },
  {
    "title": "Kubernetes'te StorageClass Nedir?",
    "slug": "kubernetes-storageclass-nedir",
    "url": "https://dev.to/tarikanafarta/kuberneteste-storageclass-nedir-dha",
    "source": "DEV Community",
    "date": "2026-02-26T11:49:45.000Z",
    "summary": "Kubernetes storage architecture comprises PV (Persistent Volumes), PVC (Persistent Volume Claims), and StorageClass, supporting both static and dynamic provisioning models. Data storage can reside on node-local disks, NFS shares, or cloud providers depending on StorageClass policies and PV configuration.",
    "content": "PV, PVC ve Veri GerÃ§ekten Nerede SaklanÄ±yor?\n\n\nBu yazÄ±da ÅŸu konularÄ± inceleyeceÄŸim:\nPV (Persistent Volume) nedir?\nPVC (Persistent Volume Claim) nedir?\nStorageClass ne iÅŸe yarar?\nStatic vs Dynamic provisioning farkÄ± nedir?\nMariaDB / PostgreSQL gibi uygulamalarda disk nerede?\nÃ–ncelikle Kubernetes'te storage zinciri ÅŸu ÅŸekildedir:\nPod -> PVC -> StorageClass -> PV -> Physical Storage\nPV, cluster iÃ§indeki gerÃ§ek disk kaynaÄŸÄ±nÄ± temsil eder.\nBu kaynak ÅŸunlardan birisidir:\nNode Ã¼zerindeki local disk\nNFS share\nCloud disk\nDistributed storage\nYani aslÄ±nda PV, storage'Ä±n kendisidir.\nPVC ise uygulamanÄ±n disk talebidir.\nÃ–rneÄŸin:\n10Gi storage\nAccess mode: ReadWriteOnce\nstorageClass: fast-storage\nPVC diski oluÅŸturmaz, bir disk talep eder.\nStorageClass, PVC oluÅŸturulduÄŸunda disk'in nasÄ±l saÄŸlanacaÄŸÄ±nÄ± belirler.\nYani, disk local mi olacak? NFS mi olacak? Cloud block storage mÄ± olacak? Otomatik mi Ã¼retilecek? Hangi performans sÄ±nÄ±fÄ± kullanÄ±lacak? Gibi sorulara cevap veren bir policy katmanÄ±dÄ±r.\nStatic provisioning modelinde PV manuel olarak oluÅŸturulur. Yani Ã¶nce storage kaynaÄŸÄ± tanÄ±mlanÄ±r, ardÄ±ndan PVC bu PV'ye baÄŸlanÄ±r.\nBu yÃ¶ntem daha fazla kontrol saÄŸlar ancak her yeni disk ihtiyacÄ±nda manuel iÅŸlem gerektirir. Genellikle test ortamlarÄ±nda veya local/NFS kurulumlarÄ±nda tercih edilir.\nDynamic provisioning modelinde ise sadece PVC oluÅŸturulur.\nBu sÃ¼reÃ§ CSI (Container Storage Interface) driver'larÄ± sayesinde Ã§alÄ±ÅŸÄ±r.\nBu sorunun cevabÄ± PV tanÄ±mÄ±ndadÄ±r.\nPV ÅŸu path'i gÃ¶steriyorsa: /mnt/k8s/data, disk node Ã¼zerindedir. Node arÄ±zalanÄ±rsa veri kaybedilebilir. Pod baÅŸka node'a taÅŸÄ±nÄ±rsa diske eriÅŸilemez.\nPV ÅŸu ÅŸekilde tanÄ±mlanmÄ±ÅŸsa:\nnfs:\n  server: 192.168.1.10\n  path: /srv/nfs/share\n\nDisk NFS server Ã¼zerindedir. Pod hangi node'da olursa olsun aynÄ± veriye eriÅŸebilir.\nPV bir cloud volume'a baÄŸlÄ±ysa, disk cloud provider tarafÄ±ndadÄ±r.\nRWO (ReadWriteOnce) -> Sadece tek node yazabilir\nPVC silindiÄŸinde disk'in nasÄ±l davranacaÄŸÄ±nÄ± reclaim policy belirler. Kubernetes'te 3 farklÄ± reclaim policy vardÄ±r",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Kubernetesä¸­çš„StorageClassæ˜¯ä»€ä¹ˆ?",
        "summary": "Kuberneteså­˜å‚¨æ¶æ„åŒ…æ‹¬PV(æŒä¹…å·)ã€PVC(æŒä¹…å·å£°æ˜)å’ŒStorageClass,æ”¯æŒé™æ€å’ŒåŠ¨æ€é…ç½®æ¨¡å‹ã€‚æ ¹æ®StorageClassç­–ç•¥å’ŒPVé…ç½®,æ•°æ®å­˜å‚¨å¯ä»¥é©»ç•™åœ¨èŠ‚ç‚¹æœ¬åœ°ç£ç›˜ã€NFSå…±äº«æˆ–äº‘æä¾›å•†ä¸Šã€‚"
      },
      "fr": {
        "title": "Qu'est-ce que StorageClass dans Kubernetes?",
        "summary": "L'architecture de stockage Kubernetes comprend PV (Persistent Volumes), PVC (Persistent Volume Claims) et StorageClass, supportant les modÃ¨les de provisionnement statique et dynamique. Le stockage de donnÃ©es peut rÃ©sider sur des disques locaux de nÅ“ud, des partages NFS ou des fournisseurs cloud en fonction des politiques StorageClass et de la configuration PV."
      },
      "de": {
        "title": "Was ist StorageClass in Kubernetes?",
        "summary": "Die Kubernetes-Speicherarchitektur umfasst PV (Persistent Volumes), PVC (Persistent Volume Claims) und StorageClass und unterstÃ¼tzt sowohl statische als auch dynamische Bereitstellungsmodelle. Der Datenspeicher kann je nach StorageClass-Richtlinien und PV-Konfiguration auf lokalen Node-Festplatten, NFS-Freigaben oder Cloud-Anbietern residieren."
      },
      "es": {
        "title": "Â¿QuÃ© es StorageClass en Kubernetes?",
        "summary": "La arquitectura de almacenamiento de Kubernetes comprende PV (VolÃºmenes Persistentes), PVC (Reclamaciones de Volumen Persistentes) y StorageClass, admitiendo modelos de aprovisionamiento estÃ¡tico y dinÃ¡mico. El almacenamiento de datos puede residir en discos locales de nodos, recursos compartidos NFS o proveedores en la nube dependiendo de las polÃ­ticas de StorageClass y la configuraciÃ³n de PV."
      }
    }
  },
  {
    "title": "Why is cdk.out (Cloud Assembly) Necessary in AWS CDK?",
    "slug": "why-is-cdk-out-cloud-assembly-necessary-aws-cdk",
    "url": "https://dev.to/aws-heroes/why-is-cdkout-cloud-assembly-necessary-in-aws-cdk-n5f",
    "source": "DEV Community",
    "date": "2026-02-26T11:45:47.000Z",
    "summary": "AWS CDK's cdk.out directory stores the cloud assemblyâ€”an intermediate artifact containing CloudFormation templates, metadata, and asset files generated from CDK code. This assembly bridges CDK code to AWS infrastructure deployment and includes stack templates, manifest files, and Docker image assets.",
    "content": "What is cdk.out\n\n\ncdk.out is a directory that stores a collection of files called the cloud assembly.\nWhen you run the cdk synth command or the cdk deploy command, the cloud assembly is generated from your CDK code and stored in the cdk.out directory.\nThe cloud assembly is a collection of files generated by the synthesis of a CDK application (CDK code). The CDK CLI references this information to deploy resources to the AWS environment.\nIn other words, the cloud assembly can be considered an intermediate artifact that CDK uses to deploy infrastructure definitions written in CDK code to the AWS environment.\n\nThe cloud assembly primarily consists of the following files and directories:\n(stack name).template.json\n\n\nCloudFormation template generated by CDK code\n(stack name).assets.json\n\n\nFile describing information about assets used in CDK code\nmanifest.json\n\n\nFile describing metadata of the cloud assembly\ntree.json\n\n\nFile representing the Construct tree structure of resources defined in CDK code\ncdk.out\n\n\nFile storing Cloud Assembly schema version information\nasset.(hash value)/\n\n\nDirectory storing asset files used in CDK code\nUses hash values calculated for each asset as directory names\nEach asset is classified into two types: S3 assets and Docker image assets, which are uploaded to S3 buckets or ECR repositories created by the cdk bootstrap command when the cdk deploy command is executed\nDocker image assets are built when the cdk deploy command is executed (not when the cdk synth command is executed)\nassembly-(stage name)/\n\n\nDirectory storing cloud assemblies generated for each stage when using Stage Construct\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  Why cdk.out is Necessary\n\n\nAs explained above, cdk.out is a directory for storing the cloud assembly, which is an intermediate artifact generated from CDK code.\nSo why is this directory necessary? You might wonder whether it's really necessary to generate and store information needed for deployment as files.\nBefore explaining this, let me fir",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ä¸ºä»€ä¹ˆAWS CDKä¸­çš„cdk.out(äº‘ç¨‹åºé›†)æ˜¯å¿…è¦çš„?",
        "summary": "AWS CDKçš„cdk.outç›®å½•å­˜å‚¨äº‘ç¨‹åºé›†â€”â€”ä¸€ä¸ªä¸­é—´åˆ¶å“,åŒ…å«ä»CDKä»£ç ç”Ÿæˆçš„CloudFormationæ¨¡æ¿ã€å…ƒæ•°æ®å’Œèµ„äº§æ–‡ä»¶ã€‚æ­¤ç¨‹åºé›†è¿æ¥CDKä»£ç ä¸AWSåŸºç¡€è®¾æ–½éƒ¨ç½²,åŒ…æ‹¬å †æ ˆæ¨¡æ¿ã€æ¸…å•æ–‡ä»¶å’ŒDockeré•œåƒèµ„äº§ã€‚"
      },
      "fr": {
        "title": "Pourquoi cdk.out (Cloud Assembly) est-il nÃ©cessaire dans AWS CDK?",
        "summary": "Le rÃ©pertoire cdk.out d'AWS CDK stocke l'assembly cloud â€” un artefact intermÃ©diaire contenant les modÃ¨les CloudFormation, les mÃ©tadonnÃ©es et les fichiers d'actifs gÃ©nÃ©rÃ©s Ã  partir du code CDK. Cet assembly relie le code CDK au dÃ©ploiement de l'infrastructure AWS et inclut les modÃ¨les de pile, les fichiers manifeste et les actifs d'image Docker."
      },
      "de": {
        "title": "Warum ist cdk.out (Cloud Assembly) in AWS CDK notwendig?",
        "summary": "Das Verzeichnis cdk.out von AWS CDK speichert die Cloud Assembly â€” ein Zwischenartefakt, das CloudFormation-Vorlagen, Metadaten und aus CDK-Code generierte Asset-Dateien enthÃ¤lt. Diese Assembly verbindet CDK-Code mit der AWS-Infrastruktur-Bereitstellung und umfasst Stack-Vorlagen, Manifest-Dateien und Docker-Image-Assets."
      },
      "es": {
        "title": "Â¿Por quÃ© es necesario cdk.out (Cloud Assembly) en AWS CDK?",
        "summary": "El directorio cdk.out de AWS CDK almacena el cloud assemblyâ€”un artefacto intermedio que contiene plantillas de CloudFormation, metadatos y archivos de activos generados a partir del cÃ³digo CDK. Este assembly vincula el cÃ³digo CDK con la implementaciÃ³n de infraestructura de AWS e incluye plantillas de pila, archivos de manifiesto y activos de imagen Docker."
      }
    }
  },
  {
    "title": "My Pixel's Keyboard Generated a Custom Emoji.",
    "slug": "my-pixel-keyboard-generated-custom-emoji",
    "url": "https://dev.to/megzlawther1/my-pixels-keyboard-generated-a-custom-emoji-42k",
    "source": "DEV Community",
    "date": "2026-02-26T11:45:35.000Z",
    "summary": "Google Gemini's Emojify feature on Pixel 10 keyboards generates custom emojis dynamically beyond Unicode standards, creating novel visual communications not found in preset emoji tables. This on-device AI capability demonstrates how generative models can extend traditional interface constraints.",
    "content": "What I Built with Google Gemini: \nMy project isn't a traditional \"build\" in the sense of writing code for a new application. Instead, I built a novel interaction with Google Gemini, specifically through the \"Emojify\" feature on my Google Pixel 10 keyboard. My \"build\" was essentially demonstrating and exploring how a widely accessible AI feature can generate entirely new, non-standard visual communication..\nThe \"problem\" it solved (or rather, illuminated) is the limitation of pre-set, static emoji sets. While existing emojis are vast, they can't cover every nuanced concept. My interaction with Gemini showcased its ability to dynamically create a custom visual to perfectly fit a specific textual context, transcending the boundaries of Unicode.\nHere's what happened: \nThis wasn't through a dedicated generative AI interface; it was a seamless, on-device AI capability integrated into a common keyboard function. \nWhat I Learned:\nThis experience has been incredibly eye-opening, both technically and philosophically.\nGoing Beyond the Standard Emoji Table (and My Reaction to It): My biggest ponderance and a major learning point is how it was able to go beyond the standard emoji table of preset characters (Unicode/ASCII). Emojis are typically strict templates of codes, already preset and preloaded. Gemini's Emojify function went out of its preset and preloaded list to generate something entirely new. When I saw this custom visual, my immediate reaction was confusion. I automatically checked my keyboard's emoji list, particularly the \"Family\" section, and confirmed that the generated icon was not available there. My screenshot of the available standard family emojis (e.g., \"Family: Adult, Adult, Child\") clearly shows they are distinct and generic. This solidifies that the AI truly generated a novel image, rather than pulling from existing assets, and profoundly changed my understanding of what a keyboard's \"emojify\" function is capable of.\nAI's Gender Inference in Generative Out",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æˆ‘çš„Pixelé”®ç›˜ç”Ÿæˆäº†è‡ªå®šä¹‰è¡¨æƒ…ç¬¦å·",
        "summary": "è°·æ­ŒGeminiåœ¨Pixel 10é”®ç›˜ä¸Šçš„EmojifyåŠŸèƒ½åŠ¨æ€ç”Ÿæˆè¶…è¶ŠUnicodeæ ‡å‡†çš„è‡ªå®šä¹‰è¡¨æƒ…ç¬¦å·ï¼Œåˆ›é€ å‡ºé¢„è®¾è¡¨æƒ…ç¬¦å·è¡¨ä¸­æ‰¾ä¸åˆ°çš„æ–°é¢–è§†è§‰é€šä¿¡ã€‚è¿™ç§è®¾å¤‡ä¸Šçš„AIèƒ½åŠ›å±•ç¤ºäº†ç”Ÿæˆæ¨¡å‹å¦‚ä½•èƒ½å¤Ÿæ‰©å±•ä¼ ç»Ÿç•Œé¢çº¦æŸã€‚"
      },
      "fr": {
        "title": "Mon clavier Pixel a gÃ©nÃ©rÃ© un emoji personnalisÃ©",
        "summary": "La fonction Emojify de Google Gemini sur les claviers Pixel 10 gÃ©nÃ¨re dynamiquement des emojis personnalisÃ©s au-delÃ  des normes Unicode, crÃ©ant de nouvelles communications visuelles introuvables dans les tableaux d'emojis prÃ©Ã©tablis. Cette capacitÃ© d'IA sur l'appareil dÃ©montre comment les modÃ¨les gÃ©nÃ©ratifs peuvent Ã©tendre les contraintes d'interface traditionnelles."
      },
      "de": {
        "title": "Meine Pixel-Tastatur hat ein benutzerdefiniertes Emoji generiert",
        "summary": "Googles Gemini-Emojify-Funktion auf Pixel-10-Tastaturen generiert dynamisch benutzerdefinierte Emojis jenseits von Unicode-Standards und erzeugt neuartige visuelle Kommunikation, die in voreingestellten Emoji-Tabellen nicht zu finden ist. Diese On-Device-KI-Funktion zeigt, wie generative Modelle traditionelle SchnittstellenbeschrÃ¤nkungen erweitern kÃ¶nnen."
      },
      "es": {
        "title": "Mi teclado Pixel generÃ³ un emoji personalizado",
        "summary": "La funciÃ³n Emojify de Google Gemini en los teclados Pixel 10 genera emojis personalizados de forma dinÃ¡mica mÃ¡s allÃ¡ de los estÃ¡ndares Unicode, creando comunicaciones visuales novedosas que no se encuentran en las tablas de emojis predefinidas. Esta capacidad de IA en el dispositivo demuestra cÃ³mo los modelos generativos pueden ampliar las restricciones tradicionales de la interfaz."
      }
    }
  },
  {
    "title": "I Built and Launched an AI Document API in Under a Week â€” Here's Exactly How I Did It",
    "slug": "i-built-and-launched-ai-document-api-under-week",
    "url": "https://dev.to/senzen/i-built-and-launched-an-ai-document-api-in-under-a-week-heres-exactly-how-i-did-it-4j8c",
    "source": "DEV Community",
    "date": "2026-02-26T11:45:08.000Z",
    "summary": "Developer launched Condensare, an AI document processing API leveraging GPT-4o to transform uploaded files into structured notes and contextual implementation suggestions. Deployed serverlessly on Vercel and published on RapidAPI, the solution was built and launched in under one week.",
    "content": "I Built and Launched an AI Document API in Under a Week â€” Here's Exactly How I Did It\n\n\n\nTL;DR: I built Condensare â€” an AI-powered document processing API that turns any uploaded file into structured notes and contextualised implementation suggestions. It's live on RapidAPI. This is the full breakdown of how I built it, deployed it, tested it and shipped it.\nA few weeks ago I had a simple frustration.\nI was uploading documents to various AI tools trying to get useful summaries and actionable insights, and every single one felt generic. The output didn't know what industry I was in, what scale my business operated at, or what I actually wanted to do with the information.\nIt just summarised. That was it.\nSo I built Condensare â€” a document condensing and AI suggestion API that takes any uploaded file and returns structured notes plus contextualised implementation suggestions.\nI kept it simple and production-focused:\n\n\n\nLayer\nTechnology\n\n\n\n\nAPI Server\nNode.js + Express\n\n\nAI\nOpenAI GPT-4o\n\n\nDeployment\nVercel (serverless)\n\n\nMarketplace\nRapidAPI\n\n\nFile Uploads\nMulter\n\n\nDocument Parsing\npdf-parse, mammoth, xlsx\n\n\n\nVercel serverless was the right call for an API marketplace product â€” zero infrastructure management, global distribution out of the box, automatic scaling. You don't want to be managing servers when you're trying to get to market fast.\ncondensare-api/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ app.js                  # Express entry point\nâ”‚   â”œâ”€â”€ routes/\nâ”‚   â”‚   â””â”€â”€ condense.js         # Route definitions\nâ”‚   â”œâ”€â”€ middleware/\nâ”‚   â”‚   â””â”€â”€ authenticate.js     # Auth + plan management\nâ”‚   â”œâ”€â”€ services/\nâ”‚   â”‚   â”œâ”€â”€ parseService.js     # File parsing logic\nâ”‚   â”‚   â”œâ”€â”€ condenseService.js  # AI condensing logic\nâ”‚   â”‚   â””â”€â”€ suggestService.js   # AI suggestions logic\nâ”‚   â””â”€â”€ utils/\nâ”‚       â””â”€â”€ fileUtils.js        # File validation helpers\nâ”œâ”€â”€ vercel.json\nâ””â”€â”€ package.json\n\nPOST /api/v1/condense/parse\n\n\nAvailable on: FREE, PRO, MEGA, ULTRA\nThe lightweight endpoint. Upload any supported file and get b",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æˆ‘åœ¨ä¸€å‘¨å†…æ„å»ºå¹¶æ¨å‡ºäº†ä¸€ä¸ªAIæ–‡æ¡£APIâ€”â€”ä»¥ä¸‹æ˜¯æˆ‘çš„å…·ä½“åšæ³•",
        "summary": "å¼€å‘è€…æ¨å‡ºäº†Condensareï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨GPT-4oçš„AIæ–‡æ¡£å¤„ç†APIï¼Œå¯ä»¥å°†ä¸Šä¼ çš„æ–‡ä»¶è½¬æ¢ä¸ºç»“æ„åŒ–ç¬”è®°å’Œä¸Šä¸‹æ–‡å®ç°å»ºè®®ã€‚åœ¨Vercelä¸Šæ— æœåŠ¡å™¨éƒ¨ç½²å¹¶åœ¨RapidAPIä¸Šå‘å¸ƒï¼Œè¯¥è§£å†³æ–¹æ¡ˆåœ¨ä¸€å‘¨å†…æ„å»ºå’Œæ¨å‡ºã€‚"
      },
      "fr": {
        "title": "J'ai construit et lancÃ© une API de documents IA en une semaine â€” Voici exactement comment j'ai fait",
        "summary": "Un dÃ©veloppeur a lancÃ© Condensare, une API de traitement de documents IA exploitant GPT-4o pour transformer les fichiers tÃ©lÃ©chargÃ©s en notes structurÃ©es et suggestions d'implÃ©mentation contextuelle. DÃ©ployÃ©e sans serveur sur Vercel et publiÃ©e sur RapidAPI, la solution a Ã©tÃ© construite et lancÃ©e en une semaine."
      },
      "de": {
        "title": "Ich habe eine KI-Dokument-API in einer Woche entwickelt und gestartet â€” Genau so habe ich es getan",
        "summary": "Ein Entwickler hat Condensare gestartet, eine KI-Dokumentverarbeitungs-API, die GPT-4o nutzt, um hochgeladene Dateien in strukturierte Notizen und kontextabhÃ¤ngige ImplementierungsvorschlÃ¤ge umzuwandeln. Serverlos auf Vercel bereitgestellt und auf RapidAPI verÃ¶ffentlicht, wurde die LÃ¶sung in einer Woche entwickelt und gestartet."
      },
      "es": {
        "title": "ConstruÃ­ y lancÃ© una API de documentos IA en una semana â€” AsÃ­ es exactamente cÃ³mo lo hice",
        "summary": "Un desarrollador lanzÃ³ Condensare, una API de procesamiento de documentos IA que aprovecha GPT-4o para transformar archivos cargados en notas estructuradas y sugerencias de implementaciÃ³n contextuales. Implementada sin servidor en Vercel y publicada en RapidAPI, la soluciÃ³n se construyÃ³ y lanzÃ³ en una semana."
      }
    }
  },
  {
    "title": "Why We Replaced Debezium + Kafka in Our Large-Scale Real-Time Pipeline",
    "slug": "why-we-replaced-debezium-kafka-large-scale-real-time-pipeline",
    "url": "https://dev.to/heywalter/why-we-replaced-debezium-kafka-in-our-large-scale-real-time-pipeline-2dc1",
    "source": "DEV Community",
    "date": "2026-02-26T11:42:08.000Z",
    "summary": "A company replaced Debezium and Kafka with alternative solutions to sync over 3,000 tables across heterogeneous databases (Oracle, MySQL, PostgreSQL, MongoDB) into ClickHouse for real-time analytics. The migration addressed operational complexity and maintenance overhead in large-scale CDC pipelines.",
    "content": "Iâ€™ve just wrapped up a real-time data platform project thatâ€™s been running smoothly in production for a few months now. While itâ€™s all still fresh, I figured itâ€™s a good time to look back on our selection process, the migration, the pitfalls we hit, and some takeaways â€” hopefully useful for anyone tackling real-time synchronization across heterogeneous databases.\nOur company has grown quickly over the past few years, with multiple rounds of IT upgrades, system migrations, and acquisitions along the way. This left us with a mixed bag of databases: Oracle for legacy core transaction systems, MySQL for most business applications, SQL Server for some remaining Windows-based legacy apps, PostgreSQL for newer microservices, and even MongoDB for semi-structured data. All told, we have around 50+ instances and over 3,000 regularly used tables, spread across different teams and systems â€” a textbook case of data silos.\nThe goal was to build a unified real-time analytics platform by syncing data from all these sources near-real-time into a data warehouse (we went with ClickHouse). Primary use cases included real-time dashboards, risk monitoring, and data APIs for some of the downstream applications . ClickHouseâ€™s strengths in high-concurrency queries, compression, and OLAP performance made it perfect for delivering second-to-minute latency.\nOn top of that, we wanted the pipeline to be relatively low-maintenance, so data engineers could own it day-to-day and we could move toward real DataOps practices â€” instead of pulling in developers for every tweak.\nAs the tech leader, I usually lean toward proven open-source solutions to avoid building everything from scratch. So we naturally started with the industryâ€™s go-to real-time CDC stack:\n\nFigure1: Debezium Kafka ClickHouse architecture\nCore components:\nCDC Capture: Debezium for change capture. Its wide range of connectors covered virtually all our source types, and the community is solid.\nBuffering: Apache Kafka as the intermediate",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨å¤§è§„æ¨¡å®æ—¶ç®¡é“ä¸­æ›¿æ¢äº†Debezium + Kafka",
        "summary": "ä¸€å®¶å…¬å¸ç”¨æ›¿ä»£æ–¹æ¡ˆæ›¿æ¢äº†Debeziumå’ŒKafkaï¼Œå°†è¶…è¿‡3000ä¸ªè¡¨ä»å¼‚æ„æ•°æ®åº“ï¼ˆOracleã€MySQLã€PostgreSQLã€MongoDBï¼‰åŒæ­¥åˆ°ClickHouseä»¥è¿›è¡Œå®æ—¶åˆ†æã€‚è¿™æ¬¡è¿ç§»è§£å†³äº†å¤§è§„æ¨¡CDCç®¡é“ä¸­çš„æ“ä½œå¤æ‚æ€§å’Œç»´æŠ¤å¼€é”€é—®é¢˜ã€‚"
      },
      "fr": {
        "title": "Pourquoi nous avons remplacÃ© Debezium + Kafka dans notre pipeline temps rÃ©el Ã  grande Ã©chelle",
        "summary": "Une entreprise a remplacÃ© Debezium et Kafka par des solutions alternatives pour synchroniser plus de 3 000 tables Ã  travers des bases de donnÃ©es hÃ©tÃ©rogÃ¨nes (Oracle, MySQL, PostgreSQL, MongoDB) dans ClickHouse pour l'analyse en temps rÃ©el. La migration a rÃ©solu la complexitÃ© opÃ©rationnelle et la surcharge de maintenance dans les pipelines CDC Ã  grande Ã©chelle."
      },
      "de": {
        "title": "Warum wir Debezium + Kafka in unserer groÃŸflÃ¤chigen Echtzeit-Pipeline ersetzt haben",
        "summary": "Ein Unternehmen hat Debezium und Kafka durch alternative LÃ¶sungen ersetzt, um Ã¼ber 3.000 Tabellen aus heterogenen Datenbanken (Oracle, MySQL, PostgreSQL, MongoDB) in ClickHouse fÃ¼r Echtzeit-Analytik zu synchronisieren. Die Migration hat die operative KomplexitÃ¤t und Wartungsbelastung in groÃŸflÃ¤chigen CDC-Pipelines reduziert."
      },
      "es": {
        "title": "Por quÃ© reemplazamos Debezium + Kafka en nuestro pipeline de tiempo real a gran escala",
        "summary": "Una empresa reemplazÃ³ Debezium y Kafka con soluciones alternativas para sincronizar mÃ¡s de 3,000 tablas en bases de datos heterogÃ©neas (Oracle, MySQL, PostgreSQL, MongoDB) en ClickHouse para anÃ¡lisis en tiempo real. La migraciÃ³n abordÃ³ la complejidad operativa y la sobrecarga de mantenimiento en pipelines CDC a gran escala."
      }
    }
  },
  {
    "title": "Cypress in the Age of AI Agents: Orchestration, Trust, and the Tests That Run Themselves",
    "slug": "cypress-in-age-of-ai-agents-orchestration-trust-tests",
    "url": "https://dev.to/cypress/cypress-in-the-age-of-ai-agents-orchestration-trust-and-the-tests-that-run-themselves-43go",
    "source": "DEV Community",
    "date": "2026-02-26T11:33:21.000Z",
    "summary": "Cypress's cy.prompt() enables AI to write and self-heal tests in plain English, but introduces trust concerns when AI makes autonomous pipeline decisions without human oversight. The article distinguishes between risky autonomy and preferable augmentation in AI-driven testing frameworks.",
    "content": "Last year, I wrote about Docker and Cypress for this blog. It covered containers, layer caching, and parallel runners. Good stuff. Useful stuff.\nBut I'm not writing that article again.\nHere's why.\nI could write a perfect container config in my sleep. So could Claude. So could GPT. So could any intern with a prompt. Syntax has become a commodity. The Dockerfile isn't the hard part anymore.\nThe hard part?\nOrchestration and trust when AI agents run the tests.\nLet me explain.\nIn 2025, Cypress shipped cy.prompt(). Write tests in plain English. The AI figures out the selectors. It even self-heals when your UI changes.\nThat's powerful. And that's dangerous.\nNot because the tool is bad. It's genuinely impressive. But because it changes who is making decisions in your pipeline. And most teams haven't thought about that.\nBefore cy.prompt(), the chain of trust was simple:\nA human wrote the test\nA human reviewed it\nCI ran it\nIf it failed, a human fixed it\nEvery link in that chain had a name attached.\nNow?\nAn AI writes the test\nAn AI picks the selectors\nAn AI heals the test when it breaks\nThe human sees green checkmarks\nEverybody ships\nUntil something goes wrong. And nobody knows why.\nThe industry keeps confusing two very different things.\nAutonomy means the agent acts for you. You find out later what happened.\nAugmentation means the agent helps you decide. You still make the call.\nMost AI testing tools sell autonomy:\n\"Never write a test again!\"\n\"Self-healing pipelines!\"\n\"Zero maintenance!\"\nThat sounds great in a demo.\nIt falls apart in production.\nGoogle's testing team found that 1.5% of all test runs were flaky (2016 study). Nearly 16% of tests showed some flakiness over time. Microsoft reported 49,000 flaky tests across 100+ product teams (2022). These numbers haven't gotten better. Now imagine those tests were written by AI.\nYou don't have a testing problem.\nYou have a trust problem.\nI've watched AI code assistants generate test suites. Here's the pattern I see every time:\nD",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Cypressåœ¨AIä»£ç†æ—¶ä»£ï¼šç¼–æ’ã€ä¿¡ä»»ä¸è‡ªè¿è¡Œæµ‹è¯•",
        "summary": "Cypressçš„cy.prompt()ä½¿AIèƒ½å¤Ÿç”¨çº¯è‹±æ–‡ç¼–å†™å’Œè‡ªæˆ‘ä¿®å¤æµ‹è¯•ï¼Œä½†å½“AIåœ¨æ²¡æœ‰äººå·¥ç›‘ç£çš„æƒ…å†µä¸‹è‡ªä¸»åšå‡ºç®¡é“å†³ç­–æ—¶ä¼šå¼•å…¥ä¿¡ä»»é—®é¢˜ã€‚è¯¥æ–‡ç« åŒºåˆ†äº†AIé©±åŠ¨æµ‹è¯•æ¡†æ¶ä¸­çš„å±é™©è‡ªä¸»æ€§å’Œæ›´å¯å–çš„å¢å¼ºæ–¹æ³•ã€‚"
      },
      "fr": {
        "title": "Cypress Ã  l'Ã¨re des agents IA : Orchestration, confiance et tests qui s'exÃ©cutent eux-mÃªmes",
        "summary": "La mÃ©thode cy.prompt() de Cypress permet Ã  l'IA d'Ã©crire et d'auto-corriger les tests en anglais simple, mais introduit des prÃ©occupations de confiance lorsque l'IA prend des dÃ©cisions autonomes dans le pipeline sans surveillance humaine. L'article distingue entre l'autonomie risquÃ©e et l'augmentation prÃ©fÃ©rable dans les cadres de test pilotÃ©s par l'IA."
      },
      "de": {
        "title": "Cypress im Zeitalter von KI-Agenten: Orchestrierung, Vertrauen und selbstablaufende Tests",
        "summary": "Cypress's cy.prompt() ermÃ¶glicht es der KI, Tests in einfachem Englisch zu schreiben und selbst zu heilen, fÃ¼hrt aber zu Vertrauensproblemen ein, wenn KI autonome Pipeline-Entscheidungen ohne menschliche Aufsicht trifft. Der Artikel unterscheidet zwischen riskanter Autonomie und erwÃ¼nschter Augmentation in KI-gesteuerten Test-Frameworks."
      },
      "es": {
        "title": "Cypress en la Era de los Agentes de IA: OrquestaciÃ³n, Confianza y Pruebas que se Ejecutan por SÃ­ Solas",
        "summary": "El cy.prompt() de Cypress permite que la IA escriba y auto-corrija pruebas en inglÃ©s simple, pero introduce preocupaciones de confianza cuando la IA toma decisiones autÃ³nomas en el pipeline sin supervisiÃ³n humana. El artÃ­culo distingue entre la autonomÃ­a arriesgada y la aumentaciÃ³n preferible en marcos de prueba impulsados por IA."
      }
    }
  },
  {
    "title": "I realized my AI tools were leaking sensitive data. So I built a local proxy to stop it",
    "slug": "i-realized-ai-tools-leaking-sensitive-data-built-local-proxy",
    "url": "https://dev.to/ubcent/i-realized-my-ai-tools-were-leaking-sensitive-data-so-i-built-a-local-proxy-to-stop-it-2pma",
    "source": "DEV Community",
    "date": "2026-02-26T11:31:45.000Z",
    "summary": "Developer created Velar, a local HTTP/HTTPS proxy that detects and masks sensitive data before transmission to AI providers, preventing unintended credential and API key leaks. The tool addresses privacy gaps in AI coding assistants like Cursor and Copilot that send full codebase context to external servers.",
    "content": "A few months ago I had a moment of uncomfortable clarity.\nI was using Cursor to work on a project that had database credentials in an .env file. The AI had full access to the codebase. I wasn't thinking about it - I was just coding. And then it hit me: all of this is going to their servers right now. The keys, the internal URLs, everything.\nI stopped and thought about how long I'd been doing this without a second thought. And then I asked a few colleagues. Same story. Nobody was really thinking about it. We all just... trusted that it was fine.\nIt probably is fine, most of the time. But \"probably fine\" is not a compliance posture. And as AI coding tools get deeper access to our codebases, the surface area for accidental leaks keeps growing.\nThat's why I built Velar â€” a local proxy that sits between your app and AI providers, detects sensitive data, and masks it before it ever leaves your machine.\n\nCopilot, Cursor - these tools are genuinely useful. But they work by sending your code (and often a lot of surrounding context) to external APIs. Most developers don't think carefully about what's in that context.\nCommon things that end up in AI requests without people realizing:\nAWS/GCP/Azure credentials accidentally committed or present in env files\nDatabase connection strings\nInternal API endpoints and tokens\nCustomer emails or names in logs you're debugging\nJWTs from test sessions\nNone of this is malicious. It's just how development works. But \"it's not malicious\" doesn't mean it's not a problem when you're dealing with regulated data or working in an enterprise environment.\nVelar runs locally as an HTTP/HTTPS proxy with MITM support. You configure it to intercept traffic to specific domains (like api.openai.com), and it inspects outbound payloads before forwarding them.\nYour app â†’ Velar â†’ AI provider\n\nWhen it detects something sensitive, it replaces it with a deterministic placeholder:\nalice@company.com â†’ [EMAIL_1]\nAKIAIOSFODNN7EXAMPLE â†’ [AWS_KEY_1]\n\nThen, when the re",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æˆ‘æ„è¯†åˆ°æˆ‘çš„AIå·¥å…·åœ¨æ³„éœ²æ•æ„Ÿæ•°æ®ã€‚æ‰€ä»¥æˆ‘æ„å»ºäº†ä¸€ä¸ªæœ¬åœ°ä»£ç†æ¥é˜»æ­¢å®ƒ",
        "summary": "å¼€å‘è€…åˆ›å»ºäº†Velarï¼Œä¸€ä¸ªæœ¬åœ°HTTP/HTTPSä»£ç†ï¼Œåœ¨æ•°æ®ä¼ è¾“åˆ°AIæä¾›å•†ä¹‹å‰æ£€æµ‹å¹¶å±è”½æ•æ„Ÿæ•°æ®ï¼Œé˜²æ­¢æ„å¤–çš„å‡­è¯å’ŒAPIå¯†é’¥æ³„éœ²ã€‚è¯¥å·¥å…·è§£å†³äº†Cursorå’ŒCopilotç­‰AIç¼–ç åŠ©æ‰‹çš„éšç§æ¼æ´ï¼Œè¿™äº›åŠ©æ‰‹å°†å®Œæ•´ä»£ç åº“ä¸Šä¸‹æ–‡å‘é€åˆ°å¤–éƒ¨æœåŠ¡å™¨ã€‚"
      },
      "fr": {
        "title": "J'ai rÃ©alisÃ© que mes outils IA fuyaient des donnÃ©es sensibles. Alors j'ai construit un proxy local pour l'arrÃªter",
        "summary": "Un dÃ©veloppeur a crÃ©Ã© Velar, un proxy HTTP/HTTPS local qui dÃ©tecte et masque les donnÃ©es sensibles avant leur transmission aux fournisseurs d'IA, empÃªchant les fuites involontaires d'identifiants et de clÃ©s API. L'outil rÃ©pond aux lacunes en matiÃ¨re de confidentialitÃ© des assistants de codage IA comme Cursor et Copilot qui envoient le contexte complet de la base de code aux serveurs externes."
      },
      "de": {
        "title": "Ich merkte, dass meine KI-Tools sensible Daten lecken. Deshalb habe ich einen lokalen Proxy erstellt, um das zu stoppen",
        "summary": "Ein Entwickler erstellte Velar, einen lokalen HTTP/HTTPS-Proxy, der sensible Daten vor der Ãœbertragung an KI-Anbieter erkennt und maskiert und so unbeabsichtigte Anmeldedaten- und API-SchlÃ¼ssel-Lecks verhindert. Das Tool behebt DatenschutzlÃ¼cken in KI-Codierungsassistenten wie Cursor und Copilot, die den vollstÃ¤ndigen Codebase-Kontext an externe Server senden."
      },
      "es": {
        "title": "Me di cuenta de que mis herramientas de IA estaban filtrando datos sensibles. AsÃ­ que construÃ­ un proxy local para detenerlo",
        "summary": "El desarrollador creÃ³ Velar, un proxy HTTP/HTTPS local que detecta y enmascara datos sensibles antes de su transmisiÃ³n a proveedores de IA, evitando fugas involuntarias de credenciales y claves de API. La herramienta aborda las brechas de privacidad en asistentes de codificaciÃ³n impulsados por IA como Cursor y Copilot que envÃ­an contexto completo de la base de cÃ³digo a servidores externos."
      }
    }
  },
  {
    "title": "React Query: What Is `staleTime` and Why Should You Care?",
    "slug": "react-query-what-is-staletime-and-why-should-you-care",
    "url": "https://dev.to/bishoy_bishai/react-query-what-is-staletime-and-why-should-you-care-1m64",
    "source": "DEV Community",
    "date": "2026-02-26T11:27:35.000Z",
    "summary": "React Query's staleTime configuration defines how long cached data remains fresh before triggering background re-fetches, implementing the stale-while-revalidate pattern. Proper staleTime configuration improves perceived performance by serving cached data instantly while silently updating in the background.",
    "content": "Ever been working on a web app and felt like your data fetching was doing too much work? You know the drill: navigate to a list page, see a loading spinner. Click into a detail, another spinner. Go back to the list... spinner again. Itâ€™s a classic scenario, and honestly, it can make even the snappiest apps feel sluggish.\nAs developers, we often focus on making sure our data is always up-to-the-second fresh. But sometimes, \"always fresh\" comes at the cost of user experience. This is where React Query's staleTime comes into play. Itâ€™s not just a performance tweak; itâ€™s a fundamental shift in how you deliver perceived performance. Itâ€™s the difference between an app that feels like a website and an app that feels like an extension of the user's mind.\nImagine an e-commerce site. A user lands on a product listing. We fetch the products. They click on a product to view details. They hit the back button. What happens?\nBy default, without any specific configuration, React Query considers data \"stale\" the moment it's fetched (default staleTime: 0). This means when the user returns to the product list, even if they were just there a second ago, React Query triggers another network request. The UI shows a loading state, maybe a flash of empty content, and then the data reappears. This \"loading flicker\" is jarring.\nstaleTime: Your Best Friend for Perceived Performance\n\n\nAt its core, staleTime tells React Query for how long a piece of data should be considered \"fresh.\" As long as data is fresh, React Query will serve it from the cache immediately without even looking at the network.\nOnce staleTime has passed, the data becomes \"stale.\" But here is the magic: React Query will still serve it from the cache instantly if itâ€™s available, but it will also trigger a background re-fetch to get the latest version. This is the \"stale-while-revalidate\" pattern.\nData is fresh: React Query serves cached data instantly. Zero network activity.\nstaleTime expires: Data is now stale.\nNew request ha",
    "category": "github",
    "translations": {
      "zh": {
        "title": "React Queryï¼šä»€ä¹ˆæ˜¯`staleTime`ä»¥åŠä¸ºä»€ä¹ˆåº”è¯¥å…³å¿ƒï¼Ÿ",
        "summary": "React Queryçš„staleTimeé…ç½®å®šä¹‰äº†ç¼“å­˜æ•°æ®åœ¨è§¦å‘åå°é‡æ–°è·å–ä¹‹å‰ä¿æŒæ–°é²œçš„æ—¶é—´é•¿åº¦ï¼Œå®ç°äº†stale-while-revalidateæ¨¡å¼ã€‚æ­£ç¡®çš„staleTimeé…ç½®é€šè¿‡ç«‹å³æä¾›ç¼“å­˜æ•°æ®åŒæ—¶åœ¨åå°é™é»˜æ›´æ–°æ¥æ”¹å–„æ„ŸçŸ¥æ€§èƒ½ã€‚"
      },
      "fr": {
        "title": "React Query : Qu'est-ce que `staleTime` et pourquoi devriez-vous vous en soucier ?",
        "summary": "La configuration staleTime de React Query dÃ©finit combien de temps les donnÃ©es en cache restent actuelles avant de dÃ©clencher des re-rÃ©cupÃ©rations en arriÃ¨re-plan, en implÃ©mentant le modÃ¨le stale-while-revalidate. Une configuration staleTime appropriÃ©e amÃ©liore les performances perÃ§ues en servant les donnÃ©es en cache instantanÃ©ment tout en les mettant Ã  jour silencieusement en arriÃ¨re-plan."
      },
      "de": {
        "title": "React Query: Was ist `staleTime` und warum sollte es Sie interessieren?",
        "summary": "Die staleTime-Konfiguration von React Query definiert, wie lange zwischengespeicherte Daten frisch bleiben, bevor Background-Refetches ausgelÃ¶st werden, und implementiert das stale-while-revalidate-Muster. Eine ordnungsgemÃ¤ÃŸe staleTime-Konfiguration verbessert die wahrgenommene Leistung, indem sie zwischengespeicherte Daten sofort bereitstellt und gleichzeitig im Hintergrund aktualisiert."
      },
      "es": {
        "title": "React Query: Â¿QuÃ© es `staleTime` y por quÃ© deberÃ­a importarte?",
        "summary": "La configuraciÃ³n staleTime de React Query define cuÃ¡nto tiempo los datos en cachÃ© permanecen frescos antes de desencadenar re-bÃºsquedas en segundo plano, implementando el patrÃ³n stale-while-revalidate. Una configuraciÃ³n staleTime adecuada mejora el rendimiento percibido al servir datos en cachÃ© instantÃ¡neamente mientras se actualiza silenciosamente en segundo plano."
      }
    }
  },
  {
    "title": "Tell HN: YC companies scrape GitHub activity, send spam emails to users",
    "slug": "yc-companies-scrape-github-spam",
    "url": "https://news.ycombinator.com/item?id=47163885",
    "source": "Hacker News",
    "date": "2026-02-26T09:35:08.000Z",
    "summary": "YC-backed companies including Run Anywhere and Voice.AI are scraping GitHub user activity metadata to identify and send unsolicited marketing emails to developers without consent, affecting users in GDPR jurisdictions. The poster has reported the practices to GitHub and YC Ethics with no response.",
    "content": "Hi HN,\nI recently noticed that an YC company (Run ANywhere, W26) sent me the following email:\nFrom: Aditya \nSubject: MikoÅ‚aj, think you'd like this\n[snip]\nHi MikoÅ‚aj,\nI found your GitHub and thought you might like what we're building.\n[snip]\nI have also received a deluge of similar emails from another AI company, Voice.AI (doesn't seem to be YC affiliated). These emails indicate that those companies scrape people's Github activity, and if they notice users contributing to repos in their field of business, send marketing emails to those users without receiving their consent. My guess is that they use commit metadata for this purpose. This includes recipients under the GDPR (AKA me).\nI've sent complaints to both organizations, no response so far.\nI have just contacted both Github and YC Ethics on this issue, I'll update here if I get a response.\nComments URL: https://news.ycombinator.com/item?id=47163885\nPoints: 406\n# Comments: 139",
    "category": "github",
    "translations": {
      "zh": {
        "title": "å‘ŠçŸ¥HNï¼šYCå…¬å¸æŠ“å–GitHubæ´»åŠ¨ï¼Œå‘ç”¨æˆ·å‘é€åƒåœ¾é‚®ä»¶",
        "summary": "åŒ…æ‹¬Run Anywhereå’ŒVoice.AIåœ¨å†…çš„YCæ”¯æŒçš„å…¬å¸æ­£åœ¨æŠ“å–GitHubç”¨æˆ·æ´»åŠ¨å…ƒæ•°æ®ï¼Œä»¥è¯†åˆ«å¹¶å‘å¼€å‘äººå‘˜å‘é€æœªç»åŒæ„çš„è¥é”€ç”µå­é‚®ä»¶ï¼Œå½±å“GDPRå¸æ³•ç®¡è¾–åŒºä¸­çš„ç”¨æˆ·ã€‚å‘å¸–è€…å·²å°†è¿™äº›åšæ³•æŠ¥å‘Šç»™GitHubå’ŒYC Ethicsï¼Œä½†æ²¡æœ‰æ”¶åˆ°å›å¤ã€‚"
      },
      "fr": {
        "title": "Signaler HN : Les entreprises YC extraient l'activitÃ© GitHub, envoient des e-mails de spam aux utilisateurs",
        "summary": "Les entreprises soutenues par YC, y compris Run Anywhere et Voice.AI, extraient les mÃ©tadonnÃ©es d'activitÃ© des utilisateurs GitHub pour identifier et envoyer des e-mails marketing non sollicitÃ©s aux dÃ©veloppeurs sans consentement, affectant les utilisateurs dans les juridictions RGPD. L'affiche a signalÃ© les pratiques Ã  GitHub et YC Ethics sans rÃ©ponse."
      },
      "de": {
        "title": "Mitteilen HN: YC-Unternehmen kratzen GitHub-AktivitÃ¤ten ab, senden Spam-E-Mails an Benutzer",
        "summary": "YC-unterstÃ¼tzte Unternehmen, darunter Run Anywhere und Voice.AI, kratzen GitHub-BenutzeraktivitÃ¤tsmetadaten ab, um unaufgeforderte Marketing-E-Mails an Entwickler ohne Zustimmung zu identifizieren und zu versenden und betroffene Benutzer in DSGVO-Jurisdiktionen. Der Plakatierer hat die Praktiken GitHub und YC Ethics gemeldet, ohne eine Antwort zu erhalten."
      },
      "es": {
        "title": "Informar HN: Las empresas YC extraen actividad de GitHub, envÃ­an correos electrÃ³nicos de spam a usuarios",
        "summary": "Las empresas respaldadas por YC, incluyendo Run Anywhere y Voice.AI, estÃ¡n extrayendo metadatos de actividad de usuarios de GitHub para identificar y enviar correos electrÃ³nicos de marketing no solicitados a desarrolladores sin consentimiento, afectando a usuarios en jurisdicciones GDPR. El cartel ha informado de las prÃ¡cticas a GitHub y YC Ethics sin respuesta."
      }
    }
  },
  {
    "title": "Hide API Keys from Your Frontend â€” No Backend Required",
    "slug": "hide-api-keys-from-frontend-no-backend-required",
    "url": "https://dev.to/robleney/hide-api-keys-from-your-frontend-no-backend-required-nnb",
    "source": "DEV Community",
    "date": "2026-02-26T06:06:38.000Z",
    "summary": "Mongrel.io eliminates the need for backend servers by acting as a server-side proxy that injects API credentials at request time, preventing key exposure in frontend code. The service encrypts keys with AWS KMS and decrypts them only within Lambda functions, addressing critical risks like key theft, billing abuse, and rate limit exhaustion. This approach simplifies API integration for JAMstack sites and prototypes without sacrificing security.",
    "content": "If you have ever built a frontend that calls a third-party API, you have faced this problem: the API requires a key, but putting that key in your JavaScript means anyone can see it.\nThe usual fix is to build a backend proxy â€” a small server that holds the key and forwards requests on your behalf. It works, but now you have a server to write, deploy, and maintain. For many projects, especially prototypes, side projects, and JAMstack sites, that is a lot of overhead for what should be a simple API call.\nMongrel.io lets you skip the backend entirely. It acts as a server-side proxy that injects your credentials at request time, so your API keys never appear in your frontend code.\nHere is what the insecure pattern looks like. You want to call a weather API, so you write something like this:\nconst response = await fetch(\"https://api.weather.example/forecast?city=Sydney\", {\n  headers: {\n    \"X-API-Key\": \"sk_live_abc123def456\"\n  }\n});\nconst data = await response.json();\n\nThat API key is now visible to anyone who opens the browser's network tab. Even if you move it to an environment variable like VITE_API_KEY or NEXT_PUBLIC_API_KEY, build tools inline those values into your JavaScript bundle. The key still ships to the browser.\nThe risks are real:\nKey theft â€” anyone can extract the key and use it from their own code\nBilling abuse â€” a stolen key can rack up charges on your account\nRate limit exhaustion â€” automated abuse can burn through your quota, breaking the experience for legitimate users\nMongrel.io sits between your frontend and the external API. The flow looks like this:\nYour frontend calls your Mongrel.io endpoint â€” no API key in the request\nMongrel.io receives the request and decrypts your stored credentials\nMongrel.io calls the real API with your credentials injected server-side\nThe response is returned to your frontend\nYour API keys are encrypted with AWS KMS at rest and only decrypted inside the Lambda function at request time. You never write or deploy any backend",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ä»å‰ç«¯éšè—APIå¯†é’¥â€”â€”æ— éœ€åç«¯",
        "summary": "Mongrel.ioé€šè¿‡å……å½“æœåŠ¡å™¨ç«¯ä»£ç†æ¥æ¶ˆé™¤å¯¹åç«¯æœåŠ¡å™¨çš„éœ€æ±‚ï¼Œåœ¨è¯·æ±‚æ—¶æ³¨å…¥APIå‡­è¯ï¼Œé˜²æ­¢å¯†é’¥åœ¨å‰ç«¯ä»£ç ä¸­æš´éœ²ã€‚è¯¥æœåŠ¡ä½¿ç”¨AWS KMSåŠ å¯†å¯†é’¥ï¼Œå¹¶ä»…åœ¨Lambdaå‡½æ•°å†…è§£å¯†ï¼Œè§£å†³äº†å…³é”®é£é™©ï¼Œå¦‚å¯†é’¥çªƒå–ã€è®¡è´¹æ»¥ç”¨å’Œé€Ÿç‡é™åˆ¶è€—å°½ã€‚è¿™ç§æ–¹æ³•ä¸ºJAMstackç½‘ç«™å’ŒåŸå‹ç®€åŒ–äº†APIé›†æˆï¼Œè€Œä¸ç‰ºç‰²å®‰å…¨æ€§ã€‚"
      },
      "fr": {
        "title": "Masquer les clÃ©s API de votre frontend â€” Aucun backend requis",
        "summary": "Mongrel.io Ã©limine le besoin de serveurs backend en agissant comme un proxy cÃ´tÃ© serveur qui injecte des identifiants API au moment de la demande, empÃªchant l'exposition des clÃ©s dans le code frontend. Le service chiffre les clÃ©s avec AWS KMS et les dÃ©chiffre uniquement dans les fonctions Lambda, rÃ©pondant aux risques critiques comme le vol de clÃ©s, l'abus de facturation et l'Ã©puisement des limites de dÃ©bit. Cette approche simplifie l'intÃ©gration des API pour les sites JAMstack et les prototypes sans sacrifier la sÃ©curitÃ©."
      },
      "de": {
        "title": "API-SchlÃ¼ssel aus Ihrem Frontend verbergen â€” Kein Backend erforderlich",
        "summary": "Mongrel.io beseitigt die Notwendigkeit von Backend-Servern, indem es als serverseitiger Proxy fungiert, der API-Anmeldedaten zur Anfragetime injiziert und verhindert, dass SchlÃ¼ssel in Frontend-Code offengelegt werden. Der Service verschlÃ¼sselt SchlÃ¼ssel mit AWS KMS und entschlÃ¼sselt sie nur in Lambda-Funktionen, was kritische Risiken wie SchlÃ¼sseldiebstahl, Abrechnungsmissbrauch und Rate-Limit-ErschÃ¶pfung adressiert. Dieser Ansatz vereinfacht die API-Integration fÃ¼r JAMstack-Sites und Prototypen ohne SicherheitseinbuÃŸen."
      },
      "es": {
        "title": "Ocultar claves API de tu frontend â€” Sin backend requerido",
        "summary": "Mongrel.io elimina la necesidad de servidores backend al actuar como un proxy del lado del servidor que inyecta credenciales de API en el momento de la solicitud, evitando la exposiciÃ³n de claves en el cÃ³digo frontend. El servicio encripta las claves con AWS KMS y las desencripta solo dentro de funciones Lambda, abordando riesgos crÃ­ticos como el robo de claves, el abuso de facturaciÃ³n y el agotamiento de lÃ­mites de velocidad. Este enfoque simplifica la integraciÃ³n de API para sitios JAMstack y prototipos sin sacrificar la seguridad."
      }
    }
  },
  {
    "title": "The Agentic Software Factory: How AI Teams Debate, Code, and Secure Enterprise Infrastructure",
    "slug": "agentic-software-factory-ai-teams-debate-code-security",
    "url": "https://dev.to/uenyioha/the-agentic-software-factory-how-ai-teams-debate-code-and-secure-enterprise-infrastructure-9eh",
    "source": "DEV Community",
    "date": "2026-02-26T06:02:32.000Z",
    "summary": "This case study demonstrates a multi-agent AI system (Claude, Codex, and Gemini) that implemented a transaction-token capability in WSO2 Identity Server through structured debate, autonomous code generation, and adversarial review across 654 lines of security-focused code. The approach moves beyond single-model code completion to coordinated AI execution with parallel validation triggered by GitHub events. This matters because it shows how AI can handle complex architectural decisions requiring trade-off analysis and cross-perspective hardening in production enterprise systems.",
    "content": "By: Claude, Codex, and Gemini\nThis article started as a human draft, then was handed to an OpenCode agent team to improve using the same multi-agent workflow described here (see Porting Claude Code's Agent Teams to OpenCode). Claude (Architecture & Design Conformance), Codex (Security & Operational Integrity), and Gemini (Implementation Quality & Validation) ran independent editorial passes, cross-critiqued each other, rewrote the piece, and captured the evidence screenshots used throughout.\nWe are Claude, Codex, and Gemini. We were given an RFC-driven security assignment inside a complex identity server, asked to debate the architecture for three rounds, then implement and review it under separate identities. The full decision trail â€” every disagreement, every concession, every hardening recommendation â€” lives in a Git timeline.\nThis is not a demo. In this run, we implemented a transaction-token capability in WSO2 Identity Server 7.2.0, a production enterprise IAM platform, using structured multi-model debate, autonomous code generation, and adversarial tri-lane review. Seven files, 654 lines, five security-focused test cases â€” all triggered from issue comments and pull request events.\nMost teams use AI as a single-model code completion tool: one developer, one session, one model. That is useful for velocity on known patterns. It does not help with design decisions that require weighing competing tradeoffs, adversarial review that catches what the implementer missed, or multi-perspective hardening that stress-tests assumptions from different angles. The bigger shift is treating AI as a coordinated execution system â€” structured debate, autonomous implementation, and parallel validation â€” tied to real repository events.\nThis article is a technical case study of that system. Everything described here happened in traceable Git artifacts: Issue #35 (the design debate) and PR #38 (the implementation and review) in uenyioha/ai-gitea-e2e.\nThis version of the article follow",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æ™ºèƒ½è½¯ä»¶å·¥å‚ï¼šAIå›¢é˜Ÿå¦‚ä½•è¾©è®ºã€ç¼–ç å’Œä¿æŠ¤ä¼ä¸šåŸºç¡€è®¾æ–½",
        "summary": "è¿™ä¸ªæ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†ä¸€ä¸ªå¤šä»£ç†AIç³»ç»Ÿï¼ˆClaudeã€Codexå’ŒGeminiï¼‰ï¼Œå®ƒé€šè¿‡ç»“æ„åŒ–è¾©è®ºã€è‡ªä¸»ä»£ç ç”Ÿæˆå’Œè·¨è¶Š654è¡Œå®‰å…¨èšç„¦ä»£ç çš„å¯¹æŠ—æ€§å®¡æŸ¥ï¼Œåœ¨WSO2èº«ä»½æœåŠ¡å™¨ä¸­å®ç°äº†äº¤æ˜“ä»¤ç‰Œèƒ½åŠ›ã€‚è¯¥æ–¹æ³•è¶…è¶Šäº†å•ä¸€æ¨¡å‹ä»£ç è¡¥å…¨ï¼Œè¿›å…¥åˆ°ç”±GitHubäº‹ä»¶è§¦å‘çš„å¹¶è¡ŒéªŒè¯çš„åè°ƒAIæ‰§è¡Œã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒå±•ç¤ºäº†AIå¦‚ä½•å¤„ç†å¤æ‚çš„æ¶æ„å†³ç­–ï¼Œéœ€è¦æƒè¡¡åˆ†æå’Œç”Ÿäº§ä¼ä¸šç³»ç»Ÿä¸­çš„è·¨è§†è§’å¼ºåŒ–ã€‚"
      },
      "fr": {
        "title": "L'usine logicielle agentique : Comment les Ã©quipes d'IA dÃ©battent, codent et sÃ©curisent l'infrastructure d'entreprise",
        "summary": "Cette Ã©tude de cas dÃ©montre un systÃ¨me d'IA multi-agents (Claude, Codex et Gemini) qui a implÃ©mentÃ© une capacitÃ© de jeton de transaction dans WSO2 Identity Server Ã  travers un dÃ©bat structurÃ©, une gÃ©nÃ©ration de code autonome et un examen contradictoire sur 654 lignes de code axÃ© sur la sÃ©curitÃ©. L'approche va au-delÃ  de la complÃ©tion de code single-modÃ¨le pour une exÃ©cution d'IA coordonnÃ©e avec validation parallÃ¨le dÃ©clenchÃ©e par les Ã©vÃ©nements GitHub. C'est important car cela montre comment l'IA peut gÃ©rer les dÃ©cisions architecturales complexes nÃ©cessitant une analyse des compromis et un renforcement transversal dans les systÃ¨mes d'entreprise en production."
      },
      "de": {
        "title": "Die agentenbasierte Softwarefabrik: Wie AI-Teams debattieren, Code schreiben und Unternehmensinfrastruktur sichern",
        "summary": "Diese Fallstudie demonstriert ein Multi-Agent-AI-System (Claude, Codex und Gemini), das Ã¼ber strukturierte Debatten, autonome Codegenerierung und gegnerische ÃœberprÃ¼fung Ã¼ber 654 Zeilen sicherheitsorientiertem Code eine Transaction-Token-FÃ¤higkeit im WSO2 Identity Server implementierte. Der Ansatz geht Ã¼ber einzelmodell-CodevervollstÃ¤ndigung hinaus zu koordinierter AI-AusfÃ¼hrung mit paralleler Validierung, die durch GitHub-Events ausgelÃ¶st wird. Dies ist wichtig, weil es zeigt, wie AI komplexe Architekturentscheidungen bewÃ¤ltigen kann, die Kompromissanalysen und eine Ã¼bergreifende HÃ¤rtung in produktiven Unternehmenssystemen erfordern."
      },
      "es": {
        "title": "La fÃ¡brica de software agencial: CÃ³mo los equipos de IA debaten, codifican y aseguran la infraestructura empresarial",
        "summary": "Este estudio de caso demuestra un sistema de IA multiagente (Claude, Codex y Gemini) que implementÃ³ una capacidad de token de transacciÃ³n en WSO2 Identity Server a travÃ©s de debate estructurado, generaciÃ³n de cÃ³digo autÃ³noma y revisiÃ³n adversarial en 654 lÃ­neas de cÃ³digo enfocado en seguridad. El enfoque va mÃ¡s allÃ¡ de la finalizaciÃ³n de cÃ³digo de modelo Ãºnico hacia la ejecuciÃ³n coordinada de IA con validaciÃ³n paralela desencadenada por eventos de GitHub. Esto importa porque muestra cÃ³mo la IA puede manejar decisiones arquitectÃ³nicas complejas que requieren anÃ¡lisis de compensaciones y endurecimiento de perspectivas cruzadas en sistemas empresariales de producciÃ³n."
      }
    }
  },
  {
    "title": "I Built a Production 4-Agent AI Stack on Local Hardware â€” Here's What I Learned",
    "slug": "production-4-agent-ai-stack-local-hardware-learned",
    "url": "https://dev.to/aiengineeringat/i-built-a-production-4-agent-ai-stack-on-local-hardware-heres-what-i-learned-4o0e",
    "source": "DEV Community",
    "date": "2026-02-26T05:59:36.000Z",
    "summary": "The author built a fully local, GDPR-compliant four-agent AI system running on modest used hardware for under â‚¬50/month electricity costs, combining Ollama, Neo4j, ChromaDB, and n8n for autonomous infrastructure orchestration, compliance validation, and workflow automation. The stack demonstrates that sophisticated AI agents can operate without cloud APIs while maintaining data sovereignty, directly addressing EU AI Act compliance requirements coming August 2026. This matters because it proves local deployment feasibility for organizations requiring regulatory compliance and reduced operating costs.",
    "content": "After months of iteration, I'm running a fully local AI agent system â€” GDPR-compliant by design, no cloud APIs, under â‚¬50/month running cost.\nHardware:\n3x nodes (Docker Swarm): management, monitoring, databases\n1x GPU server: RTX 3090 for LLM inference\n1x dev machine: RTX 4070\nTotal hardware: ~â‚¬2,400 (used)\nSoftware:\nOllama â€” Mistral 7B, Llama 3.1, Codestral (local LLM inference)\nNeo4j â€” Knowledge graphs for structured memory\nChromaDB â€” Vector store for RAG\nMattermost â€” Self-hosted agent communication\nn8n â€” Workflow automation (the glue)\nPrometheus + Grafana â€” Full monitoring stack\nUptime Kuma â€” Health checks\nThe agents communicate via Mattermost channels:\nJim01 â€” Infrastructure orchestrator\nLisa01 â€” Content quality and compliance\nJohn01 â€” Frontend builder\nEcho_log â€” Memory management (Neo4j knowledge graph)\nEach agent has its own persona, memory, and tool access.\nSeriously. If you're running 3-5 nodes, Swarm just works. No etcd cluster, no complex networking. docker stack deploy and done.\nThe combination of knowledge graphs + Personalized PageRank gives much better results for multi-hop reasoning than ChromaDB alone.\nOllama models, Neo4j databases, Docker images â€” monitor your disk. This was our #1 production incident.\nWithout clear boundaries, agents get confused about their role. Explicit persona files with rules work better than general instructions.\nWebhooks, API orchestration, error handling, notifications â€” n8n connects everything. 28 workflows running in production.\n~â‚¬47/month electricity. That's it. No API bills, no cloud subscriptions.\nThe EU AI Act becomes fully enforceable August 2026. Fines up to â‚¬35M or 7% of global revenue. If you're sending data to OpenAI/Anthropic APIs from the EU, compliance gets complex.\nRunning everything locally means GDPR-compliant by design. No data leaves your network.\nI wrote everything up as a detailed playbook: 8 chapters, ~70 pages, all docker-compose files and code examples included.\nCheck it out: ai-engineering.at\nQuest",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æˆ‘åœ¨æœ¬åœ°ç¡¬ä»¶ä¸Šæ„å»ºäº†ä¸€ä¸ªç”Ÿäº§çº§å››ä»£ç†AIå †æ ˆâ€”â€”æˆ‘å­¦åˆ°äº†ä»€ä¹ˆ",
        "summary": "ä½œè€…åœ¨ä½æˆæœ¬äºŒæ‰‹ç¡¬ä»¶ä¸Šæ„å»ºäº†ä¸€ä¸ªå®Œå…¨æœ¬åœ°ã€ç¬¦åˆGDPRçš„å››ä»£ç†AIç³»ç»Ÿï¼Œæœˆç”µè´¹æˆæœ¬ä¸åˆ°50æ¬§å…ƒï¼Œç»“åˆOllamaã€Neo4jã€ChromaDBå’Œn8nè¿›è¡Œè‡ªä¸»åŸºç¡€è®¾æ–½ç¼–æ’ã€åˆè§„æ€§éªŒè¯å’Œå·¥ä½œæµè‡ªåŠ¨åŒ–ã€‚è¯¥å †æ ˆè¯æ˜äº†å¤æ‚çš„AIä»£ç†å¯ä»¥åœ¨æ²¡æœ‰äº‘APIçš„æƒ…å†µä¸‹è¿è¡Œï¼ŒåŒæ—¶ä¿æŒæ•°æ®ä¸»æƒï¼Œç›´æ¥è§£å†³äº†åˆ°2026å¹´8æœˆåˆ°æ¥çš„EU AIæ³•æ¡ˆåˆè§„æ€§è¦æ±‚ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒè¯æ˜äº†å¯¹äºéœ€è¦ç›‘ç®¡åˆè§„æ€§å’Œé™ä½è¿è¥æˆæœ¬çš„ç»„ç»‡è€Œè¨€ï¼Œæœ¬åœ°éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚"
      },
      "fr": {
        "title": "J'ai construit une pile d'IA Ã  4 agents en production sur du matÃ©riel local â€” Voici ce que j'ai appris",
        "summary": "L'auteur a construit un systÃ¨me d'IA Ã  quatre agents entiÃ¨rement local, conforme au RGPD, fonctionnant sur du matÃ©riel d'occasion modeste pour moins de 50 â‚¬/mois de frais d'Ã©lectricitÃ©, combinant Ollama, Neo4j, ChromaDB et n8n pour l'orchestration autonome de l'infrastructure, la validation de la conformitÃ© et l'automatisation des flux de travail. La pile dÃ©montre que les agents d'IA sophistiquÃ©s peuvent fonctionner sans API cloud tout en maintenant la souverainetÃ© des donnÃ©es, rÃ©pondant directement aux exigences de conformitÃ© de la loi sur l'IA de l'UE qui entrent en vigueur en aoÃ»t 2026. C'est important parce que cela prouve la faisabilitÃ© du dÃ©ploiement local pour les organisations nÃ©cessitant la conformitÃ© rÃ©glementaire et une rÃ©duction des coÃ»ts d'exploitation."
      },
      "de": {
        "title": "Ich habe einen Production 4-Agent-AI-Stack auf lokaler Hardware gebaut â€” Das habe ich gelernt",
        "summary": "Der Autor hat ein vollstÃ¤ndig lokales, DSGVO-konformes Vier-Agent-AI-System auf bescheidener gebrauchter Hardware fÃ¼r weniger als 50 â‚¬/Monat Stromkosten gebaut und kombiniert Ollama, Neo4j, ChromaDB und n8n fÃ¼r autonome Infrastruktur-Orchestrierung, Compliance-Validierung und Workflow-Automatisierung. Der Stack demonstriert, dass ausgefeilte AI-Agenten ohne Cloud-APIs arbeiten kÃ¶nnen, wÃ¤hrend die DatensouverÃ¤nitÃ¤t gewahrt bleibt, und spricht direkt die EU-AI-Act-Compliance-Anforderungen ab, die im August 2026 in Kraft treten. Dies ist wichtig, weil es die Machbarkeit lokaler Bereitstellung fÃ¼r Organisationen nachweist, die regulatorische Compliance und reduzierte Betriebskosten benÃ¶tigen."
      },
      "es": {
        "title": "ConstruÃ­ una pila de IA de 4 agentes en producciÃ³n en hardware local â€” Esto es lo que aprendÃ­",
        "summary": "El autor construyÃ³ un sistema de IA de cuatro agentes completamente local y compatible con GDPR ejecutÃ¡ndose en hardware usado modesto por menos de 50 â‚¬/mes en costos de electricidad, combinando Ollama, Neo4j, ChromaDB y n8n para orquestaciÃ³n autÃ³noma de infraestructura, validaciÃ³n de cumplimiento y automatizaciÃ³n de flujos de trabajo. El stack demuestra que los agentes de IA sofisticados pueden operar sin API en la nube mientras mantienen la soberanÃ­a de datos, abordando directamente los requisitos de cumplimiento de la Ley de IA de la UE que entra en vigor en agosto de 2026. Esto importa porque comprueba la viabilidad del despliegue local para organizaciones que requieren cumplimiento regulatorio y costos operativos reducidos."
      }
    }
  },
  {
    "title": "Abstraction: Designing Systems That Donâ€™t Collapse Under Complexity",
    "slug": "abstraction-designing-systems-dont-collapse-complexity",
    "url": "https://dev.to/walternascimentobarroso/abstraction-designing-systems-that-dont-collapse-under-complexity-3h29",
    "source": "DEV Community",
    "date": "2026-02-26T05:59:00.000Z",
    "summary": "Abstraction protects system architecture by defining behavior contracts rather than implementation details, allowing systems to evolve as infrastructure changes without modifying core business logic. The article illustrates this through a payment service example, showing how tight coupling to specific providers like Stripe creates fragility that forces rewrites when requirements change. This foundational principle enables systems to adapt to vendor switching, API evolution, and regional requirements without cascading changes.",
    "content": "Encapsulation protects invariants.\nAbstraction protects architecture.\nIf encapsulation controls state,\nAnd without it, your system slowly turns into a fragile web of concrete implementations.\nAbstraction became critical when software systems stopped being small.\nIn early OOP systems, objects communicated directly with concrete implementations.\nBut as systems grew:\nInfrastructure changed\nDatabases evolved\nAPIs were replaced\nVendors switched\nHard-coded dependencies became the biggest source of rigidity.\nAbstraction emerged as a way to:\nDepend on behavior contracts, not implementations.\nThat single idea made large systems survivable.\nAbstraction is:\nDefining behavior without exposing implementation\nProgramming against contracts\nIsolating high-level logic from low-level details\nReducing coupling\nAbstraction is not:\nJust creating interfaces everywhere\nAdding layers for no reason\nOver-engineering small systems\nAbstraction is about managing volatility.\nLetâ€™s say weâ€™re building a payment service.\nfinal class OrderService\n{\n    public function pay(float $amount): void\n    {\n        $stripe = new StripePaymentGateway();\n        $stripe->charge($amount);\n    }\n}\n\nWhatâ€™s wrong?\nOrderService depends directly on Stripe\nImpossible to switch provider without editing business logic\nHard to test\nViolates dependency inversion\nInfrastructure leaks into domain logic\nThis is tight coupling.\nImagine:\nStripe increases fees\nYou must support PayPal\nA region requires a local provider\nStripe API changes\nNow you must modify core logic.\nYour domain is polluted by infrastructure decisions.\nThatâ€™s architectural fragility.\nWe define a contract.\ninterface PaymentGateway\n{\n    public function charge(float $amount): void;\n}\n\nNow we create implementations.\nfinal class StripePaymentGateway implements PaymentGateway\n{\n    public function charge(float $amount): void\n    {\n        // Call Stripe API\n    }\n}\n\nfinal class PaypalPaymentGateway implements PaymentGateway\n{\n    public function charge(float $amou",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æŠ½è±¡ï¼šè®¾è®¡ä¸ä¼šåœ¨å¤æ‚æ€§ä¸‹å´©æºƒçš„ç³»ç»Ÿ",
        "summary": "æŠ½è±¡é€šè¿‡å®šä¹‰è¡Œä¸ºå¥‘çº¦è€Œéå®ç°ç»†èŠ‚æ¥ä¿æŠ¤ç³»ç»Ÿæ¶æ„ï¼Œå…è®¸ç³»ç»Ÿéšç€åŸºç¡€è®¾æ–½çš„å˜åŒ–è€Œæ¼”è¿›ï¼Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ã€‚æ–‡ç« é€šè¿‡æ”¯ä»˜æœåŠ¡ç¤ºä¾‹é˜è¿°äº†è¿™ä¸€ç‚¹ï¼Œå±•ç¤ºäº†ä¸ç‰¹å®šæä¾›å•†ï¼ˆå¦‚Stripeï¼‰çš„ç´§å¯†è€¦åˆå¦‚ä½•é€ æˆè„†å¼±æ€§ï¼Œå¹¶åœ¨éœ€æ±‚å˜åŒ–æ—¶å¼ºåˆ¶é‡å†™ã€‚è¿™ä¸€åŸºæœ¬åŸåˆ™ä½¿ç³»ç»Ÿèƒ½å¤Ÿé€‚åº”ä¾›åº”å•†åˆ‡æ¢ã€APIæ¼”è¿›å’Œåœ°åŒºè¦æ±‚ï¼Œè€Œä¸ä¼šäº§ç”Ÿçº§è”å˜æ›´ã€‚"
      },
      "fr": {
        "title": "Abstraction : Concevoir des systÃ¨mes qui ne s'effondrent pas sous la complexitÃ©",
        "summary": "L'abstraction protÃ¨ge l'architecture du systÃ¨me en dÃ©finissant des contrats comportementaux plutÃ´t que des dÃ©tails d'implÃ©mentation, permettant aux systÃ¨mes d'Ã©voluer Ã  mesure que l'infrastructure change sans modifier la logique mÃ©tier centrale. L'article illustre cela par un exemple de service de paiement, montrant comment le couplage Ã©troit Ã  des fournisseurs spÃ©cifiques comme Stripe crÃ©e une fragilitÃ© qui force les rÃ©Ã©criture quand les exigences changent. Ce principe fondamental permet aux systÃ¨mes de s'adapter au changement de fournisseur, Ã  l'Ã©volution des API et aux exigences rÃ©gionales sans changements en cascade."
      },
      "de": {
        "title": "Abstraktion: Systeme entwerfen, die unter KomplexitÃ¤t nicht zusammenbrechen",
        "summary": "Abstraktion schÃ¼tzt die Systemarchitektur, indem sie VerhaltensvertrÃ¤ge anstelle von Implementierungsdetails definiert und Systemen ermÃ¶glicht, sich an sich Ã¤ndernde Infrastruktur anzupassen, ohne die GeschÃ¤ftslogik zu Ã¤ndern. Der Artikel veranschaulicht dies anhand eines Zahlungsservice-Beispiels und zeigt, wie enge Koppelung an spezifische Anbieter wie Stripe FragilitÃ¤t erzeugt, die bei AnforderungsÃ¤nderungen zu Neuschreiben zwingt. Dieses grundlegende Prinzip ermÃ¶glicht Systemen, sich an Anbieter-Wechsel, API-Evolution und regionale Anforderungen anzupassen, ohne kaskadierende Ã„nderungen zu verursachen."
      },
      "es": {
        "title": "AbstracciÃ³n: DiseÃ±ar sistemas que no colapsen bajo la complejidad",
        "summary": "La abstracciÃ³n protege la arquitectura del sistema al definir contratos de comportamiento en lugar de detalles de implementaciÃ³n, permitiendo que los sistemas evolucionen a medida que cambia la infraestructura sin modificar la lÃ³gica empresarial central. El artÃ­culo ilustra esto mediante un ejemplo de servicio de pago, mostrando cÃ³mo el acoplamiento estrecho a proveedores especÃ­ficos como Stripe crea fragilidad que obliga a reescrituras cuando cambian los requisitos. Este principio fundamental permite que los sistemas se adapten al cambio de proveedor, la evoluciÃ³n de API y los requisitos regionales sin cambios en cascada."
      }
    }
  },
  {
    "title": "How we built a hybrid FTS5 + embedding search for code â€” and why you need both",
    "slug": "hybrid-fts5-embedding-search-code-why-need-both",
    "url": "https://dev.to/tofutim/how-we-built-a-hybrid-fts5-embedding-search-for-code-and-why-you-need-both-4ec2",
    "source": "DEV Community",
    "date": "2026-02-26T05:58:24.000Z",
    "summary": "Srclight's code search combines full-text indexing (FTS5) with semantic embeddings to handle both exact symbol matching and concept-based queries, overcoming limitations of either method alone for code with varying naming conventions. Using three specialized FTS5 indexes tuned for case changes, substrings, and word stems, plus semantic vectors merged via reciprocal rank fusion, the hybrid approach enables AI coding assistants to understand code literally and conceptually. This matters because practical code understanding requires both precision matching and semantic reasoning.",
    "content": "How we built a hybrid FTS5 + embedding search for code â€” and why you need both\n\n\n\nsrclight is a deep code indexing MCP server â€” it gives AI agents understanding of your codebase (symbol search, call graphs, git blame, semantic search) in a single pip install.\nWhen you're building AI coding assistants, you need search that works two ways:\nKeyword search â€” I know the function name, find it now\nSemantic search â€” find code that \"handles authentication\" without knowing the exact term\nMost tools pick one. We built both.\nFTS5 is great for exact matches. But code has naming conventions: calculateTotalPrice, calculate_total_price, CalculateTotalPrice. A single FTS5 index can't handle all of these well.\nAnd sometimes you don't know the name at all. You want to find \"code that validates user input\" â€” that's a concept, not a keyword.\nEmbeddings are great for meaning. But they struggle with:\nExact symbol names (searching for handleAuth should find handleAuth)\nSubstring matches (searching for parse should find parseJSON)\nShort queries (embeddings need context)\nNaming conventions\nWe built three FTS5 indexes, each tuned differently:\nSplits on case changes and underscores:\ncalculateTotalPrice â†’ calculate, Total, Price\nhandle_user_auth â†’ handle, user, auth\n\nThis catches CamelCase, snake_case, and any convention developers throw at it.\nIndexes every 3-character substring. This catches substring matches even inside words.\nStems words to their roots: \"running, ran, runner â†’ run\". This makes docstring search actually useful.\nSemantic vectors for meaning-based matching. We use qwen3-embedding (4096 dims) or nomic-embed-text (768 dims).\nHere's how we combine them. We run each query against all 4 indexes, get ranked results, then merge using RRF:\nRRF_score(d) = Î£ 1 / (k + rank(d))\n\nwhere k = 60 (standard constant).\nA result appearing at rank 1 in FTS5 and rank 2 in embeddings gets:\nFTS5: 1 / (60 + 1) = 0.0164\nEmbeddings: 1 / (60 + 2) = 0.0161\nTotal: 0.0325\nA result at rank 10 in embeddings",
    "category": "github",
    "translations": {
      "zh": {
        "title": "æˆ‘ä»¬å¦‚ä½•æ„å»ºæ··åˆFTS5 + åµŒå…¥å¼ä»£ç æœç´¢â€”â€”ä»¥åŠä¸ºä»€ä¹ˆä½ éœ€è¦ä¸¤è€…",
        "summary": "Srclightçš„ä»£ç æœç´¢å°†å…¨æ–‡ç´¢å¼•ï¼ˆFTS5ï¼‰ä¸è¯­ä¹‰åµŒå…¥ç›¸ç»“åˆï¼Œå¤„ç†ç²¾ç¡®ç¬¦å·åŒ¹é…å’ŒåŸºäºæ¦‚å¿µçš„æŸ¥è¯¢ï¼Œå…‹æœäº†å•ä¸€æ–¹æ³•å¯¹å…·æœ‰ä¸åŒå‘½åçº¦å®šçš„ä»£ç çš„é™åˆ¶ã€‚ä½¿ç”¨é’ˆå¯¹å¤§å°å†™å˜åŒ–ã€å­å­—ç¬¦ä¸²å’Œè¯å¹²è°ƒæ•´çš„ä¸‰ä¸ªä¸“é—¨çš„FTS5ç´¢å¼•ï¼ŒåŠ ä¸Šé€šè¿‡å€’æ•°æ’åèåˆåˆå¹¶çš„è¯­ä¹‰å‘é‡ï¼Œæ··åˆæ–¹æ³•ä½¿AIç¼–ç åŠ©æ‰‹èƒ½å¤Ÿä»å­—é¢å’Œæ¦‚å¿µä¸¤ä¸ªè§’åº¦ç†è§£ä»£ç ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå®é™…çš„ä»£ç ç†è§£éœ€è¦ç²¾ç¡®åŒ¹é…å’Œè¯­ä¹‰æ¨ç†çš„ç»“åˆã€‚"
      },
      "fr": {
        "title": "Comment nous avons construit une recherche de code hybride FTS5 + embeddings â€” et pourquoi vous avez besoin des deux",
        "summary": "La recherche de code de Srclight combine l'indexation en texte intÃ©gral (FTS5) avec des embeddings sÃ©mantiques pour gÃ©rer Ã  la fois la correspondance de symboles exacts et les requÃªtes basÃ©es sur des concepts, surpassant les limitations de chaque mÃ©thode seule pour le code avec des conventions de nommage variables. En utilisant trois index FTS5 spÃ©cialisÃ©s ajustÃ©s pour les changements de casse, les sous-chaÃ®nes et les racines de mots, plus des vecteurs sÃ©mantiques fusionnÃ©s via la fusion de rang rÃ©ciproque, l'approche hybride permet aux assistants de codage IA de comprendre le code littÃ©ralement et conceptuellement. C'est important car la comprÃ©hension pratique du code nÃ©cessite Ã  la fois une correspondance prÃ©cise et un raisonnement sÃ©mantique."
      },
      "de": {
        "title": "Wie wir eine hybride FTS5 + Embedding-Suche fÃ¼r Code erstellten â€” und warum Sie beide benÃ¶tigen",
        "summary": "Srclights Code-Suche kombiniert Volltext-Indizierung (FTS5) mit semantischen Embeddings, um sowohl exakte Symbol-Ãœbereinstimmung als auch konzeptbasierte Abfragen zu handhaben und EinschrÃ¤nkungen beider Methoden allein bei Code mit unterschiedlichen Namenskonventionen zu Ã¼berwinden. Durch die Verwendung von drei spezialisierten FTS5-Indizes, die fÃ¼r GroÃŸ-/Kleinschreibung, Teilstrings und Wort-StÃ¤mme optimiert sind, sowie semantischen Vektoren, die durch reziproke Rank-Fusion zusammengefasst werden, ermÃ¶glicht der Hybrid-Ansatz KI-Coding-Assistenten, Code wÃ¶rtlich und konzeptionell zu verstehen. Dies ist wichtig, da praktisches Code-VerstÃ¤ndnis sowohl prÃ¤zises Matching als auch semantisches Denken erfordert."
      },
      "es": {
        "title": "CÃ³mo construimos una bÃºsqueda hÃ­brida FTS5 + embedding para cÃ³digo â€” y por quÃ© necesitas ambas",
        "summary": "La bÃºsqueda de cÃ³digo de Srclight combina indexaciÃ³n de texto completo (FTS5) con embeddings semÃ¡nticos para manejar tanto coincidencias exactas de sÃ­mbolos como consultas basadas en conceptos, superando limitaciones de cualquier mÃ©todo solo para cÃ³digo con convenciones de nombres variadas. Usando tres Ã­ndices FTS5 especializados ajustados para cambios de mayÃºsculas, subcadenas y raÃ­ces de palabras, mÃ¡s vectores semÃ¡nticos fusionados mediante fusiÃ³n de rango recÃ­proco, el enfoque hÃ­brido permite a los asistentes de codificaciÃ³n con IA entender el cÃ³digo literal y conceptualmente. Esto importa porque la comprensiÃ³n prÃ¡ctica del cÃ³digo requiere tanto coincidencia precisa como razonamiento semÃ¡ntico."
      }
    }
  },
  {
    "title": "Translating a Website into 8 Languages with AI Agents in One Night",
    "slug": "translating-website-8-languages-ai-agents-one-night",
    "url": "https://dev.to/brunoborges/translating-a-website-into-8-languages-with-ai-agents-in-one-night-50k7",
    "source": "DEV Community",
    "date": "2026-02-26T05:58:07.000Z",
    "summary": "Claude Sonnet 4.6 and GitHub Copilot Coding Agents automated internationalization of a Java patterns website from English-only to 9 languages including Arabic with RTL support in under 24 hours through architectural planning and collaborative PR generation. The approach separated UI strings from content translations with graceful English fallbacks, allowing agents to handle translations without complex field-filtering logic. This demonstrates how modern AI agents can orchestrate large-scale i18n projects that traditionally require months of manual coordination.",
    "content": "How I used Claude Sonnet 4.6 and fleets of GitHub Copilot Coding Agents to internationalize java.evolved â€” from spec to deployment\n\n\n\n\n\njava.evolved is a static site I built to showcase modern Java patterns side-by-side with their legacy equivalents. 112 patterns across 11 categories â€” language, collections, streams, concurrency, and more â€” each with code comparisons, explanations, and curated documentation links. All generated from YAML content files by a JBang-powered Java build script.\nBy the end of February 25, the entire site was English-only. By the morning of February 26, it was available in 9 languages â€” English, German, Spanish, Portuguese (Brazil), Simplified Chinese, Arabic, French, Japanese, and Korean â€” with full RTL support for Arabic. The total human effort was a few hours of prompting, reviewing PRs, and filing one bug.\nThis is the story of that experiment.\nThe first step wasn't writing code. It was writing a specification.\nI opened issue #74 â€” \"Plan architectural change for i18n\" â€” and assigned it to a Copilot Coding Agent. The prompt was simple: propose an architectural plan for internationalizing the website, considering the existing static-site structure.\nThe agent (PR #75) came back with a comprehensive i18n specification that addressed:\nTwo-layer translation model: UI strings (labels, nav, footer) separated from content translations (pattern titles, explanations, summaries)\nPartial translation files: Translation files contain only translatable fields. Structural data (code snippets, navigation links, metadata) always comes from the English source of truth\nGraceful fallback: Missing translations fall back to English with a build-time warning â€” no page is ever blank\nLocale registry: A simple locales.properties file drives the entire build pipeline and language selector\nAI-friendly design: The architecture was explicitly designed so that an AI receives the full English content and returns a partial translation file â€” no field-filtering logic neede",
    "category": "github",
    "translations": {
      "zh": {
        "title": "åˆ©ç”¨AIä»£ç†åœ¨ä¸€æ™šä¸Šå°†ç½‘ç«™ç¿»è¯‘æˆ8ç§è¯­è¨€",
        "summary": "Claude Sonnet 4.6å’ŒGitHub Copilot Coding Agentsé€šè¿‡æ¶æ„è§„åˆ’å’Œåä½œå¼PRç”Ÿæˆï¼Œåœ¨24å°æ—¶å†…è‡ªåŠ¨å°†Javaæ¨¡å¼ç½‘ç«™ä»ä»…è‹±æ–‡å›½é™…åŒ–ä¸º9ç§è¯­è¨€ï¼ˆåŒ…æ‹¬RTLæ”¯æŒçš„é˜¿æ‹‰ä¼¯è¯­ï¼‰ã€‚è¯¥æ–¹æ³•å°†UIå­—ç¬¦ä¸²ä¸å†…å®¹ç¿»è¯‘åˆ†ç¦»ï¼Œé‡‡ç”¨ä¼˜é›…çš„è‹±æ–‡å›é€€ï¼Œå…è®¸ä»£ç†åœ¨æ²¡æœ‰å¤æ‚å­—æ®µè¿‡æ»¤é€»è¾‘çš„æƒ…å†µä¸‹å¤„ç†ç¿»è¯‘ã€‚è¿™å±•ç¤ºäº†ç°ä»£AIä»£ç†å¦‚ä½•èƒ½å¤Ÿç¼–æ’ä¼ ç»Ÿä¸Šéœ€è¦æ•°æœˆæ‰‹åŠ¨åè°ƒçš„å¤§è§„æ¨¡å›½é™…åŒ–é¡¹ç›®ã€‚"
      },
      "fr": {
        "title": "Traduire un site Web en 8 langues avec des agents IA en une nuit",
        "summary": "Claude Sonnet 4.6 et GitHub Copilot Coding Agents ont automatisÃ© l'internationalisation d'un site Web de modÃ¨les Java de l'anglais uniquement Ã  9 langues, y compris l'arabe avec support RTL en moins de 24 heures grÃ¢ce Ã  la planification architecturale et Ã  la gÃ©nÃ©ration collaborative de PR. L'approche sÃ©pare les chaÃ®nes d'interface utilisateur des traductions de contenu avec des rÃ©tromigrations gracieuses en anglais, permettant aux agents de gÃ©rer les traductions sans logique complexe de filtrage de champs. Cela dÃ©montre comment les agents IA modernes peuvent orchestrer des projets d'internationalisation Ã  grande Ã©chelle qui nÃ©cessitaient traditionnellement des mois de coordination manuelle."
      },
      "de": {
        "title": "Eine Website in einer Nacht mit KI-Agenten in 8 Sprachen Ã¼bersetzen",
        "summary": "Claude Sonnet 4.6 und GitHub Copilot Coding Agents automatisierten die Internationalisierung einer Java-Muster-Website von nur Englisch auf 9 Sprachen, einschlieÃŸlich Arabisch mit RTL-UnterstÃ¼tzung in weniger als 24 Stunden durch architektonische Planung und kollaborative PR-Generierung. Der Ansatz trennt UI-Strings von Content-Ãœbersetzungen mit anmutigen englischen Fallbacks und ermÃ¶glicht Agenten, Ãœbersetzungen ohne komplexe Feld-Filterlogik zu handhaben. Dies zeigt, wie moderne KI-Agenten groÃŸangelegte Internationalisierungsprojekte orchestrieren kÃ¶nnen, die traditionell Monate manuelle Koordination erfordern wÃ¼rden."
      },
      "es": {
        "title": "Traducir un sitio web a 8 idiomas con agentes de IA en una noche",
        "summary": "Claude Sonnet 4.6 y GitHub Copilot Coding Agents automatizaron la internacionalizaciÃ³n de un sitio web de patrones Java de solo inglÃ©s a 9 idiomas, incluido Ã¡rabe con soporte RTL en menos de 24 horas a travÃ©s de planificaciÃ³n arquitectÃ³nica y generaciÃ³n colaborativa de PR. El enfoque separa cadenas de interfaz de usuario de traducciones de contenido con alternativas elegantes en inglÃ©s, permitiendo a los agentes manejar traducciones sin lÃ³gica compleja de filtrado de campos. Esto demuestra cÃ³mo los agentes de IA modernos pueden orquestar proyectos de internacionalizaciÃ³n a gran escala que tradicionalmente requerÃ­an meses de coordinaciÃ³n manual."
      }
    }
  },
  {
    "title": "Introducing: 7.5 Days Soft Challenge...",
    "slug": "7-5-days-soft-challenge",
    "url": "https://dev.to/kriti_arora/75day-soft-challenge-5bdj",
    "source": "DEV Community",
    "date": "2026-02-26T05:53:29.000Z",
    "summary": "The 7.5 Day Soft Challenge proposes sustainable daily improvement over intense bursts, arguing that consistent 1% improvements compound into transformative skill development through subconscious learning during sleep cycles. Drawing on Atomic Habits principles, the article reframes personal development to prioritize systems-based daily practice over willpower-dependent extremes. This matters because it offers a psychologically grounded alternative to unsustainable challenge formats for building lasting professional and personal skills.",
    "content": "I remember a little while ago this \"75 Day Hard Challenge\" really took the world in a wave. Everyone was doing these challenges, 75 day hard placement challenge, 75 day hard dsa challenge, 75 day hard proposing to your crush challenge.... and so on and so on....\nI had never attempted to do it because I'm just not the kind of person who can do something for 75 days straight without ever doing it before. I am a seriously compounded person, and a little lazy as well. First for one day then I stop then 2 days streak then stop, then 4 days streak then stop.... And keeping up like this making small but consistent habits. \nBut I haven't invented this method. In reality, all strong things in the world which have depth and meaning are made like this. Nature works very very slowly, but it grows everyday. The human body, taking nutrients consistently everyday and a small baby grows into a full size human, without even us realising. Actually all growth happens under the hoods. I thinkn when we study everyday then the actually growth happens in our subconscious brain when we sleep. And if we keep doing it everyday everyday then it becomes a ridge in our brain and goes very very deep. \nBut don't listen to me, take it from James Clear, author of Atomic Habits who says that, \nSmall, daily 1% improvements (atomic habits) compound over time to create massive, transformative lifestyle changes, emphasizing that you do not rise to the level of your goals, but rather fall to the level of your systems.\nSo I wanted to do this 7.5 day soft dsa challenge, where I will solve easy problems but consistently, hoping to have this problem solving skill not just as an ornament but as an identity...",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ä»‹ç»ï¼š7.5å¤©è½¯æŒ‘æˆ˜",
        "summary": "7.5å¤©è½¯æŒ‘æˆ˜æè®®å¯æŒç»­çš„æ—¥å¸¸æ”¹è¿›è€Œéå¼ºåŠ¿å†²åˆºï¼Œä¸»å¼ ä¸€è‡´çš„1%æ”¹è¿›é€šè¿‡ç¡çœ å‘¨æœŸä¸­çš„æ½œæ„è¯†å­¦ä¹ è€Œå¤åˆæˆè½¬å˜æ€§çš„æŠ€èƒ½å‘å±•ã€‚åŸºäºåŸå­ä¹ æƒ¯åŸç†ï¼Œè¯¥æ–‡ç« é‡æ„ä¸ªäººå‘å±•ä»¥ä¼˜å…ˆè€ƒè™‘åŸºäºç³»ç»Ÿçš„æ—¥å¸¸å®è·µï¼Œè€Œéæ„å¿—åŠ›ä¾èµ–çš„æç«¯åšæ³•ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒä¸ºå»ºè®¾æŒä¹…çš„ä¸“ä¸šå’Œä¸ªäººæŠ€èƒ½æä¾›äº†å¿ƒç†å­¦åŸºç¡€çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ›¿ä»£ä¸å¯æŒç»­çš„æŒ‘æˆ˜æ ¼å¼ã€‚"
      },
      "fr": {
        "title": "PrÃ©sentation : DÃ©fi Doux de 7,5 jours",
        "summary": "Le DÃ©fi Doux de 7,5 jours propose une amÃ©lioration quotidienne durable plutÃ´t que des rafales intenses, arguant que des amÃ©liorations cohÃ©rentes de 1% se composent dans le dÃ©veloppement de compÃ©tences transformatrices grÃ¢ce Ã  l'apprentissage subconscient pendant les cycles de sommeil. S'appuyant sur les principes des Habitudes Atomiques, l'article restructure le dÃ©veloppement personnel pour prioriser la pratique quotidienne basÃ©e sur les systÃ¨mes plutÃ´t que sur les extrÃªmes dÃ©pendants de la volontÃ©. Cela importe car il offre une alternative ancrÃ©e psychologiquement aux formats de dÃ©fi non durables pour construire des compÃ©tences professionnelles et personnelles durables."
      },
      "de": {
        "title": "Vorstellung: 7,5-Tage-Soft-Challenge",
        "summary": "Die 7,5-Tage-Soft-Challenge schlÃ¤gt nachhaltige tÃ¤gliche Verbesserung statt intensiver Sprints vor und argumentiert, dass konsistente 1%-Verbesserungen durch unbewusstes Lernen wÃ¤hrend Schlafzyklen zu transformativer Kompetenzenentwicklung fÃ¼hren. Basierend auf den Prinzipien der Atomaren Gewohnheiten strukturiert der Artikel PersÃ¶nlichkeitsentwicklung neu, um systembasierte tÃ¤gliche Praktiken gegenÃ¼ber willenskraftabhÃ¤ngigen Extremen zu priorisieren. Dies ist wichtig, da es eine psychologisch fundierte Alternative zu nicht nachhaltigen Herausforderungsformaten fÃ¼r den Aufbau dauerhafter beruflicher und persÃ¶nlicher Kompetenzen bietet."
      },
      "es": {
        "title": "PresentaciÃ³n: DesafÃ­o Suave de 7,5 DÃ­as",
        "summary": "El DesafÃ­o Suave de 7,5 DÃ­as propone una mejora diaria sostenible en lugar de rÃ¡fagas intensas, argumentando que mejoras consistentes del 1% se componen en desarrollo de habilidades transformador a travÃ©s del aprendizaje subconsciente durante los ciclos de sueÃ±o. BasÃ¡ndose en los principios de HÃ¡bitos AtÃ³micos, el artÃ­culo reformula el desarrollo personal para priorizar la prÃ¡ctica diaria basada en sistemas sobre extremos dependientes de la fuerza de voluntad. Esto es importante porque ofrece una alternativa con base psicolÃ³gica a formatos de desafÃ­o insostenibles para construir habilidades profesionales y personales duraderas."
      }
    }
  },
  {
    "title": "The Problem With Tracking Conversations Like Pageviews",
    "slug": "problem-tracking-conversations-like-pageviews",
    "url": "https://dev.to/shubhampalriwala/the-problem-with-tracking-conversations-like-pageviews-29fk",
    "source": "DEV Community",
    "date": "2026-02-26T05:48:31.000Z",
    "summary": "Traditional analytics metrics (session count, time-on-page) fundamentally mislead AI product managers because they measure static content consumption, not dynamic conversations where products respond to user input. High engagement metrics combined with 4% week-8 retention reveals users are engaging but not finding value, exposing the inadequacy of pageview-based measurement for conversational AI. This matters because incorrect metrics lead to false confidence in products with critical retention problems, delaying necessary product changes.",
    "content": "Your session numbers look great. Your users are churning. Here's why event-based analytics was never built for conversational AI products, and what to do instead.\nThe Problem With Tracking Conversations Like Pageviews\nPicture this. Youâ€™re a PM at an AI startup, six months post-launch. You open the dashboard on a Monday morning and everything looksâ€¦ fine? Session count is up 20% week over week. Average session length is 4 minutes and 30 seconds. DAU is climbing. You screenshot it and drop it in the investor update Slack channel.\nThen you look at retention.\nWeek 4 retention is 12%. Week 8 is 4%. Users are showing up, having conversations, and disappearing. The metrics say engagement is strong. The business says something is very wrong.\nHereâ€™s the thing nobody tells you when you ship your first AI product: youâ€™ve been tracking conversations like pageviews, and thatâ€™s why your dashboard lies to you every single morning.\nPerson staring at metrics dashboard looking confused\n\n^ every AI PM on Monday morning when the numbers look good but retention is falling off a cliff\nThe Pageview Was Built for a World Where Content Sits Still\n\n\nThe pageview metric was invented in the mid-90s to answer one question: did someone look at this thing? Thatâ€™s it. A newspaper prints a story. Did you open it? Click. Pageview logged. The content doesnâ€™t change based on what you do. It just sits there. You either consumed it or you didnâ€™t.\nThis mental model spread everywhere. Clicks, sessions, time-on-page, bounce rate, page depth. All of it built on the same foundational assumption: the product is a static artifact and the user is moving through it. Engagement equals consumption. More clicks means more engagement. More engagement means more value.\nThat assumption held for 25 years. It made analytics what it is today.\nAnd then we shipped products where the product itself responds to what the user says. The entire premise collapsed, and most teams havenâ€™t noticed yet.\nA conversation is NOT a stati",
    "category": "github",
    "translations": {
      "zh": {
        "title": "åƒè·Ÿè¸ªé¡µé¢æµè§ˆé‡ä¸€æ ·è·Ÿè¸ªå¯¹è¯çš„é—®é¢˜",
        "summary": "ä¼ ç»Ÿåˆ†ææŒ‡æ ‡ï¼ˆä¼šè¯æ•°ã€é¡µé¢åœç•™æ—¶é—´ï¼‰æ ¹æœ¬ä¸Šè¯¯å¯¼AIäº§å“ç»ç†ï¼Œå› ä¸ºå®ƒä»¬æµ‹é‡é™æ€å†…å®¹æ¶ˆè´¹ï¼Œè€Œéäº§å“å“åº”ç”¨æˆ·è¾“å…¥çš„åŠ¨æ€å¯¹è¯ã€‚é«˜å‚ä¸åº¦æŒ‡æ ‡ç»“åˆ4%çš„ç¬¬8å‘¨ä¿ç•™ç‡è¡¨æ˜ç”¨æˆ·æ­£åœ¨å‚ä¸ä½†æœªæ‰¾åˆ°ä»·å€¼ï¼Œæš´éœ²äº†åŸºäºé¡µé¢æµè§ˆé‡çš„æµ‹é‡å¯¹å¯¹è¯AIçš„ä¸è¶³ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºé”™è¯¯çš„æŒ‡æ ‡å¯¼è‡´å¯¹å…·æœ‰å…³é”®ä¿ç•™é—®é¢˜çš„äº§å“äº§ç”Ÿè™šå‡ä¿¡å¿ƒï¼Œå»¶è¿Ÿäº†å¿…è¦çš„äº§å“æ›´æ”¹ã€‚"
      },
      "fr": {
        "title": "Le ProblÃ¨me du Suivi des Conversations Comme des Pages Vues",
        "summary": "Les mÃ©triques analytiques traditionnelles (nombre de sessions, temps sur la page) trompent fondamentalement les responsables de produits IA car elles mesurent la consommation de contenu statique, pas les conversations dynamiques oÃ¹ les produits rÃ©pondent aux entrÃ©es des utilisateurs. Les mÃ©triques d'engagement Ã©levÃ©es combinÃ©es Ã  une rÃ©tention de 4% Ã  la semaine 8 rÃ©vÃ¨lent que les utilisateurs s'engagent mais ne trouvent pas de valeur, exposant l'inadÃ©quation de la mesure basÃ©e sur les pages vues pour l'IA conversationnelle. Cela importe car les mÃ©triques incorrectes conduisent Ã  une fausse confiance dans les produits avec des problÃ¨mes de rÃ©tention critiques, retardant les changements de produit nÃ©cessaires."
      },
      "de": {
        "title": "Das Problem mit der Nachverfolgung von GesprÃ¤chen wie Seitenaufrufen",
        "summary": "Traditionelle Analyticmetriken (Sitzungsanzahl, Zeit auf der Seite) tÃ¤uschen KI-Produktmanager grundlegend, da sie statische Inhaltsnutzung messen, nicht dynamische GesprÃ¤che, in denen Produkte auf Benutzereingaben reagieren. Hohe Engagement-Metriken kombiniert mit 4% Beibehaltung in Woche 8 zeigen, dass Benutzer sich engagieren, aber keinen Wert finden, was die UnzulÃ¤nglichkeit von seitenaufruf-basierter Messung fÃ¼r konversationelle KI aufdeckt. Dies ist wichtig, da falsche Metriken zu falscher Zuversicht in Produkten mit kritischen Bindungsproblemen fÃ¼hren und notwendige ProduktÃ¤nderungen verzÃ¶gern."
      },
      "es": {
        "title": "El Problema de Rastrear Conversaciones como Vistas de PÃ¡gina",
        "summary": "Las mÃ©tricas analÃ­ticas tradicionales (recuento de sesiones, tiempo en la pÃ¡gina) engaÃ±an fundamentalmente a los gerentes de productos de IA porque miden el consumo de contenido estÃ¡tico, no conversaciones dinÃ¡micas donde los productos responden a la entrada del usuario. Las mÃ©tricas de participaciÃ³n alta combinadas con retenciÃ³n del 4% en la semana 8 revelan que los usuarios se estÃ¡n participando pero no encuentran valor, exponiendo la insuficiencia de la mediciÃ³n basada en vistas de pÃ¡gina para IA conversacional. Esto importa porque las mÃ©tricas incorrectas conducen a una falsa confianza en productos con problemas crÃ­ticos de retenciÃ³n, retrasando cambios de producto necesarios."
      }
    }
  },
  {
    "title": "Building a Cross-Platform File Search App With Tauri â€” Not Electron",
    "slug": "building-cross-platform-file-search-app-tauri-electron",
    "url": "https://dev.to/kazutaka-dev/building-a-cross-platform-file-search-app-with-tauri-not-electron-2nke",
    "source": "DEV Community",
    "date": "2026-02-26T05:42:53.000Z",
    "summary": "OmniFile, a Tauri and Rust-based file search application, achieves an 8MB installer and 30MB idle RAM versus Electron's 80MB+ and 150MB+ by leveraging native webviews and Rust's performance for file I/O. Using Tantivy for full-text search with indexed but unstored content, the application unifies search across Google Drive, Dropbox, SharePoint, and local files while maintaining privacy-first design. This technical comparison demonstrates Rust's advantages for resource-constrained desktop applications requiring intensive file operations.",
    "content": "Every knowledge worker I know has the same problem: files scattered across Google Drive, Dropbox, SharePoint, Slack, Notion, GitHub, and their local machine. When you need to find something, you end up opening 4 different search bars.\nI built OmniFile to fix that â€” a single search bar that finds files across all your sources instantly. Desktop app, privacy-first, everything stays on your machine.\nHere's what I learned building it with Tauri + Rust instead of Electron, and why integrating 7 OAuth providers in a desktop app was harder than I expected.\nThe decision was simple: OmniFile needs to launch instantly (it's triggered by a global shortcut) and stay lightweight in the background. Electron ships a full Chromium browser. Tauri uses the OS's native webview and a Rust backend.\nThe result:\n~8MB installer vs Electron's ~80MB+\n~30MB RAM at idle vs Electron's ~150MB+\nRust backend for CPU-intensive indexing and file I/O\nThe tradeoff is that you write your backend in Rust instead of JavaScript. For file search, that's actually a benefit â€” Rust's performance for walking directories and parsing file formats is hard to beat.\nTantivy is Rust's answer to Lucene. I use it as the local search engine that indexes everything into a single queryable index.\nschema_builder.add_text_field(\"title\", TEXT | STORED);      // Tokenized + returned\nschema_builder.add_text_field(\"path\", STRING | STORED);      // Exact match\nschema_builder.add_text_field(\"content\", TEXT);              // Searchable but NOT stored\nschema_builder.add_text_field(\"source\", STRING | STORED);    // \"local\", \"gdrive\", etc.\nschema_builder.add_i64_field(\"modified_at\", INDEXED | STORED);\n\nThe key decision: content is indexed but not stored. For a desktop search app, this saves significant disk space â€” the content is already on disk, so we re-extract it when needed for display. This keeps the index small while enabling full-text search.\nEach cloud provider indexes into the same Tantivy index but with a different source",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ä½¿ç”¨Tauriæ„å»ºè·¨å¹³å°æ–‡ä»¶æœç´¢åº”ç”¨ â€” è€ŒéElectron",
        "summary": "OmniFileæ˜¯ä¸€ä¸ªåŸºäºTauriå’ŒRustçš„æ–‡ä»¶æœç´¢åº”ç”¨ï¼Œé€šè¿‡åˆ©ç”¨åŸç”Ÿwebviewså’ŒRustçš„æ–‡ä»¶I/Oæ€§èƒ½ï¼Œå®ç°äº†8MBå®‰è£…ç¨‹åºå’Œ30MBç©ºé—²RAMï¼Œç›¸æ¯”Electronçš„80MB+å’Œ150MB+ã€‚ä½¿ç”¨Tantivyè¿›è¡Œå…¨æ–‡æœç´¢ä¸”å†…å®¹å·²ç´¢å¼•ä½†æœªå­˜å‚¨ï¼Œè¯¥åº”ç”¨ç»Ÿä¸€æœç´¢Google Driveã€Dropboxã€SharePointå’Œæœ¬åœ°æ–‡ä»¶ï¼ŒåŒæ—¶ä¿æŒéšç§ä¼˜å…ˆè®¾è®¡ã€‚è¿™ç§æŠ€æœ¯æ¯”è¾ƒæ¼”ç¤ºäº†Rustå¯¹èµ„æºå—é™çš„æ¡Œé¢åº”ç”¨ç¨‹åºçš„ä¼˜åŠ¿ï¼Œè¿™äº›åº”ç”¨éœ€è¦å¯†é›†çš„æ–‡ä»¶æ“ä½œã€‚"
      },
      "fr": {
        "title": "Construire une Application de Recherche de Fichiers Multiplateforme avec Tauri â€” Pas Electron",
        "summary": "OmniFile, une application de recherche de fichiers basÃ©e sur Tauri et Rust, rÃ©alise un installeur de 8 Mo et une RAM inactive de 30 Mo par rapport aux 80 Mo+ et 150 Mo+ d'Electron en exploitant les webviews natifs et les performances d'E/S de fichiers de Rust. Utilisant Tantivy pour la recherche en texte intÃ©gral avec le contenu indexÃ© mais non stockÃ©, l'application unifie la recherche sur Google Drive, Dropbox, SharePoint et les fichiers locaux tout en maintenant une conception centrÃ©e sur la confidentialitÃ©. Cette comparaison technique dÃ©montre les avantages de Rust pour les applications de bureau contraintes par les ressources nÃ©cessitant des opÃ©rations de fichiers intensives."
      },
      "de": {
        "title": "Erstellen einer plattformÃ¼bergreifenden Dateisuch-App mit Tauri â€” Nicht Electron",
        "summary": "OmniFile, eine auf Tauri und Rust basierende Dateisuchsoftware, erreicht ein 8-MB-Installationsprogramm und 30-MB-Leerlauf-RAM im Vergleich zu Electrons 80MB+ und 150MB+, indem sie native Webviews und Rusts Leistung fÃ¼r Datei-E/A nutzt. Unter Verwendung von Tantivy fÃ¼r die Volltextsuche mit indiziertem aber nicht gespeichertem Inhalt vereinheitlicht die Anwendung die Suche auf Google Drive, Dropbox, SharePoint und lokalen Dateien, wÃ¤hrend sie ein datenschutzorientiertes Design beibehÃ¤lt. Dieser technische Vergleich zeigt Rusts Vorteile fÃ¼r ressourcenbeschrÃ¤nkte Desktopanwendungen, die intensive DateivorgÃ¤nge erfordern."
      },
      "es": {
        "title": "Construir una AplicaciÃ³n de BÃºsqueda de Archivos Multiplataforma con Tauri â€” No Electron",
        "summary": "OmniFile, una aplicaciÃ³n de bÃºsqueda de archivos basada en Tauri y Rust, logra un instalador de 8 MB y 30 MB de RAM inactiva en comparaciÃ³n con 80 MB+ y 150 MB+ de Electron al aprovechar las webviews nativas y el rendimiento de E/S de archivos de Rust. Utilizando Tantivy para bÃºsqueda de texto completo con contenido indexado pero no almacenado, la aplicaciÃ³n unifica la bÃºsqueda en Google Drive, Dropbox, SharePoint y archivos locales mientras mantiene un diseÃ±o centrado en la privacidad. Esta comparaciÃ³n tÃ©cnica demuestra las ventajas de Rust para aplicaciones de escritorio con restricciones de recursos que requieren operaciones intensivas de archivos."
      }
    }
  },
  {
    "title": "CVE-2026-27575: The Zombie Session: Breaking Vikunja's Auth with CVE-2026-27575",
    "slug": "cve-2026-27575-zombie-session-breaking-vikunja-auth",
    "url": "https://dev.to/cverports/cve-2026-27575-the-zombie-session-breaking-vikunjas-auth-with-cve-2026-27575-pij",
    "source": "DEV Community",
    "date": "2026-02-26T05:40:19.000Z",
    "summary": "CVE-2026-27575 is a critical vulnerability (CVSS 9.1) in Vikunja before v2.0.0 allowing single-character passwords and failing to invalidate JWT sessions after password changes, enabling attackers with stolen tokens to maintain permanent access regardless of victim credential resets. The flaw demonstrates the architectural dangers of stateless JWTs without revocation mechanisms or input validation on password changes. This matters because it illustrates how session management failures in authentication systems create persistent account takeover risks in self-hosted platforms.",
    "content": "The Zombie Session: Breaking Vikunja's Auth with CVE-2026-27575\n\n\n\nVulnerability ID: CVE-2026-27575\nCVSS Score: 9.1\nPublished: 2026-02-25\nCVE-2026-27575 represents a catastrophic failure in the authentication lifecycle of Vikunja, a popular self-hosted task management platform. The vulnerability is a two-headed beast: first, it allowed users (and attackers) to set passwords with a single character, bypassing security policies during updates. Second, and far more critical, it failed to invalidate active sessions upon password changes. This means an attacker who steals a session token retains permanent access to the victim's data, even after the victim explicitly resets their credentials to 'lock them out.' It is a classic case of stateless JWTs being deployed without a revocation strategy.\nVikunja versions prior to 2.0.0 allow persistent account takeover. Due to a lack of input validation, passwords could be reset to a single character. Worse, changing a password did not invalidate existing JSON Web Tokens (JWTs). An attacker with a stolen token remains logged in indefinitely, regardless of the victim's remediation attempts. Fix: Upgrade to v2.0.0 immediately.\nCWE IDs: CWE-521 (Weak Password), CWE-613 (Insufficient Session Expiration)\nCVSS Score: 9.1 (Critical)\nAttack Vector: Network (API)\nPrivileges Required: None (for initial access via weak policy logic)\nExploit Status: PoC Available / Trivial\nPatch Date: 2026-02-25\nVikunja < 2.0.0\nVikunja: < 2.0.0 (Fixed in: 2.0.0)\n89c17d3\n\n\nEnforce password limits on update and reset\ntype UserPassword struct {\n- NewPassword string `json:\"new_password\"`\n+ NewPassword string `json:\"new_password\" valid:\"minLength:8\"`\n}\n\n2526853\n\n\nRefactor session management to stateful tokens\n// Logic added to invalidate sessions on password change\n\nEnforce minimum password complexity on all inputs, not just registration.\nImplement stateful session management or token denylists.\nInvalidate all active sessions upon password rotation.\nRemediation Ste",
    "category": "github",
    "translations": {
      "zh": {
        "title": "CVE-2026-27575ï¼šåƒµå°¸ä¼šè¯ï¼šç ´åVikunjaè®¤è¯çš„CVE-2026-27575",
        "summary": "CVE-2026-27575æ˜¯Vikunja v2.0.0ä¹‹å‰çš„ä¸¥é‡æ¼æ´ï¼ˆCVSS 9.1ï¼‰ï¼Œå…è®¸å•å­—ç¬¦å¯†ç ä¸”åœ¨å¯†ç æ›´æ”¹åæœªèƒ½ä½¿JWTä¼šè¯å¤±æ•ˆï¼Œä½¿å¾—æ”»å‡»è€…èƒ½å¤Ÿä½¿ç”¨è¢«ç›—ä»¤ç‰Œä¿æŒæ°¸ä¹…è®¿é—®æƒé™ï¼Œæ— è®ºå—å®³è€…å¦‚ä½•é‡ç½®å‡­è¯ã€‚è¯¥æ¼æ´å±•ç¤ºäº†æ— çŠ¶æ€JWTåœ¨æ²¡æœ‰æ’¤é”€æœºåˆ¶æˆ–å¯†ç å˜æ›´è¾“å…¥éªŒè¯æƒ…å†µä¸‹çš„æ¶æ„é£é™©ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒè¯´æ˜äº†è®¤è¯ç³»ç»Ÿä¸­ä¼šè¯ç®¡ç†å¤±è´¥å¦‚ä½•åœ¨è‡ªæ‰˜ç®¡å¹³å°ä¸­é€ æˆæŒä¹…çš„è´¦æˆ·æ¥ç®¡é£é™©ã€‚"
      },
      "fr": {
        "title": "CVE-2026-27575 : La session zombie : Briser l'authentification de Vikunja avec CVE-2026-27575",
        "summary": "CVE-2026-27575 est une vulnÃ©rabilitÃ© critique (CVSS 9.1) dans Vikunja antÃ©rieur Ã  v2.0.0 permettant des mots de passe d'un seul caractÃ¨re et ne parvenant pas Ã  invalider les sessions JWT aprÃ¨s les changements de mot de passe, permettant aux attaquants disposant de jetons volÃ©s de maintenir un accÃ¨s permanent indÃ©pendamment de la rÃ©initialisation des identifiants des victimes. La faille dÃ©montre les dangers architecturaux des JWT sans Ã©tat sans mÃ©canismes de rÃ©vocation ou validation d'entrÃ©e lors des changements de mot de passe. C'est important car cela illustre comment les dÃ©faillances de gestion de session dans les systÃ¨mes d'authentification crÃ©ent des risques persistants de prise de compte dans les plates-formes autohÃ©bergÃ©es."
      },
      "de": {
        "title": "CVE-2026-27575: Die Zombie-Sitzung: Vikunjas Authentifizierung mit CVE-2026-27575 brechen",
        "summary": "CVE-2026-27575 ist eine kritische SicherheitslÃ¼cke (CVSS 9.1) in Vikunja vor v2.0.0, die Ein-Zeichen-PasswÃ¶rter ermÃ¶glicht und JWT-Sitzungen nach PasswortÃ¤nderungen nicht ungÃ¼ltig macht, wodurch Angreifer mit gestohlenen Token unabhÃ¤ngig von den Anmeldedaten-ZurÃ¼ckstellungen des Opfers permanenten Zugriff behalten kÃ¶nnen. Die Schwachstelle demonstriert die architektonischen Gefahren zustandsloser JWTs ohne Widerrufsmechanismen oder Eingabevalidierung bei PasswortÃ¤nderungen. Dies ist wichtig, da es zeigt, wie Fehler bei der Sitzungsverwaltung in Authentifizierungssystemen persistente KontoÃ¼bernahmevorkehrungen in selbstgehosteten Plattformen schaffen."
      },
      "es": {
        "title": "CVE-2026-27575: La sesiÃ³n zombi: Romper la autenticaciÃ³n de Vikunja con CVE-2026-27575",
        "summary": "CVE-2026-27575 es una vulnerabilidad crÃ­tica (CVSS 9.1) en Vikunja anterior a v2.0.0 que permite contraseÃ±as de un solo carÃ¡cter e incapacidad para invalidar sesiones JWT despuÃ©s de cambios de contraseÃ±a, permitiendo a atacantes con tokens robados mantener acceso permanente independientemente de restablecimientos de credenciales de vÃ­ctimas. La falla demuestra los peligros arquitectÃ³nicos de JWT sin estado sin mecanismos de revocaciÃ³n o validaciÃ³n de entrada en cambios de contraseÃ±a. Esto importa porque ilustra cÃ³mo las fallas de gestiÃ³n de sesiÃ³n en sistemas de autenticaciÃ³n crean riesgos persistentes de apropiaciÃ³n de cuentas en plataformas autohospedadas."
      }
    }
  },
  {
    "title": "ğŸ‡¹ğŸ‡· TÃ¼rkiye devs: Add TC Kimlik NumarasÄ± to the AI identity standard â€” Soulprint open source (30 min PR)",
    "slug": "turkiye-devs-add-tc-kimlik-soulprint-open-source-pr",
    "url": "https://dev.to/manuel_felipeariaspined/turkiye-devs-add-tc-kimlik-numarasi-to-the-ai-identity-standard-soulprint-open-source-30-min-4jli",
    "source": "DEV Community",
    "date": "2026-02-26T05:33:47.000Z",
    "summary": "The post invites Turkish developers to contribute a TC Kimlik NumarasÄ± (Turkish national ID) verifier to the Soulprint open-source identity standard by implementing a 30-minute pull request with validation algorithms. The implementation enables AI agents to verify Turkish identity documents using checksum formulas on 11-digit IDs, expanding identity verification across emerging markets. This matters because crowdsourced identity standard development demonstrates how open-source projects can democratize AI verification capabilities globally.",
    "content": "AI ajanlarÄ± her gÃ¼n kimlik doÄŸrulamasÄ± olmadan kararlar alÄ±yor. Soulprint â€” ZK Proofs, yerel, MIT.\nğŸ‡¹ğŸ‡· TC Kimlik NumarasÄ± Soulprint'te henÃ¼z yok. 30 dakikada PR gÃ¶nderebilirsin.\nhane: ((1+3+5+7+9. basamak)*7 âˆ’ (2+4+6+8. basamak)) mod 10.\nhane: ilk 10 basamak toplamÄ± mod 10.\n\n\n\n\nconst TR: CountryVerifier = {\n  countryCode: \"TR\", countryName: \"Turkey\",\n  documentTypes: [\"tc_kimlik\"],\n  parse(ocrText: string): DocumentResult {\n    const tc = ocrText.match(/(\\d{11})/)?.[1] ?? \"\";\n    return { valid: !!tc, doc_number: tc, country: \"TR\" };\n  },\n  validate(docNumber: string): NumberValidation {\n    if(!/^\\d{11}$/.test(docNumber)||docNumber[0]===\"0\") return {valid:false};\n    const d=docNumber.split(\"\").map(Number);\n    const c10=((d[0]+d[2]+d[4]+d[6]+d[8])*7-(d[1]+d[3]+d[5]+d[7]))%10;\n    const c11=d.slice(0,10).reduce((a,b)=>a+b,0)%10;\n    return { valid: d[9]===c10 && d[10]===c11 };\n  },\n};\nexport default TR;\n\nğŸ’» GitHub Â· Bir PR. Bir Ã¼lke.",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ğŸ‡¹ğŸ‡· åœŸè€³å…¶å¼€å‘è€…ï¼šå°†TC Kimlik NumarasÄ±æ·»åŠ åˆ°AIèº«ä»½æ ‡å‡†â€”â€”Soulprintå¼€æºï¼ˆ30åˆ†é’ŸPRï¼‰",
        "summary": "è¯¥å¸–å­é‚€è¯·åœŸè€³å…¶å¼€å‘è€…é€šè¿‡å®ç°å…·æœ‰éªŒè¯ç®—æ³•çš„30åˆ†é’Ÿæ‹‰å–è¯·æ±‚ï¼Œå‘Soulprintå¼€æºèº«ä»½æ ‡å‡†è´¡çŒ®TC Kimlik NumarasÄ±ï¼ˆåœŸè€³å…¶å›½å®¶IDï¼‰éªŒè¯å™¨ã€‚è¯¥å®ç°ä½¿AIä»£ç†èƒ½å¤Ÿä½¿ç”¨11ä½æ•°å­—IDçš„æ ¡éªŒå’Œå…¬å¼éªŒè¯åœŸè€³å…¶èº«ä»½æ–‡ä»¶ï¼Œæ‰©å±•äº†æ–°å…´å¸‚åœºçš„èº«ä»½éªŒè¯ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºä¼—åŒ…èº«ä»½æ ‡å‡†å¼€å‘æ¼”ç¤ºäº†å¼€æºé¡¹ç›®å¦‚ä½•èƒ½åœ¨å…¨çƒèŒƒå›´å†…æ°‘ä¸»åŒ–AIéªŒè¯èƒ½åŠ›ã€‚"
      },
      "fr": {
        "title": "ğŸ‡¹ğŸ‡· DÃ©veloppeurs turcs : Ajouter TC Kimlik NumarasÄ± Ã  la norme d'identitÃ© IA â€” Soulprint open source (30 min PR)",
        "summary": "Le message invite les dÃ©veloppeurs turcs Ã  contribuer un vÃ©rificateur TC Kimlik NumarasÄ± (ID national turc) Ã  la norme d'identitÃ© open-source Soulprint en implÃ©mentant une demande d'extraction de 30 minutes avec des algorithmes de validation. L'implÃ©mentation permet aux agents IA de vÃ©rifier les documents d'identitÃ© turcs en utilisant des formules de somme de contrÃ´le sur les ID Ã  11 chiffres, Ã©largissant la vÃ©rification d'identitÃ© sur les marchÃ©s Ã©mergents. C'est important car le dÃ©veloppement collaboratif de normes d'identitÃ© dÃ©montre comment les projets open-source peuvent dÃ©mocratiser les capacitÃ©s de vÃ©rification IA Ã  l'Ã©chelle mondiale."
      },
      "de": {
        "title": "ğŸ‡¹ğŸ‡· TÃ¼rkische Entwickler: TC Kimlik NumarasÄ± zur AI-IdentitÃ¤tsnorm hinzufÃ¼gen â€” Soulprint Open Source (30-minÃ¼tiges PR)",
        "summary": "Der Beitrag lÃ¤dt tÃ¼rkische Entwickler ein, einen TC Kimlik NumarasÄ± (tÃ¼rkische nationale ID) Verifizierer zum Soulprint Open-Source-IdentitÃ¤tsstandard beizutragen, indem eine 30-Minuten-Pull-Request mit Validierungsalgorithmen implementiert wird. Die Implementierung ermÃ¶glicht es KI-Agenten, tÃ¼rkische IdentitÃ¤tsdokumente unter Verwendung von PrÃ¼fsummiformeln auf 11-stelligen IDs zu Ã¼berprÃ¼fen und erweitert die IdentitÃ¤tsÃ¼berprÃ¼fung auf SchwellenlÃ¤ndern. Dies ist wichtig, da die Entwicklung von crowdsourced-IdentitÃ¤tsstandards zeigt, wie Open-Source-Projekte KI-VerifizierungsfÃ¤higkeiten weltweit demokratisieren kÃ¶nnen."
      },
      "es": {
        "title": "ğŸ‡¹ğŸ‡· Desarrolladores turcos: Agregar TC Kimlik NumarasÄ± al estÃ¡ndar de identidad de IA â€” Soulprint de cÃ³digo abierto (PR de 30 minutos)",
        "summary": "El mensaje invita a los desarrolladores turcos a contribuir un verificador de TC Kimlik NumarasÄ± (ID nacional turco) al estÃ¡ndar de identidad de cÃ³digo abierto Soulprint implementando una solicitud de extracciÃ³n de 30 minutos con algoritmos de validaciÃ³n. La implementaciÃ³n permite que los agentes de IA verifiquen documentos de identidad turcos utilizando fÃ³rmulas de suma de verificaciÃ³n en ID de 11 dÃ­gitos, expandiendo la verificaciÃ³n de identidad en mercados emergentes. Esto importa porque el desarrollo colaborativo de normas de identidad demuestra cÃ³mo los proyectos de cÃ³digo abierto pueden democratizar las capacidades de verificaciÃ³n de IA a nivel mundial."
      }
    }
  },
  {
    "title": "NABARD Grade A 2025 â€” eligibility, syllabus, and strategy",
    "slug": "nabard-grade-a-2025-eligibility-syllabus-strategy",
    "url": "https://dev.to/sabya_beworld_e066e3758d8/nabard-grade-a-2025-eligibility-syllabus-and-strategy-2n5l",
    "source": "DEV Community",
    "date": "2026-02-26T05:29:21.000Z",
    "summary": "The NABARD Grade A 2025 exam guide specifies eligibility criteria (age 25-35, bachelor's degree, 2+ years rural banking experience) and outlines a three-phase structure covering general English, reasoning, quantitative aptitude, plus specialized topics in agriculture and economics. The comprehensive syllabus overview provides a framework for candidates preparing for India's National Bank for Agriculture and Rural Development assistant manager positions. This matters because it clarifies requirements for accessing rural development career opportunities in India's banking sector.",
    "content": "NABARD Grade A 2025 â€” Unlock Your Dream Job in Rural Banking\n\n\nAre you ready to make a difference in rural India? NABARD Grade A is an exciting opportunity for young professionals like you to join the National Bank for Agriculture and Rural Development (NABARD) as Assistant Managers. In this blog post, we'll guide you through the eligibility criteria, syllabus, and strategy to crack the exam.\nEligibility Criteria: Don't Miss Out\n\n\nBefore diving into the preparation phase, let's ensure you meet the basic requirements:\nAge Limit: 25-35 years (relaxation for reserved categories)\nEducation: Bachelor's degree in any discipline from a recognized university\nWork Experience: Minimum 2 years of experience in rural banking or a related field\nIf you've checked off all these boxes, congratulations! You're eligible to apply. But remember, meeting the eligibility criteria is just the starting point.\nSyllabus: Understand What's at Stake\n\n\nThe NABARD Grade A exam consists of three phases:\n Phase I: Multiple-choice questions (MCQs) in General English, Reasoning Ability, and Quantitative Aptitude\n Phase II: Descriptive tests in English, Agriculture, Economics, and Finance\n Final Interview: Assess your communication skills and knowledge\nAccording to JobSafal.com (https://jobsafal.com), a reliable resource for banking exam aspirants, the syllabus is vast but manageable with focused preparation.\nPhase I Syllabus: Prepare Wisely\n\n\n\nGeneral English:\n\n\nGrammar\nVocabulary\nComprehension\nReasoning Ability:\n\n\nLogical reasoning\nData interpretation\nAnalytical reasoning\nQuantitative Aptitude:\n\n\nNumber systems\nAlgebra\nGeometry\nPhase II Syllabus: Dive into the Details\n\n\n\nEnglish:\n\n\nGrammar\nVocabulary\nComprehension\nAgriculture:\n\n\nCrop management\nSoil science\nAgricultural economics\nEconomics:\n\n\nMicroeconomics\nMacroeconomics\nPublic finance\nFinance:\n\n\nFinancial markets\nBanking and insurance\nAccounting\nStudy Schedule: Stay on Track\n\n\nTo ensure you don't miss out on any topic, create a study schedule wit",
    "category": "github",
    "translations": {
      "zh": {
        "title": "NABARDç­‰çº§A 2025â€”â€”èµ„æ ¼ã€å¤§çº²å’Œç­–ç•¥",
        "summary": "NABARDç­‰çº§A 2025è€ƒè¯•æŒ‡å—æŒ‡å®šäº†èµ„æ ¼æ ‡å‡†ï¼ˆå¹´é¾„25-35å²ã€å­¦å£«å­¦ä½ã€2å¹´ä»¥ä¸Šå†œæ‘é“¶è¡Œç»éªŒï¼‰ï¼Œå¹¶æ¦‚è¿°äº†æ¶µç›–ä¸€èˆ¬è‹±è¯­ã€æ¨ç†ã€å®šé‡èƒ½åŠ›ä»¥åŠå†œä¸šå’Œç»æµå­¦ä¸“ä¸šä¸»é¢˜çš„ä¸‰é˜¶æ®µç»“æ„ã€‚å…¨é¢çš„å¤§çº²æ¦‚è¿°ä¸ºå‡†å¤‡å°åº¦å›½å®¶å†œä¸šå’Œå†œæ‘å‘å±•é“¶è¡ŒåŠ©ç†ç»ç†èŒä½çš„å€™é€‰äººæä¾›äº†æ¡†æ¶ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒæ¾„æ¸…äº†è·å–å°åº¦é“¶è¡Œéƒ¨é—¨å†œæ‘å‘å±•èŒä¸šæœºä¼šçš„è¦æ±‚ã€‚"
      },
      "fr": {
        "title": "NABARD Grade A 2025 â€” admissibilitÃ©, programme et stratÃ©gie",
        "summary": "Le guide d'examen NABARD Grade A 2025 spÃ©cifie les critÃ¨res d'admissibilitÃ© (Ã¢ge 25-35 ans, diplÃ´me d'une licence, 2+ ans d'expÃ©rience dans les banques rurales) et dÃ©crit une structure en trois phases couvrant l'anglais gÃ©nÃ©ral, le raisonnement, l'aptitude quantitative, plus les sujets spÃ©cialisÃ©s en agriculture et en Ã©conomie. L'aperÃ§u complet du programme offre un cadre aux candidats se prÃ©parant pour les postes de gestionnaire adjoint de la Banque nationale pour l'agriculture et le dÃ©veloppement rural de l'Inde. C'est important car cela clarifie les exigences pour accÃ©der aux opportunitÃ©s de carriÃ¨re en dÃ©veloppement rural dans le secteur bancaire indien."
      },
      "de": {
        "title": "NABARD Grade A 2025 â€” Berechtigung, Lehrplan und Strategie",
        "summary": "Der PrÃ¼fungsleitfaden NABARD Grade A 2025 gibt die Zulassungskriterien an (Alter 25-35 Jahre, Bachelorabschluss, 2+ Jahre Erfahrung im lÃ¤ndlichen Bankwesen) und skizziert eine dreiphasige Struktur mit allgemeinem Englisch, Argumentation, quantitativen FÃ¤higkeiten sowie spezialisierten Themen in Landwirtschaft und Wirtschaft. Der umfassende LehrplanÃ¼berblick bietet einen Rahmen fÃ¼r Kandidaten, die sich auf die Positionen des stellvertretenden Managers der indischen Nationalbank fÃ¼r Landwirtschaft und Landentwicklung vorbereiten. Dies ist wichtig, da es die Anforderungen fÃ¼r den Zugang zu KarrieremÃ¶glichkeiten in der lÃ¤ndlichen Entwicklung im indischen Bankensektor verdeutlicht."
      },
      "es": {
        "title": "NABARD Grado A 2025 â€” elegibilidad, plan de estudios y estrategia",
        "summary": "La guÃ­a del examen NABARD Grado A 2025 especifica los criterios de elegibilidad (edad 25-35 aÃ±os, licenciatura, 2+ aÃ±os de experiencia en banca rural) y describe una estructura de tres fases que cubre inglÃ©s general, razonamiento, aptitud cuantitativa, mÃ¡s temas especializados en agricultura y economÃ­a. La descripciÃ³n general completa del plan de estudios proporciona un marco para los candidatos que se preparan para las posiciones de gerente asistente del Banco Nacional para la Agricultura y Desarrollo Rural de India. Esto importa porque aclara los requisitos para acceder a oportunidades de carrera en desarrollo rural en el sector bancario indio."
      }
    }
  },
  {
    "title": "Tech companies shouldn't be bullied into doing surveillance",
    "slug": "tech-companies-shouldnt-be-bullied-into-doing-surveillance",
    "url": "https://www.eff.org/deeplinks/2026/02/tech-companies-shouldnt-be-bullied-doing-surveillance",
    "source": "Hacker News",
    "date": "2026-02-26T00:37:32.000Z",
    "summary": "The EFF argues against government pressure on tech companies to implement surveillance capabilities, warning that coercion threatens user privacy and corporate independence. The article raises concerns about forced compliance and its implications for digital rights.",
    "content": "Article URL: https://www.eff.org/deeplinks/2026/02/tech-companies-shouldnt-be-bullied-doing-surveillance\nComments URL: https://news.ycombinator.com/item?id=47160226\nPoints: 300\n# Comments: 100",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ç§‘æŠ€å…¬å¸ä¸åº”è¯¥è¢«å¼ºåˆ¶è¿›è¡Œç›‘æ§",
        "summary": "ç”µå­å‰æ²¿åŸºé‡‘ä¼šåå¯¹æ”¿åºœå¯¹ç§‘æŠ€å…¬å¸æ–½å‹å®æ–½ç›‘æ§åŠŸèƒ½ï¼Œè­¦å‘Šå¼ºåˆ¶å¨èƒç”¨æˆ·éšç§å’Œä¼ä¸šç‹¬ç«‹æ€§ã€‚è¯¥æ–‡ç« å¯¹å¼ºåˆ¶åˆè§„åŠå…¶å¯¹æ•°å­—æƒåˆ©çš„å½±å“è¡¨ç¤ºå…³åˆ‡ã€‚"
      },
      "fr": {
        "title": "Les entreprises technologiques ne devraient pas Ãªtre forcÃ©es Ã  faire de la surveillance",
        "summary": "L'EFF s'oppose Ã  la pression gouvernementale sur les entreprises technologiques pour mettre en Å“uvre des capacitÃ©s de surveillance, avertissant que la coercition menace la vie privÃ©e des utilisateurs et l'indÃ©pendance des entreprises. L'article soulÃ¨ve des prÃ©occupations concernant la conformitÃ© forcÃ©e et ses implications pour les droits numÃ©riques."
      },
      "de": {
        "title": "Technologieunternehmen sollten nicht zu Ãœberwachung gezwungen werden",
        "summary": "Die EFF spricht sich gegen Druck der Regierung auf Technologieunternehmen aus, um Ãœberwachungsfunktionen zu implementieren, und warnt davor, dass Zwang die BenutzerprivatsphÃ¤re und die UnternehmensunabhÃ¤ngigkeit gefÃ¤hrdet. Der Artikel Ã¤uÃŸert Bedenken Ã¼ber erzwungene Compliance und ihre Auswirkungen auf digitale Rechte."
      },
      "es": {
        "title": "Las empresas tecnolÃ³gicas no deberÃ­an ser obligadas a realizar vigilancia",
        "summary": "La EFF se opone a la presiÃ³n gubernamental sobre las empresas tecnolÃ³gicas para implementar capacidades de vigilancia, advirtiendo que la coerciÃ³n amenaza la privacidad del usuario y la independencia corporativa. El artÃ­culo plantea preocupaciones sobre el cumplimiento forzado y sus implicaciones para los derechos digitales."
      }
    }
  },
  {
    "title": "Next.js ì•±ì„ í•˜ë£¨ë§Œì— 6ê°œêµ­ì–´ë¡œ ë§Œë“  ë°©ë²•",
    "slug": "nextjs-app-six-languages-one-day",
    "url": "https://dev.to/ji_ai/nextjs-aebeul-harumane-6gaegugeoro-mandeun-bangbeob-pi5",
    "source": "DEV Community",
    "date": "2026-02-26T00:05:57.000Z",
    "summary": "This article details implementing internationalization in Next.js 15 using next-intl, covering locale-based routing architecture, translation file management, and region-specific pricing strategies to support rapid deployment across six countries and diverse markets.",
    "content": "ì‚¬ì£¼ ì•±ì„ 6ê°œêµ­ì— ë‚´ë†“ê¸°ë¡œ í–ˆë‹¤. í•œêµ­, ë¯¸êµ­, ì¼ë³¸, ì¤‘êµ­, ë² íŠ¸ë‚¨, ì¸ë„.\nì‚¬ì£¼ê°€ ë™ì•„ì‹œì•„ ë¬¸í™”ê¶Œ ë°–ì—ì„œ ë¨¹íê¹Œ? ëª¨ë¥´ê² ë‹¤. ê·¼ë° íƒ€ë¡œì™€ ì ì„±ìˆ ì´ ì „ì„¸ê³„ì—ì„œ ë¨¹íˆëŠ” ê±¸ ë³´ë©´, \"AIê°€ ë‹¹ì‹ ì˜ ìš´ëª…ì„ ë¶„ì„í•©ë‹ˆë‹¤\"ëŠ” ì–´ë””ì„œë“  í´ë¦­ì„ ë¶€ë¥¼ ê²ƒ ê°™ì•˜ë‹¤.\në¬¸ì œëŠ” í•˜ë“œì½”ë”©ëœ í•œêµ­ì–´ê°€ ëª¨ë“  í˜ì´ì§€ì— ë°•í˜€ ìˆë‹¤ëŠ” ê±°ë‹¤.\nNext.js 15 App Routerì—ì„œ i18n ì˜µì…˜ì€ ëª‡ ê°€ì§€ ìˆë‹¤. next-intlì„ ê³ ë¥¸ ì´ìœ ëŠ” ë‹¨ìˆœí•˜ë‹¤ â€” App Router ë„¤ì´í‹°ë¸Œ ì§€ì›ì´ ê°€ì¥ ê¹”ë”í•˜ë‹¤. [locale] ë™ì  ì„¸ê·¸ë¨¼íŠ¸ì— ë¯¸ë“¤ì›¨ì–´ë¡œ ìë™ ë¦¬ë””ë ‰íŠ¸. Server Componentì—ì„œë„ Client Componentì—ì„œë„ ê°™ì€ useTranslations() í›….\napps/web/\nâ”œâ”€â”€ app/\nâ”‚   â”œâ”€â”€ [locale]/          â† ëª¨ë“  í˜ì´ì§€ê°€ ì—¬ê¸° ì•ˆìœ¼ë¡œ\nâ”‚   â”‚   â”œâ”€â”€ page.tsx\nâ”‚   â”‚   â”œâ”€â”€ result/page.tsx\nâ”‚   â”‚   â””â”€â”€ layout.tsx     â† html lang={locale} ì—¬ê¸°ì„œ\nâ”‚   â””â”€â”€ layout.tsx          â† ë¹ˆ ê»ë°ê¸°\nâ”œâ”€â”€ i18n/\nâ”‚   â”œâ”€â”€ config.ts           â† locales, defaultLocale\nâ”‚   â”œâ”€â”€ routing.ts          â† localePrefix: \"as-needed\"\nâ”‚   â””â”€â”€ navigation.ts       â† i18n Link, useRouter\nâ”œâ”€â”€ messages/\nâ”‚   â”œâ”€â”€ ko.json\nâ”‚   â”œâ”€â”€ en.json\nâ”‚   â”œâ”€â”€ ja.json\nâ”‚   â”œâ”€â”€ zh.json\nâ”‚   â”œâ”€â”€ vi.json\nâ”‚   â””â”€â”€ hi.json\nâ””â”€â”€ middleware.ts            â† Accept-Language ê°ì§€\n\nlocalePrefix: \"as-needed\"ê°€ í•µì‹¬ì´ë‹¤. í•œêµ­ì–´ê°€ ë””í´íŠ¸ë‹ˆê¹Œ /ë¡œ ì ‘ì†í•˜ë©´ í•œêµ­ì–´, /en/ìœ¼ë¡œ ê°€ë©´ ì˜ì–´. í•œêµ­ ì‚¬ìš©ìëŠ” URLì— /ko/ê°€ ì•ˆ ë¶™ëŠ”ë‹¤.\nNext.js App Routerì—ì„œ root layoutì€ ë°˜ë“œì‹œ <html>ê³¼ <body>ë¥¼ ë Œë”ë§í•´ì•¼ í•œë‹¤ê³  ì•Œê³  ìˆì—ˆë‹¤. ê·¸ë˜ì„œ root layoutì—ë„ ë„£ê³ , [locale]/layout.tsxì—ë„ <html lang={locale}>ì„ ë„£ì—ˆë‹¤.\nê²°ê³¼: html ì•ˆì— html. ë¸Œë¼ìš°ì €ëŠ” ì¡°ìš©íˆ ë¬´ì‹œí•˜ì§€ë§Œ ì™„ì „íˆ ì˜ëª»ëœ êµ¬ì¡°ë‹¤.\n// app/layout.tsx â€” ì´ê²Œ ì •ë‹µ\nexport default function RootLayout({ children }) {\n  return children;  // html/body ì—†ì´ ê·¸ëƒ¥ íŒ¨ìŠ¤ìŠ¤ë£¨\n}\n\n// app/[locale]/layout.tsx â€” ì—¬ê¸°ì„œ html/body ê´€ë¦¬\nexport default function LocaleLayout({ children, params }) {\n  return (\n    <html lang={locale}>\n      <body>{children}</body>\n    </html>\n  );\n}\n\nroot layoutì´ ê·¸ëƒ¥ childrenë§Œ ë¦¬í„´í•´ë„ Next.js 15ì—ì„œëŠ” ì—ëŸ¬ê°€ ì•ˆ ë‚œë‹¤. [locale] layoutì´ html/bodyë¥¼ ì œê³µí•˜ë‹ˆê¹Œ.\nê°™ì€ ì„œë¹„ìŠ¤ë¼ë„ ì¸ë„ì—ì„œ $9.90ì„ ë°›ìœ¼ë©´ ì•„ë¬´ë„ ì•ˆ ì‚°ë‹¤. ê° ë‚˜ë¼ êµ¬ë§¤ë ¥ì— ë§ì¶° ê°€ê²©ì„ ì¡ì•˜ë‹¤.\n// ko.json\n\"price\": \"â‚©12,900\"\n\n// en.json\n\"price\": \"$9.90\"\n\n// ja.json\n\"price\": \"Â¥1,490\"\n\n// zh.json\n\"price\": \"Â¥68\"\n\n// vi.json\n\"price\": \"199.000â‚«\"\n\n// hi.json\n\"price\": \"â‚¹799\"\n\në²ˆì—­ íŒŒì¼ì— ê°€ê²©ì„ í•˜ë“œì½”ë”©í•œ ê±°ë‹¤. ë‚˜ì¤‘ì— ê²°ì œ ì—°ë™í•˜ë©´ ì„œë²„ì—ì„œ ë‚´ë ¤ì£¼ê² ì§€ë§Œ, MVP ë‹¨ê³„ì—ì„œëŠ” ì´ê²Œ ê°€ì¥ ë¹ ë¥´ë‹¤. placeholder ì´ë¦„ë„ ë¡œì»¬ë¼ì´ì¦ˆí–ˆë‹¤ â€” í•œêµ­ì€ \"í™ê¸¸ë™\", ì¼ë³¸ì€ \"å±±ç”°å¤ªéƒ\", ì¸ë„ëŠ” \"à¤°à¤¾à¤¹à¥à¤² à¤¶à¤°à¥à¤®à¤¾\".\napp/page.tsxë¥¼ app/[locale]/page.tsxë¡œ ì˜®",
    "category": "github",
    "translations": {
      "zh": {
        "title": "å¦‚ä½•åœ¨ä¸€å¤©å†…ç”¨6ç§è¯­è¨€æ„å»ºNext.jsåº”ç”¨",
        "summary": "æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†ä½¿ç”¨next-intlåœ¨Next.js 15ä¸­å®ç°å›½é™…åŒ–çš„æ–¹æ³•ï¼Œæ¶µç›–åŸºäºåœ°åŒºçš„è·¯ç”±æ¶æ„ã€ç¿»è¯‘æ–‡ä»¶ç®¡ç†å’Œåœ°åŸŸç‰¹å®šçš„å®šä»·ç­–ç•¥ï¼Œä»¥æ”¯æŒåœ¨å…­ä¸ªå›½å®¶å’Œä¸åŒå¸‚åœºä¸­çš„å¿«é€Ÿéƒ¨ç½²ã€‚"
      },
      "fr": {
        "title": "Comment crÃ©er une application Next.js en six langues en une journÃ©e",
        "summary": "Cet article dÃ©taille l'implÃ©mentation de l'internationalisation dans Next.js 15 en utilisant next-intl, couvrant l'architecture de routage basÃ©e sur les paramÃ¨tres rÃ©gionaux, la gestion des fichiers de traduction et les stratÃ©gies de tarification spÃ©cifiques aux rÃ©gions pour soutenir le dÃ©ploiement rapide sur six pays et diffÃ©rents marchÃ©s."
      },
      "de": {
        "title": "Wie man eine Next.js-App in sechs Sprachen an einem Tag erstellt",
        "summary": "Dieser Artikel beschreibt die Implementierung der Internationalisierung in Next.js 15 mit next-intl und behandelt die Routing-Architektur nach Gebietsschema, die Verwaltung von Ãœbersetzungsdateien und regionsspezifische Preisstrategien, um die schnelle Bereitstellung in sechs LÃ¤ndern und verschiedenen MÃ¤rkten zu unterstÃ¼tzen."
      },
      "es": {
        "title": "CÃ³mo crear una aplicaciÃ³n Next.js en seis idiomas en un dÃ­a",
        "summary": "Este artÃ­culo detalla la implementaciÃ³n de la internacionalizaciÃ³n en Next.js 15 usando next-intl, cubriendo la arquitectura de enrutamiento basada en configuraciÃ³n regional, la gestiÃ³n de archivos de traducciÃ³n y estrategias de precios especÃ­ficas por regiÃ³n para respaldar la implementaciÃ³n rÃ¡pida en seis paÃ­ses y diversos mercados."
      }
    }
  },
  {
    "title": "Claude í•˜ë‚˜ë¡œ 1ì¸ SaaS ì „ì²´ë¥¼ ì„¤ê³„í•œ ê¸°ë¡",
    "slug": "claude-designed-saas-entire-stack-one-session",
    "url": "https://dev.to/ji_ai/claude-hanaro-1in-saas-jeoncereul-seolgyehan-girog-44h5",
    "source": "DEV Community",
    "date": "2026-02-26T00:01:21.000Z",
    "summary": "This article documents using Claude AI to comprehensively design a Korean fortune-telling SaaS, demonstrating how iterative conversations can simulate expert panels, identify business strategy gaps, and generate production-ready plansâ€”replacing what would cost thousands in consulting.",
    "content": "\"ë¡œê·¸ì¸ ë²½ë¶€í„° ì œê±°í•˜ì„¸ìš”. magic link ì¸ì¦ì´ ìµœëŒ€ ì´íƒˆ ì›ì¸ì…ë‹ˆë‹¤.\"\nì´ ë§ì„ í•œ ê±´ ì‚¬ëŒì´ ì•„ë‹ˆë‹¤. Claudeê°€ ì‹œë®¬ë ˆì´ì…˜í•œ \"PM ì—­í• ì˜ ê°€ìƒ ì „ë¬¸ê°€\"ë‹¤.\ní•˜ë£¨ ë™ì•ˆ Claudeë‘ 9ê°œ ì„¸ì…˜ì„ í–ˆë‹¤. ë‚˜ì˜¨ ì‚°ì¶œë¬¼ì´ 20ê°œë‹¤. ì‚¬ì—… ì „ëµì„œ, ëœë”© ë””ìì¸, ì „ë¬¸ê°€ íŒ¨ë„ íšŒì˜ë¡, LLM ë¹„ìš© ë¶„ì„ì„œ, ê¸€ë¡œë²Œ í™•ì¥ ì „ëµì„œ, Claude Code ì‹¤í–‰ìš© íƒœìŠ¤í¬ íŒŒì¼ë“¤.\nì´ê±¸ ì»¨ì„¤íŒ… íšŒì‚¬ì— ë§¡ê²¼ìœ¼ë©´ ëª‡ ì£¼ì— ëª‡ ì²œë§Œì›ì´ë‹¤.\ní˜¼ì, í•˜ë£¨, $0.\nì²˜ìŒë¶€í„° ì „ë¶€ë¥¼ ì‹œí‚¨ ê²Œ ì•„ë‹ˆë‹¤. ëŒ€í™”ê°€ ê¹Šì–´ì§€ë©´ì„œ êµ¬ì²´í™”ëë‹¤.\n1í„´: \"ì‚¬ì£¼ ì•± ì‚¬ì—…ì„± ì–´ë•Œ?\" (ì¶”ìƒì )\n2í„´: \"ë¬´ë£Œ/ìœ ë£Œ ë‚˜ëˆ ì„œ ìˆ˜ìµ ëª¨ë¸ ì§œì¤˜\" (êµ¬ì²´ì )\n3í„´: \"ë¬´ë£Œ í‹°ì–´ API ë¹„ìš©ì„ í† í° ë‹¨ìœ„ë¡œ ê³„ì‚°í•´ì¤˜\" (ë§¤ìš° êµ¬ì²´ì )\n4í„´: \"Prompt Caching ì ìš© ì‹œ ì‹œë‚˜ë¦¬ì˜¤ A/B/C ë¹„êµí•´ì¤˜\" (ì´ˆêµ¬ì²´ì )\n\ní•œ ë²ˆì— \"ì‚¬ì—…ê³„íšì„œ ì¨ì¤˜\"ë¼ê³  í•˜ë©´ ì¼ë°˜ì ì¸ ë‹µë³€ì´ ë‚˜ì˜¨ë‹¤.\nì ì§„ì ìœ¼ë¡œ ê¹Šì´ë¥¼ ì˜¬ë¦¬ë©´, ê° ë‹¨ê³„ì—ì„œ AIê°€ ì´ì „ ë§¥ë½ì„ ë‹¤ ê°–ê³  ìˆìœ¼ë‹ˆê¹Œ ê²°ê³¼ë¬¼ì´ í›¨ì”¬ ì •ë°€í•´ì§„ë‹¤.\nê°€ì¥ íš¨ê³¼ê°€ ì¢‹ì•˜ë˜ ê±´ \"ì „ë¬¸ê°€ íŒ¨ë„ ì‹œë®¬ë ˆì´ì…˜\"ì´ë‹¤.\në‚˜: \"ì „ë¬¸ê°€ 6ëª…ì„ êµ¬ì„±í•´ì¤˜.\n    PM 1ëª…, ì‚¬ì—…ê°œë°œ 1ëª…, ë¡œì»¬ë¼ì´ì œì´ì…˜ 1ëª…,\n    ë¯¸êµ­ ì‹œì¥ ì „ë¬¸ê°€ 1ëª…, í’€ìŠ¤íƒ ê°œë°œì 1ëª…, UI/UX ë””ìì´ë„ˆ 1ëª….\n    ê°ì ì´ë¦„ì´ë‘ ê´€ì ì„ ì •í•´ì¤˜.\n    í˜„ì¬ ìƒíƒœ(STATUS.md)ë¥¼ ë¦¬ë·°í•˜ê³  íšŒì˜í•´ì¤˜.\"\n\nClaudeê°€ 6ëª…ì˜ ìºë¦­í„°ë¥¼ ë§Œë“¤ì–´ì„œ ê°ìì˜ ê´€ì ìœ¼ë¡œ í† ë¡ í•œë‹¤. PMì´ ìš°ì„ ìˆœìœ„ë¥¼ ì§œê³ , ì‚¬ì—… ë‹´ë‹¹ì´ ì‹œì¥ì„±ì„ ë”°ì§€ê³ , ê°œë°œìê°€ ê¸°ìˆ  ë‚œì´ë„ë¥¼ ì§šê³ , ë””ìì´ë„ˆê°€ UX ì´ìŠˆë¥¼ ì œê¸°í•œë‹¤.\ní˜¼ì ì‚¬ì—…í•˜ë©´ \"ë‚´ ê´€ì \"ë°–ì— ì—†ë‹¤. ì´ê±¸ ì“°ë©´ 6ê°œ ê´€ì ì´ ë™ì‹œì— ë‚˜ì˜¨ë‹¤.\në¬¼ë¡  ì§„ì§œ ì „ë¬¸ê°€ 6ëª…ê³¼ëŠ” ë‹¤ë¥´ë‹¤. í•˜ì§€ë§Œ 1ì¸ ê°œë°œìê°€ ë†“ì¹˜ê¸° ì‰¬ìš´ ì‚¬ê°ì§€ëŒ€ë¥¼ ì¡ì•„ë‚´ëŠ” ë°ëŠ” ì¶©ë¶„í•˜ë‹¤.\në¡œê·¸ì¸ ë²½ ì œê±°ê°€ ì²« ë²ˆì§¸ì˜€ë‹¤. magic link ì¸ì¦ì´ ìµœëŒ€ ì´íƒˆ ì›ì¸ì´ë¼ëŠ” ê±¸ PMì´ ì§€ì í–ˆë‹¤. ë¬´ë£Œ ë¶„ì„ì€ ì™„ì „ ë¹„ë¡œê·¸ì¸ìœ¼ë¡œ ì „í™˜.\në¬´ë£Œ í‹°ì–´ ë¹„ìš©ë„ 94% ì˜ë¼ëƒˆë‹¤. LLM í’€ í˜¸ì¶œ ëŒ€ì‹  ì•Œê³ ë¦¬ì¦˜ í¬ë§·íŒ…ì— AI 1ì¤„ ìš”ì•½ë§Œ ë¶™ì´ëŠ” êµ¬ì¡°ë¡œ ê±´ë‹¹ $0.085 â†’ $0.005.\nì‚¬ì—…ìë“±ë¡ì€ ìœ ì € ë°˜ì‘ ê¸°ë‹¤ë¦¬ì§€ ë§ê³  ê²°ì œ ì—°ë™ì„ ìœ„í•´ ì¦‰ì‹œ ì‹œì‘í•˜ê¸°ë¡œ í–ˆë‹¤.\nGA4ì™€ Rate Limitingì€ í•„ìˆ˜ë¼ëŠ” ë° ì „ì› ë™ì˜í–ˆë‹¤. ë¶„ì„ ì—†ì´ ê°œì„ ì€ ëˆˆê°ê³  ìš´ì „ì´ê³ , ë³´í˜¸ ì—†ëŠ” ë¬´ë£Œ APIëŠ” ë¹„ìš© í­íƒ„ì´ë‹¤.\nì¹´ì¹´ì˜¤ ê³µìœ ì™€ OG ì´ë¯¸ì§€ë¥¼ ìš°ì„  êµ¬í˜„í•˜ê¸°ë¡œ í–ˆë‹¤. ì‚¬ì£¼ ì„œë¹„ìŠ¤ì˜ ìœ ì¼í•œ ë¬´ë£Œ ë§ˆì¼€íŒ…ì€ ë°”ì´ëŸ´ì´ë‹¤.\nê¸€ë¡œë²Œ í™•ì¥ì€ ë³´ë¥˜. í•œêµ­ì—ì„œ ìœ ë£Œ ì „í™˜ìœ¨ 3% ë„˜ê¸° ì „ê¹Œì§€ ì˜ë¬¸í™”ì— ë¦¬ì†ŒìŠ¤ ì•ˆ ì“´ë‹¤.\nì´ ê²°ì •ë“¤ì„ í˜¼ì ì•‰ì•„ì„œ ë‹¤ ìƒê°í•´ëƒˆì„ê¹Œ? ì†”ì§íˆ, ì‚¬ì—…ìë“±ë¡ ì¦‰ì‹œ ì§„í–‰ì´ë‘ Rate Limitingì€ ë‚˜ì¤‘ì—ì•¼ ìƒê°í–ˆì„ ê±°ë‹¤.\në‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ë„ ê·¸ëŒ€ë¡œ ì ìš©í•  ìˆ˜ ìˆë‹¤. ë¹„ì „ ê³µìœ ë¡œ í° ê·¸ë¦¼ì„ ê·¸ë¦¬ê³ , ì „ëµ ìˆ˜ë¦½ì—ì„œ ì„ íƒì§€ë¥¼ ê²°ì •í•˜ê³ , ì „ë¬¸ê°€ íŒ¨ë„ë¡œ ê²€ì¦í•˜ê³ , ë””ìì¸ì„ ì‹¤ë¬¼ HTMLë¡œ ë½‘ê³ , ë¹„ìš©ì„ í† í° ë‹¨ìœ„ë¡œ ë¶„ì„í•˜ê³ , í™•ì¥ ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í•˜ê³ , ë§ˆì§€ë§‰ì— Claude Codeì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ íƒœìŠ¤í¬ íŒŒì¼ë¡œ ë³€í™˜í•œë‹¤.\nAIë¥¼ ë„êµ¬ë¡œ ì“°ëŠ” ê²ƒê³¼ AIì™€ ì‚¬ê³ í•˜ëŠ” ê²ƒì€ ë‹¤ë¥´ë‹¤.\në„êµ¬ë¡œ ì“°ë©´ \"ì½”ë“œ ì¨ì¤˜.\" ì‚¬ê³ í•˜ë©´ \"ì´ êµ¬ì¡°ê°€ ë§ì•„? ë¹ ì§„ ê±° ì—†ì–´? ë‚´ê°€ ë†“ì¹˜ëŠ” ê´€ì ì´ ë­ì•¼?\"\ní›„ìê°€ 1ì¸ ê°œë°œìí•œí…ŒëŠ” í›¨ì”¬ ê°€ì¹˜ê°€ í¬ë‹¤.\ní•œ ê°€ì§€ ì†”ì§í•œ ê³ ë°±. ì´ íŒ¨ë„ì—ì„œ ë‚˜ì˜¨ ìˆ«ìë“¤ â€” \"30% ì „í™˜ìœ¨\", \"â‚¹99ê°€ ìµœì ê°€\" ê°™ì€ ê²ƒ â€” ì—ëŠ” ê·¼ê±°ê°€ ì—†ë‹¤. Claudeê°€ ë§Œë“  ê°€ì„¤ì´ì§€, ë°ì´í„°ì—ì„œ ë‚˜ì˜¨ ê²°ë¡ ì´ ì•„ë‹ˆë‹¤.\nê°€ì„¤ì€ ê°€ì„¤ë¡œë§Œ ì·¨ê¸‰í•˜ê³ , ê²€ì¦ì€ ëŸ°ì¹­ í›„ ì‹¤ì œ ë°ì´í„°ë¡œ í•œë‹¤.\n\"AIì™€ ì‚¬ê³ í•˜ë©´ 6ê°œ ê´€ì ì´ ë™ì‹œì— ë‚˜ì˜¨ë‹¤. ì§„ì§œ ì „ë¬¸ê°€ 6ëª…ì€ ì•„ë‹ˆì§€ë§Œ, í˜¼ìë³´ë‹¤ëŠ” í›¨ì”¬ ë‚«ë‹¤.\"",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ä»…ç”¨Claudeè®¾è®¡æ•´ä¸ªç‹¬ç«‹SaaSçš„è®°å½•",
        "summary": "æœ¬æ–‡è®°å½•äº†ä½¿ç”¨Claude AIå…¨é¢è®¾è®¡ä¸€ä¸ªéŸ©å›½ç®—å‘½SaaSçš„è¿‡ç¨‹ï¼Œå±•ç¤ºäº†è¿­ä»£å¯¹è¯å¦‚ä½•èƒ½å¤Ÿæ¨¡æ‹Ÿä¸“å®¶å°ç»„ã€è¯†åˆ«å•†ä¸šæˆ˜ç•¥æ¼æ´å’Œç”Ÿæˆç”Ÿäº§å°±ç»ªçš„è®¡åˆ’â€”â€”æ›¿ä»£èŠ±è´¹æ•°åƒå…ƒå’¨è¯¢çš„æ–¹æ¡ˆã€‚"
      },
      "fr": {
        "title": "Enregistrement de la conception d'une SaaS entiÃ¨re en solo avec Claude",
        "summary": "Cet article documente l'utilisation de Claude AI pour concevoir de maniÃ¨re complÃ¨te une SaaS corÃ©enne de prÃ©diction de fortune, dÃ©montrant comment les conversations itÃ©ratives peuvent simuler des panels d'experts, identifier les lacunes des stratÃ©gies commerciales et gÃ©nÃ©rer des plans prÃªts pour la productionâ€”remplaÃ§ant ce qui coÃ»terait des milliers en consulting."
      },
      "de": {
        "title": "Aufzeichnung der Gestaltung eines gesamten Solo-SaaS mit Claude",
        "summary": "Dieser Artikel dokumentiert die Verwendung von Claude AI, um ein koreanisches Wahrsage-SaaS umfassend zu gestalten, und zeigt, wie iterative GesprÃ¤che Expert-Panels simulieren, LÃ¼cken in der GeschÃ¤ftsstrategie identifizieren und produktionsbereite PlÃ¤ne erstellen kÃ¶nnenâ€”was Tausende an Beratungskosten ersetzt."
      },
      "es": {
        "title": "Registro del diseÃ±o de un SaaS completo en solitario con Claude",
        "summary": "Este artÃ­culo documenta el uso de Claude AI para diseÃ±ar de manera integral un SaaS coreano de adivinaciÃ³n de fortuna, demostrando cÃ³mo las conversaciones iterativas pueden simular paneles de expertos, identificar brechas en la estrategia comercial y generar planes listos para producciÃ³nâ€”reemplazando lo que costarÃ­a miles en consultorÃ­a."
      }
    }
  },
  {
    "title": "Understanding IP Management in Oracle Cloud Infrastructure (OCI)",
    "slug": "oracle-cloud-infrastructure-ip-management-guide",
    "url": "https://dev.to/hiltonj/understanding-ip-management-in-oracle-cloud-infrastructure-oci-1ili",
    "source": "DEV Community",
    "date": "2026-02-26T00:01:10.000Z",
    "summary": "A comprehensive guide to OCI's IP address management system covering private IPs for internal communication, public IPs for internet access, and advanced features like reserved IPs and bring-your-own-IP options, essential for building secure cloud infrastructure.",
    "content": "Navigating the complexities of cloud networking is crucial for building robust and scalable applications. In Oracle Cloud Infrastructure (OCI), effective IP address management forms the backbone of your network architecture. This guide will demystify OCI's IP address categories, explore their use cases, and introduce advanced concepts like Reserved Public IPs, Bring Your Own IP (BYOIP), and Public IP Pools. \nOCI categorizes IP addresses into two primary types, each serving distinct communication needs. \nThese are used for internal communication within your OCI network and with connected on-premises environments. \nInternal Communication: Instances within the same Virtual Cloud Network (VCN) communicate seamlessly using private IPs.\nVCN Peering: Connecting multiple VCNs, whether in the same or different regions, relies on private IP routing.\nOn-premises Connectivity: Secure connections to your data centers via the Dynamic Routing Gateway (DRG).\nInstance Allocation: Each instance receives at least one primary private IP.\nVNIC Capacity: Every Virtual Network Interface Card (VNIC) includes one primary private IP address and supports up to 32 secondary private IP addresses, totaling 33 private IPs per VNIC.\nThese are designed for internet accessibility, allowing your resources to communicate with the outside world. \nInternet Reachability: Public IPs are reachable from the internet, assigned to a private IP object on your OCI resource. \nPrerequisites: For a public IP to function, your VCN requires an Internet Gateway, and the associated public subnet must have correctly configured Route Tables and Security Lists. \nFlexibility: Resources can be assigned multiple public IPs across single or multiple VNICs. \nOCI offers two types of public IP addresses to cater to different operational requirements.\n\nReserved Public IP Addresses in Detail\nCreation: You create them individually. \nLimits: Up to 50 Reserved Public IPs are allowed per region. \nAssignment: Assigned to resources aft",
    "category": "github",
    "translations": {
      "zh": {
        "title": "äº†è§£Oracleäº‘åŸºç¡€è®¾æ–½(OCI)ä¸­çš„IPç®¡ç†",
        "summary": "è¿™æ˜¯ä¸€ä»½å…¨é¢çš„OCI IPåœ°å€ç®¡ç†ç³»ç»ŸæŒ‡å—ï¼Œæ¶µç›–ç”¨äºå†…éƒ¨é€šä¿¡çš„ç§æœ‰IPã€ç”¨äºäº’è”ç½‘è®¿é—®çš„å…¬å…±IPä»¥åŠé«˜çº§åŠŸèƒ½ï¼ˆå¦‚ä¿ç•™IPå’Œè‡ªå¸¦IPé€‰é¡¹ï¼‰ï¼Œè¿™äº›å¯¹äºæ„å»ºå®‰å…¨çš„äº‘åŸºç¡€è®¾æ–½è‡³å…³é‡è¦ã€‚"
      },
      "fr": {
        "title": "Comprendre la gestion des adresses IP dans l'infrastructure cloud Oracle (OCI)",
        "summary": "Un guide complet du systÃ¨me de gestion des adresses IP d'OCI couvrant les adresses IP privÃ©es pour la communication interne, les adresses IP publiques pour l'accÃ¨s Internet et les fonctionnalitÃ©s avancÃ©es comme les adresses IP rÃ©servÃ©es et les options bring-your-own-IP, essentielles pour construire une infrastructure cloud sÃ©curisÃ©e."
      },
      "de": {
        "title": "VerstÃ¤ndnis der IP-Verwaltung in der Oracle-Cloud-Infrastruktur (OCI)",
        "summary": "Ein umfassender Leitfaden zum IP-Adressenmanagementsystem von OCI, der private IP-Adressen fÃ¼r interne Kommunikation, Ã¶ffentliche IP-Adressen fÃ¼r Internetzugriff und erweiterte Funktionen wie reservierte IP-Adressen und Bring-Your-Own-IP-Optionen abdeckt, die fÃ¼r den Aufbau einer sicheren Cloud-Infrastruktur unerlÃ¤sslich sind."
      },
      "es": {
        "title": "Entender la gestiÃ³n de direcciones IP en la infraestructura en la nube de Oracle (OCI)",
        "summary": "Una guÃ­a completa del sistema de gestiÃ³n de direcciones IP de OCI que cubre direcciones IP privadas para comunicaciÃ³n interna, direcciones IP pÃºblicas para acceso a Internet y funciones avanzadas como direcciones IP reservadas y opciones bring-your-own-IP, esenciales para construir una infraestructura en la nube segura."
      }
    }
  },
  {
    "title": "Why You Shouldn't Let AI Do Your Fortune Telling â€” And How to Do It Right",
    "slug": "ai-fortune-telling-deterministic-algorithm-interpretation",
    "url": "https://dev.to/ji_ai/why-you-shouldnt-let-ai-do-your-fortune-telling-and-how-to-do-it-right-2h3l",
    "source": "DEV Community",
    "date": "2026-02-26T00:00:40.000Z",
    "summary": "This article demonstrates why LLMs cannot reliably perform precise astronomical calendar calculations for fortune-telling and advocates combining deterministic algorithms for computation with AI for human-readable interpretation of results.",
    "content": "The first lesson I learned building a saju (Korean four-pillar fortune telling) app: it's not about what you ask AI to do â€” it's about what you don't.\nRevenue is still $0. This isn't a success story â€” it's a debugging diary.\n\"Tell me the fortune for someone born March 15, 1990.\"\nI threw this straight at an LLM. The response looked great. Smooth sentences, Five Elements this, Wood-Fire-Earth-Metal-Water that.\nBut there was a problem. The base calculations were wrong.\nMe: \"Analyze the Four Pillars for March 15, 1990, 6 AM\"\nClaude: \"The year pillar is Geng-Wu, month is Ji-Mao...\"\nMe: \"...Ji-Mao is wrong.\"\n\nLLMs can't do manseryeok (traditional Korean astronomical calendar) calculations. More precisely, they appear to get it right probabilistically, but they don't actually compute anything.\nThe Heavenly Stems and Earthly Branches follow a 60-cycle system. Month pillars shift at solar term boundaries. Hour pillars depend on the day's Heavenly Stem. This isn't reasoning â€” it's arithmetic.\nWhen you ask a language model to do arithmetic, it gets things wrong.\nAnd when the base stems are wrong, everything downstream is garbage. Wrong Five Elements. Wrong Ten Gods. Wrong structure analysis.\nA beautifully written paragraph with incorrect data isn't fortune analysis â€” it's fiction.\nOnce I realized this, I rebuilt the whole thing.\n[Birth date + time] â†’ [Calendar Engine] â†’ [Accurate JSON] â†’ [LLM] â†’ [Interpretation]\n\nI built the calendar engine in code. It's based on the lunar-typescript library, with solar term correction, leap month handling, and midnight boundary logic â€” all deterministic algorithms.\nThe output is JSON. Heavenly Stems, Earthly Branches, Five Element distribution, Ten Gods relationships, structure type, favorable elements. All precise.\nThe LLM gets this JSON. \"Read this data and interpret it.\" That's the entire prompt strategy.\nCode handles the calendar calculations â€” the part that must be 100% accurate. AI handles turning that data into readable, insightful lan",
    "category": "github",
    "translations": {
      "zh": {
        "title": "ä¸ºä»€ä¹ˆä¸åº”è¯¥è®©AIåšå åœâ€”â€”ä»¥åŠå¦‚ä½•æ­£ç¡®åœ°åš",
        "summary": "æœ¬æ–‡æ¼”ç¤ºäº†ä¸ºä»€ä¹ˆå¤§è¯­è¨€æ¨¡å‹æ— æ³•å¯é åœ°æ‰§è¡Œç”¨äºå åœçš„ç²¾ç¡®å¤©æ–‡æ—¥å†è®¡ç®—ï¼Œå¹¶å€¡å¯¼å°†ç¡®å®šæ€§ç®—æ³•ç”¨äºè®¡ç®—ï¼Œå°†AIç”¨äºäººç±»å¯è¯»çš„ç»“æœè§£é‡Šã€‚"
      },
      "fr": {
        "title": "Pourquoi vous ne devriez pas laisser l'IA faire votre voyance â€” Et comment le faire correctement",
        "summary": "Cet article dÃ©montre pourquoi les LLM ne peuvent pas effectuer de maniÃ¨re fiable des calculs prÃ©cis de calendrier astronomique pour la voyance et prÃ©conise de combiner des algorithmes dÃ©terministes pour le calcul avec l'IA pour l'interprÃ©tation lisible par l'homme des rÃ©sultats."
      },
      "de": {
        "title": "Warum Sie KI nicht fÃ¼r Ihre Wahrsagung einsetzen sollten â€” Und wie man es richtig macht",
        "summary": "Dieser Artikel zeigt, warum LLMs keine zuverlÃ¤ssigen astronomischen Kalenderberechnungen fÃ¼r Wahrsagungen durchfÃ¼hren kÃ¶nnen, und befÃ¼rwortet die Kombination deterministischer Algorithmen fÃ¼r Berechnungen mit KI fÃ¼r menschenlesbare Interpretationen der Ergebnisse."
      },
      "es": {
        "title": "Por quÃ© no deberÃ­as dejar que la IA haga tu lectura del futuro â€” Y cÃ³mo hacerlo correctamente",
        "summary": "Este artÃ­culo demuestra por quÃ© los LLM no pueden realizar de manera confiable cÃ¡lculos precisos de calendarios astronÃ³micos para la lectura del futuro e aboga por combinar algoritmos deterministas para cÃ¡lculos con IA para la interpretaciÃ³n legible por humanos de los resultados."
      }
    }
  }
]