[
  {
    "title": "How to Use React Query with React Router Loaders (Pre-fetch & Cache Data)",
    "slug": "how-to-use-react-query-with-react-router-loaders-pre-fetch-cache",
    "url": "https://dev.to/edriso/how-to-use-react-query-with-react-router-loaders-pre-fetch-cache-data-kag",
    "source": "DEV Community",
    "date": "2026-02-26T12:11:05.000Z",
    "summary": "Combining React Query with React Router loaders enables data pre-fetching before components mount, eliminating loading spinners. Using queryClient.ensureQueryData() checks the cache first and fetches fresh data if needed, ensuring the page renders immediately when mounted.",
    "content": "The Problem\n\n\nWhen you navigate to a page, there's usually a delay while data is being fetched. The user sees a loading spinner, and the content pops in after the request finishes. Not great.\nWhat if the data was already there when the page loads?\nThat's exactly what combining React Query with React Router loaders gives you.\nLoader runs before the component mounts (React Router calls it on navigation).\nInside the loader, we ask React Query: \"Do you already have this data cached?\"\n\n\n\nYes → Use it instantly. No network request.\nNo → Fetch it now, wait for it, then cache it.\nWhen the component finally mounts, it calls useQuery with the same query. Since the data is already cached, it renders immediately — no loading state.\nThe key method is queryClient.ensureQueryData(queryOptions). Think of it as: \"Make sure this data exists — get it from cache or fetch it.\"\nLet's build a page that shows Pokémon details. When you navigate to /pokemon/pikachu, the data is already loaded.\n// pages/Pokemon.jsx\nimport { useQuery } from '@tanstack/react-query';\nimport { useLoaderData } from 'react-router-dom';\nimport axios from 'axios';\n\n// A function that returns the query config (key + fetch function).\n// We reuse this in BOTH the loader and the component.\nconst pokemonQuery = (name) => {\n  return {\n    queryKey: ['pokemon', name],\n    queryFn: async () => {\n      const response = await axios.get(\n        `https://pokeapi.co/api/v2/pokemon/${name}`\n      );\n      return response.data;\n    },\n  };\n};\n\n// The loader receives queryClient from the router setup (see step 4).\n// It runs BEFORE the component mounts.\nexport const loader = (queryClient) => {\n  return async ({ params }) => {\n    const { name } = params;\n\n    // ensureQueryData checks the cache first:\n    //   - cached? → returns it instantly\n    //   - not cached? → fetches, caches, and returns it\n    await queryClient.ensureQueryData(pokemonQuery(name));\n\n    // We only return the param — the actual data lives in React Query's ca",
    "category": "github",
    "translations": {
      "zh": {
        "title": "如何将React Query与React Router加载程序一起使用（预获取和缓存数据）",
        "summary": "将React Query与React Router加载程序结合使用可以在组件挂载前预获取数据，消除加载旋转器。使用queryClient.ensureQueryData()首先检查缓存，如果需要则获取新鲜数据，确保页面在挂载时立即呈现。"
      },
      "fr": {
        "title": "Comment utiliser React Query avec les loaders de React Router (pré-récupération et mise en cache des données)",
        "summary": "La combinaison de React Query avec les loaders de React Router permet la pré-récupération des données avant que les composants ne se montent, éliminant les spinners de chargement. L'utilisation de queryClient.ensureQueryData() vérifie d'abord le cache et récupère les données fraîches si nécessaire, garantissant que la page s'affiche immédiatement lors du montage."
      },
      "de": {
        "title": "Verwendung von React Query mit React Router-Loadern (Vorab-Abrufen und Zwischenspeicherung von Daten)",
        "summary": "Das Kombinieren von React Query mit React Router-Loadern ermöglicht das Vorab-Abrufen von Daten, bevor Komponenten bereitgestellt werden, und beseitigt Lade-Spinner. Die Verwendung von queryClient.ensureQueryData() prüft zunächst den Cache und ruft bei Bedarf neue Daten ab, um sicherzustellen, dass die Seite beim Bereitstellen sofort gerendert wird."
      },
      "es": {
        "title": "Cómo usar React Query con React Router Loaders (Captura previa y almacenamiento en caché de datos)",
        "summary": "La combinación de React Query con los cargadores de React Router permite la captura previa de datos antes de que se monten los componentes, eliminando los indicadores de carga. El uso de queryClient.ensureQueryData() verifica primero el caché y obtiene datos frescos si es necesario, asegurando que la página se procese inmediatamente cuando se monta."
      }
    }
  },
  {
    "title": "Appends for AI apps: Stream into a single message with Ably AI Transport",
    "slug": "appends-for-ai-apps-stream-into-single-message-ably-ai-transport",
    "url": "https://dev.to/ablyblog/appends-for-ai-apps-stream-into-a-single-message-with-ably-ai-transport-398a",
    "source": "DEV Community",
    "date": "2026-02-26T12:05:30.000Z",
    "summary": "Ably AI Transport's message appends feature streams AI tokens into a single message rather than individual messages, solving fragmentation issues from client disconnections and refreshes. This approach simplifies UI reconstruction and message history handling without manual orchestration.",
    "content": "Streaming tokens is easy. Resuming cleanly is not. A user refreshes mid-response, another client joins late, a mobile connection drops for 10 seconds, and suddenly your \"one answer\" is 600 tiny messages that your UI has to stitch back together. Message history turns into fragments. You start building a side store just to reconstruct \"the response so far\".\nThis is not a model problem. It's a delivery problem\nThat's why we developed message appends for Ably AI Transport. Appends let you stream AI output tokens into a single message as they are produced, so you get progressive rendering for live subscribers and a clean, compact response in history.\nThe failure mode we're fixing\n\n\nThe usual implementation is to stream each token as a single message, which is simple and works perfectly on a stable connection. In production, clients disconnect and resume mid-stream: refreshes, mobile dropouts, backgrounded tabs, and late joins.\nOnce you have real reconnects and refreshes, you inherit work you did not plan for: ordering, dedupe, buffering, \"latest wins\" logic, and replay rules that make history and realtime agree. You can build it, but it is the kind of work that quietly eats weeks of engineering time.\n\nWith appends you can avoid that by changing the shape of the data. Instead of hundreds of token messages, you have one response message whose content grows over time.\nIn Ably AI Transport, you publish an initial response message and capture its server-assigned serial. That serial is what you append to.\nIt's a small detail that ends up doing a lot of work for you:\nconst result = await channel.publish({ name: 'response', data: '' });\nconst { serials: [msgSerial] } = result;\n\nNow, as your model yields tokens, you append each fragment to that same message:\nif (event.type === 'token') {\n  channel.appendMessage({ serial: msgSerial, data: event.text });\n}\n\nWhat changes for clients\n\n\nSubscribers still see progressive output, but they see it as actions on the same message serial. A",
    "category": "github",
    "translations": {
      "zh": {
        "title": "AI应用程序的追加：使用Ably AI Transport流入单个消息",
        "summary": "Ably AI Transport的消息追加功能将AI令牌流入单个消息而不是多个消息，解决了客户端断开连接和刷新导致的碎片化问题。这种方法简化了UI重建和消息历史处理，无需手动编排。"
      },
      "fr": {
        "title": "Appends pour les applications IA : Flux dans un message unique avec Ably AI Transport",
        "summary": "La fonctionnalité d'ajout de messages d'Ably AI Transport diffuse les jetons IA dans un message unique plutôt que dans des messages individuels, résolvant les problèmes de fragmentation dus aux déconnexions et actualisations des clients. Cette approche simplifie la reconstruction de l'interface utilisateur et la gestion de l'historique des messages sans orchestration manuelle."
      },
      "de": {
        "title": "Appends für KI-Apps: Stream in eine einzelne Nachricht mit Ably AI Transport",
        "summary": "Die Nachrichtenanhang-Funktion von Ably AI Transport streamt KI-Token in eine einzelne Nachricht statt in einzelne Nachrichten und löst Fragmentierungsprobleme durch Client-Trennungen und Aktualisierungen. Dieser Ansatz vereinfacht die UI-Rekonstruktion und das Nachrichtenverlauf-Management ohne manuelle Orchestrierung."
      },
      "es": {
        "title": "Appends para aplicaciones de IA: Stream en un mensaje único con Ably AI Transport",
        "summary": "La función de adición de mensajes de Ably AI Transport transmite tokens de IA en un mensaje único en lugar de mensajes individuales, resolviendo problemas de fragmentación por desconexiones y actualizaciones de clientes. Este enfoque simplifica la reconstrucción de la interfaz de usuario y el manejo del historial de mensajes sin orquestación manual."
      }
    }
  },
  {
    "title": "Node.js Application with CI/CD GitLab Pipeline on AWS EC2",
    "slug": "node-js-application-ci-cd-gitlab-pipeline-aws-ec2",
    "url": "https://dev.to/addwebsolutionpvtltd/nodejs-application-with-cicd-gitlab-pipeline-on-aws-ec2-2kk9",
    "source": "DEV Community",
    "date": "2026-02-26T12:01:06.000Z",
    "summary": "This guide demonstrates setting up a CI/CD pipeline for Node.js applications using GitLab CI/CD to automate testing, building, and deployment to AWS EC2 instances. The automated workflow reduces manual errors and accelerates development cycles through SSH-based code deployment and PM2-managed application restarts.",
    "content": "“Automation is the key to speed and reliability in modern software development.”\nIntroduction\nArchitecture Overview\nPrerequisites\nCI/CD Workflow (Step-by-Step)\nGitLab Pipeline Configuration\nDeployment Process on AWS EC2\nSecurity Best Practices\nInteresting Facts & Statistics\nFAQs\nKey Takeaways\nConclusion\nContinuous Integration and Continuous Deployment (CI/CD) is a modern development practice that automates the process of building, testing, and deploying applications. This document explains how to set up a CI/CD pipeline for a Node.js application using GitLab CI/CD and deploy it automatically to an AWS EC2 instance.\nAutomate deployment\nReduce manual errors\nImprove development speed\nEnsure reliable releases\nBackend: Node.js\nVersion Control: GitLab\nCI/CD Tool: GitLab Pipeline\nServer: AWS EC2 (Ubuntu)\nProcess Manager: PM2\nSSH Authentication: Secure Key-based login\nHigh-level Flow:\nDeveloper pushes code to GitLab repository.\nGitLab pipeline triggers automatically.\nPipeline installs dependencies and builds project.\nGitLab connects to EC2 via SSH.\nCode is pulled on EC2 server.\nApplication restarts using PM2.\nNginx routes HTTP traffic to the Node.js app\nBefore setting up CI/CD, ensure the following:\nGitLab Setup\nGitLab repository created\nBranches (dev/stage/prod) configured\nGitLab Runner enabled (shared runner works)\nAWS EC2 Setup\nUbuntu EC2 instance running\nNode.js & npm installed\nGit installed on server\nSSH access configured\nPM2 installed globally\n\n\n\nnpm install pm2 -g\n\nSSH Key Setup\nGenerate SSH key on local system:\n\n\n\n    ssh-keygen -t rsa -b 4096\n\nAdd public key to EC2:\n\n\n\n    ~/.ssh/authorized_keys\n\n- Add private key in GitLab:\n    **GitLab → Settings → CI/CD → Variables**\n\nSSH_PRIVATE_KEY\nSSH_HOST\nSSH_USER\nStep 1: Developer Pushes Code\nStep 2: Pipeline Triggered\nStep 3: Install Dependencies\nStep 4: SSH Connection\nStep 5: Deployment\nStep 6: Live Deployment\n“CI/CD turns deployment from a risky event into a routine process.”\nCreate .gitlab-ci.yml in project root:\nproduc",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Node.js应用程序与CI/CD GitLab管道在AWS EC2上的集成",
        "summary": "本指南演示了如何使用GitLab CI/CD为Node.js应用程序设置CI/CD管道，以自动化测试、构建和部署到AWS EC2实例。自动化工作流通过基于SSH的代码部署和PM2管理的应用程序重启来减少手动错误并加快开发周期。"
      },
      "fr": {
        "title": "Application Node.js avec pipeline CI/CD GitLab sur AWS EC2",
        "summary": "Ce guide démontre comment configurer un pipeline CI/CD pour les applications Node.js en utilisant GitLab CI/CD pour automatiser le test, la construction et le déploiement sur des instances AWS EC2. Le flux de travail automatisé réduit les erreurs manuelles et accélère les cycles de développement grâce au déploiement de code basé sur SSH et aux redémarrages d'application gérés par PM2."
      },
      "de": {
        "title": "Node.js-Anwendung mit CI/CD-GitLab-Pipeline auf AWS EC2",
        "summary": "Diese Anleitung demonstriert die Einrichtung einer CI/CD-Pipeline für Node.js-Anwendungen mit GitLab CI/CD zur Automatisierung von Tests, Builds und Bereitstellung auf AWS EC2-Instanzen. Der automatisierte Workflow reduziert manuelle Fehler und beschleunigt Entwicklungszyklen durch SSH-basierte Code-Bereitstellung und PM2-verwaltete Anwendungsneustart."
      },
      "es": {
        "title": "Aplicación Node.js con pipeline CI/CD de GitLab en AWS EC2",
        "summary": "Esta guía demuestra cómo configurar un pipeline CI/CD para aplicaciones Node.js usando GitLab CI/CD para automatizar pruebas, compilación e implementación en instancias de AWS EC2. El flujo de trabajo automatizado reduce errores manuales y acelera los ciclos de desarrollo mediante la implementación de código basada en SSH y reinicios de aplicaciones gestionados por PM2."
      }
    }
  },
  {
    "title": "The £20 Billion Handshake",
    "slug": "the-20-billion-handshake",
    "url": "https://dev.to/rawveg/the-ps20-billion-handshake-2mmk",
    "source": "DEV Community",
    "date": "2026-02-26T12:00:00.000Z",
    "summary": "Google pays Apple $20 billion annually to remain Safari's default search engine, a relationship exposed during DOJ antitrust proceedings that reflects deeper competition among tech giants. As AI-powered search tools reshape how users find information, these backend licensing deals increasingly determine competitive advantage in the digital economy.",
    "content": "The smartphone in your pocket contains a curious paradox. Apple, one of the world's most valuable companies, builds its own chips, designs its own operating system, and controls every aspect of its ecosystem with obsessive precision. Yet when you tap Safari's search bar, you're not using an Apple search engine. You're using Google. And Google pays Apple a staggering $20 billion every year to keep it that way.\nThis colossal payment, revealed during the US Department of Justice's antitrust trial against Google, represents far more than a simple business arrangement. It's the visible tip of a fundamental transformation in how digital platforms compete, collaborate, and ultimately extract value from the billions of searches and queries humans perform daily. As artificial intelligence reshapes the search landscape and digital assistants become genuine conversational partners rather than glorified keyword matchers, these backend licensing deals are quietly redrawing the competitive map of the digital economy.\nThe stakes have never been higher. Search advertising generated $102.9 billion in revenue in the United States alone during 2024, accounting for nearly 40 per cent of all digital advertising spending. But the ground is shifting beneath the industry's feet. AI-powered search experiences from OpenAI's ChatGPT, Microsoft's Copilot, and Google's own AI Overviews are fundamentally changing how people find information, and these changes threaten to upend decades of established business models. Into this volatile mix come a new wave of licensing deals, platform partnerships, and strategic alliances that could determine which companies dominate the next generation of digital interaction.\nTo understand where we're heading, it helps to grasp how we got here. Google's dominance in search wasn't accidental. The company built the best search engine, captured roughly 90 per cent of the market, and then methodically paid billions to ensure its search bar appeared by default on ever",
    "category": "github",
    "translations": {
      "zh": {
        "title": "200亿英镑的握手",
        "summary": "谷歌每年向苹果支付200亿美元以保持在Safari中作为默认搜索引擎的地位,这一关系在司法部反垄断诉讼中曝光,反映了科技巨头之间更深层的竞争。随着由人工智能驱动的搜索工具重塑用户如何查找信息,这些后端许可协议日益决定了数字经济中的竞争优势。"
      },
      "fr": {
        "title": "La Poignée de Main de 20 Milliards de Livres",
        "summary": "Google paie à Apple 20 milliards de dollars par an pour rester le moteur de recherche par défaut de Safari, une relation exposée lors des poursuites antitrust du ministère de la Justice qui reflète une concurrence plus profonde entre les géants de la technologie. À mesure que les outils de recherche alimentés par l'IA redéfinissent la façon dont les utilisateurs trouvent des informations, ces accords de licence de backend déterminent de plus en plus l'avantage concurrentiel dans l'économie numérique."
      },
      "de": {
        "title": "Der 20-Milliarden-Pfund-Handschlag",
        "summary": "Google zahlt Apple jährlich 20 Milliarden Dollar, um die Standard-Suchmaschine von Safari zu bleiben, eine Beziehung, die in den Kartellverfahren des US-Justizministeriums offengelegt wurde und tiefer gehende Konkurrenzen zwischen Technologieriesen widerspiegelt. Da KI-gestützte Suchtools die Art und Weise neu gestalten, wie Benutzer Informationen finden, bestimmen diese Backend-Lizenzvereinbarungen zunehmend den Wettbewerbsvorteil in der digitalen Wirtschaft."
      },
      "es": {
        "title": "El Apretón de Manos de 20 Mil Millones de Libras",
        "summary": "Google paga a Apple 20 mil millones de dólares anuales para seguir siendo el motor de búsqueda predeterminado de Safari, una relación expuesta durante los procedimientos antimonopolio del Departamento de Justicia que refleja una competencia más profunda entre los gigantes tecnológicos. A medida que las herramientas de búsqueda impulsadas por IA remodelan cómo los usuarios encuentran información, estos acuerdos de licencia de backend determinan cada vez más la ventaja competitiva en la economía digital."
      }
    }
  },
  {
    "title": "Kubernetes'te StorageClass Nedir?",
    "slug": "kubernetes-storageclass-nedir",
    "url": "https://dev.to/tarikanafarta/kuberneteste-storageclass-nedir-dha",
    "source": "DEV Community",
    "date": "2026-02-26T11:49:45.000Z",
    "summary": "Kubernetes storage architecture comprises PV (Persistent Volumes), PVC (Persistent Volume Claims), and StorageClass, supporting both static and dynamic provisioning models. Data storage can reside on node-local disks, NFS shares, or cloud providers depending on StorageClass policies and PV configuration.",
    "content": "PV, PVC ve Veri Gerçekten Nerede Saklanıyor?\n\n\nBu yazıda şu konuları inceleyeceğim:\nPV (Persistent Volume) nedir?\nPVC (Persistent Volume Claim) nedir?\nStorageClass ne işe yarar?\nStatic vs Dynamic provisioning farkı nedir?\nMariaDB / PostgreSQL gibi uygulamalarda disk nerede?\nÖncelikle Kubernetes'te storage zinciri şu şekildedir:\nPod -> PVC -> StorageClass -> PV -> Physical Storage\nPV, cluster içindeki gerçek disk kaynağını temsil eder.\nBu kaynak şunlardan birisidir:\nNode üzerindeki local disk\nNFS share\nCloud disk\nDistributed storage\nYani aslında PV, storage'ın kendisidir.\nPVC ise uygulamanın disk talebidir.\nÖrneğin:\n10Gi storage\nAccess mode: ReadWriteOnce\nstorageClass: fast-storage\nPVC diski oluşturmaz, bir disk talep eder.\nStorageClass, PVC oluşturulduğunda disk'in nasıl sağlanacağını belirler.\nYani, disk local mi olacak? NFS mi olacak? Cloud block storage mı olacak? Otomatik mi üretilecek? Hangi performans sınıfı kullanılacak? Gibi sorulara cevap veren bir policy katmanıdır.\nStatic provisioning modelinde PV manuel olarak oluşturulur. Yani önce storage kaynağı tanımlanır, ardından PVC bu PV'ye bağlanır.\nBu yöntem daha fazla kontrol sağlar ancak her yeni disk ihtiyacında manuel işlem gerektirir. Genellikle test ortamlarında veya local/NFS kurulumlarında tercih edilir.\nDynamic provisioning modelinde ise sadece PVC oluşturulur.\nBu süreç CSI (Container Storage Interface) driver'ları sayesinde çalışır.\nBu sorunun cevabı PV tanımındadır.\nPV şu path'i gösteriyorsa: /mnt/k8s/data, disk node üzerindedir. Node arızalanırsa veri kaybedilebilir. Pod başka node'a taşınırsa diske erişilemez.\nPV şu şekilde tanımlanmışsa:\nnfs:\n  server: 192.168.1.10\n  path: /srv/nfs/share\n\nDisk NFS server üzerindedir. Pod hangi node'da olursa olsun aynı veriye erişebilir.\nPV bir cloud volume'a bağlıysa, disk cloud provider tarafındadır.\nRWO (ReadWriteOnce) -> Sadece tek node yazabilir\nPVC silindiğinde disk'in nasıl davranacağını reclaim policy belirler. Kubernetes'te 3 farklı reclaim policy vardır",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Kubernetes中的StorageClass是什么?",
        "summary": "Kubernetes存储架构包括PV(持久卷)、PVC(持久卷声明)和StorageClass,支持静态和动态配置模型。根据StorageClass策略和PV配置,数据存储可以驻留在节点本地磁盘、NFS共享或云提供商上。"
      },
      "fr": {
        "title": "Qu'est-ce que StorageClass dans Kubernetes?",
        "summary": "L'architecture de stockage Kubernetes comprend PV (Persistent Volumes), PVC (Persistent Volume Claims) et StorageClass, supportant les modèles de provisionnement statique et dynamique. Le stockage de données peut résider sur des disques locaux de nœud, des partages NFS ou des fournisseurs cloud en fonction des politiques StorageClass et de la configuration PV."
      },
      "de": {
        "title": "Was ist StorageClass in Kubernetes?",
        "summary": "Die Kubernetes-Speicherarchitektur umfasst PV (Persistent Volumes), PVC (Persistent Volume Claims) und StorageClass und unterstützt sowohl statische als auch dynamische Bereitstellungsmodelle. Der Datenspeicher kann je nach StorageClass-Richtlinien und PV-Konfiguration auf lokalen Node-Festplatten, NFS-Freigaben oder Cloud-Anbietern residieren."
      },
      "es": {
        "title": "¿Qué es StorageClass en Kubernetes?",
        "summary": "La arquitectura de almacenamiento de Kubernetes comprende PV (Volúmenes Persistentes), PVC (Reclamaciones de Volumen Persistentes) y StorageClass, admitiendo modelos de aprovisionamiento estático y dinámico. El almacenamiento de datos puede residir en discos locales de nodos, recursos compartidos NFS o proveedores en la nube dependiendo de las políticas de StorageClass y la configuración de PV."
      }
    }
  },
  {
    "title": "Why is cdk.out (Cloud Assembly) Necessary in AWS CDK?",
    "slug": "why-is-cdk-out-cloud-assembly-necessary-aws-cdk",
    "url": "https://dev.to/aws-heroes/why-is-cdkout-cloud-assembly-necessary-in-aws-cdk-n5f",
    "source": "DEV Community",
    "date": "2026-02-26T11:45:47.000Z",
    "summary": "AWS CDK's cdk.out directory stores the cloud assembly—an intermediate artifact containing CloudFormation templates, metadata, and asset files generated from CDK code. This assembly bridges CDK code to AWS infrastructure deployment and includes stack templates, manifest files, and Docker image assets.",
    "content": "What is cdk.out\n\n\ncdk.out is a directory that stores a collection of files called the cloud assembly.\nWhen you run the cdk synth command or the cdk deploy command, the cloud assembly is generated from your CDK code and stored in the cdk.out directory.\nThe cloud assembly is a collection of files generated by the synthesis of a CDK application (CDK code). The CDK CLI references this information to deploy resources to the AWS environment.\nIn other words, the cloud assembly can be considered an intermediate artifact that CDK uses to deploy infrastructure definitions written in CDK code to the AWS environment.\n\nThe cloud assembly primarily consists of the following files and directories:\n(stack name).template.json\n\n\nCloudFormation template generated by CDK code\n(stack name).assets.json\n\n\nFile describing information about assets used in CDK code\nmanifest.json\n\n\nFile describing metadata of the cloud assembly\ntree.json\n\n\nFile representing the Construct tree structure of resources defined in CDK code\ncdk.out\n\n\nFile storing Cloud Assembly schema version information\nasset.(hash value)/\n\n\nDirectory storing asset files used in CDK code\nUses hash values calculated for each asset as directory names\nEach asset is classified into two types: S3 assets and Docker image assets, which are uploaded to S3 buckets or ECR repositories created by the cdk bootstrap command when the cdk deploy command is executed\nDocker image assets are built when the cdk deploy command is executed (not when the cdk synth command is executed)\nassembly-(stage name)/\n\n\nDirectory storing cloud assemblies generated for each stage when using Stage Construct\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  Why cdk.out is Necessary\n\n\nAs explained above, cdk.out is a directory for storing the cloud assembly, which is an intermediate artifact generated from CDK code.\nSo why is this directory necessary? You might wonder whether it's really necessary to generate and store information needed for deployment as files.\nBefore explaining this, let me fir",
    "category": "github",
    "translations": {
      "zh": {
        "title": "为什么AWS CDK中的cdk.out(云程序集)是必要的?",
        "summary": "AWS CDK的cdk.out目录存储云程序集——一个中间制品,包含从CDK代码生成的CloudFormation模板、元数据和资产文件。此程序集连接CDK代码与AWS基础设施部署,包括堆栈模板、清单文件和Docker镜像资产。"
      },
      "fr": {
        "title": "Pourquoi cdk.out (Cloud Assembly) est-il nécessaire dans AWS CDK?",
        "summary": "Le répertoire cdk.out d'AWS CDK stocke l'assembly cloud — un artefact intermédiaire contenant les modèles CloudFormation, les métadonnées et les fichiers d'actifs générés à partir du code CDK. Cet assembly relie le code CDK au déploiement de l'infrastructure AWS et inclut les modèles de pile, les fichiers manifeste et les actifs d'image Docker."
      },
      "de": {
        "title": "Warum ist cdk.out (Cloud Assembly) in AWS CDK notwendig?",
        "summary": "Das Verzeichnis cdk.out von AWS CDK speichert die Cloud Assembly — ein Zwischenartefakt, das CloudFormation-Vorlagen, Metadaten und aus CDK-Code generierte Asset-Dateien enthält. Diese Assembly verbindet CDK-Code mit der AWS-Infrastruktur-Bereitstellung und umfasst Stack-Vorlagen, Manifest-Dateien und Docker-Image-Assets."
      },
      "es": {
        "title": "¿Por qué es necesario cdk.out (Cloud Assembly) en AWS CDK?",
        "summary": "El directorio cdk.out de AWS CDK almacena el cloud assembly—un artefacto intermedio que contiene plantillas de CloudFormation, metadatos y archivos de activos generados a partir del código CDK. Este assembly vincula el código CDK con la implementación de infraestructura de AWS e incluye plantillas de pila, archivos de manifiesto y activos de imagen Docker."
      }
    }
  },
  {
    "title": "My Pixel's Keyboard Generated a Custom Emoji.",
    "slug": "my-pixel-keyboard-generated-custom-emoji",
    "url": "https://dev.to/megzlawther1/my-pixels-keyboard-generated-a-custom-emoji-42k",
    "source": "DEV Community",
    "date": "2026-02-26T11:45:35.000Z",
    "summary": "Google Gemini's Emojify feature on Pixel 10 keyboards generates custom emojis dynamically beyond Unicode standards, creating novel visual communications not found in preset emoji tables. This on-device AI capability demonstrates how generative models can extend traditional interface constraints.",
    "content": "What I Built with Google Gemini: \nMy project isn't a traditional \"build\" in the sense of writing code for a new application. Instead, I built a novel interaction with Google Gemini, specifically through the \"Emojify\" feature on my Google Pixel 10 keyboard. My \"build\" was essentially demonstrating and exploring how a widely accessible AI feature can generate entirely new, non-standard visual communication..\nThe \"problem\" it solved (or rather, illuminated) is the limitation of pre-set, static emoji sets. While existing emojis are vast, they can't cover every nuanced concept. My interaction with Gemini showcased its ability to dynamically create a custom visual to perfectly fit a specific textual context, transcending the boundaries of Unicode.\nHere's what happened: \nThis wasn't through a dedicated generative AI interface; it was a seamless, on-device AI capability integrated into a common keyboard function. \nWhat I Learned:\nThis experience has been incredibly eye-opening, both technically and philosophically.\nGoing Beyond the Standard Emoji Table (and My Reaction to It): My biggest ponderance and a major learning point is how it was able to go beyond the standard emoji table of preset characters (Unicode/ASCII). Emojis are typically strict templates of codes, already preset and preloaded. Gemini's Emojify function went out of its preset and preloaded list to generate something entirely new. When I saw this custom visual, my immediate reaction was confusion. I automatically checked my keyboard's emoji list, particularly the \"Family\" section, and confirmed that the generated icon was not available there. My screenshot of the available standard family emojis (e.g., \"Family: Adult, Adult, Child\") clearly shows they are distinct and generic. This solidifies that the AI truly generated a novel image, rather than pulling from existing assets, and profoundly changed my understanding of what a keyboard's \"emojify\" function is capable of.\nAI's Gender Inference in Generative Out",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我的Pixel键盘生成了自定义表情符号",
        "summary": "谷歌Gemini在Pixel 10键盘上的Emojify功能动态生成超越Unicode标准的自定义表情符号，创造出预设表情符号表中找不到的新颖视觉通信。这种设备上的AI能力展示了生成模型如何能够扩展传统界面约束。"
      },
      "fr": {
        "title": "Mon clavier Pixel a généré un emoji personnalisé",
        "summary": "La fonction Emojify de Google Gemini sur les claviers Pixel 10 génère dynamiquement des emojis personnalisés au-delà des normes Unicode, créant de nouvelles communications visuelles introuvables dans les tableaux d'emojis préétablis. Cette capacité d'IA sur l'appareil démontre comment les modèles génératifs peuvent étendre les contraintes d'interface traditionnelles."
      },
      "de": {
        "title": "Meine Pixel-Tastatur hat ein benutzerdefiniertes Emoji generiert",
        "summary": "Googles Gemini-Emojify-Funktion auf Pixel-10-Tastaturen generiert dynamisch benutzerdefinierte Emojis jenseits von Unicode-Standards und erzeugt neuartige visuelle Kommunikation, die in voreingestellten Emoji-Tabellen nicht zu finden ist. Diese On-Device-KI-Funktion zeigt, wie generative Modelle traditionelle Schnittstellenbeschränkungen erweitern können."
      },
      "es": {
        "title": "Mi teclado Pixel generó un emoji personalizado",
        "summary": "La función Emojify de Google Gemini en los teclados Pixel 10 genera emojis personalizados de forma dinámica más allá de los estándares Unicode, creando comunicaciones visuales novedosas que no se encuentran en las tablas de emojis predefinidas. Esta capacidad de IA en el dispositivo demuestra cómo los modelos generativos pueden ampliar las restricciones tradicionales de la interfaz."
      }
    }
  },
  {
    "title": "I Built and Launched an AI Document API in Under a Week — Here's Exactly How I Did It",
    "slug": "i-built-and-launched-ai-document-api-under-week",
    "url": "https://dev.to/senzen/i-built-and-launched-an-ai-document-api-in-under-a-week-heres-exactly-how-i-did-it-4j8c",
    "source": "DEV Community",
    "date": "2026-02-26T11:45:08.000Z",
    "summary": "Developer launched Condensare, an AI document processing API leveraging GPT-4o to transform uploaded files into structured notes and contextual implementation suggestions. Deployed serverlessly on Vercel and published on RapidAPI, the solution was built and launched in under one week.",
    "content": "I Built and Launched an AI Document API in Under a Week — Here's Exactly How I Did It\n\n\n\nTL;DR: I built Condensare — an AI-powered document processing API that turns any uploaded file into structured notes and contextualised implementation suggestions. It's live on RapidAPI. This is the full breakdown of how I built it, deployed it, tested it and shipped it.\nA few weeks ago I had a simple frustration.\nI was uploading documents to various AI tools trying to get useful summaries and actionable insights, and every single one felt generic. The output didn't know what industry I was in, what scale my business operated at, or what I actually wanted to do with the information.\nIt just summarised. That was it.\nSo I built Condensare — a document condensing and AI suggestion API that takes any uploaded file and returns structured notes plus contextualised implementation suggestions.\nI kept it simple and production-focused:\n\n\n\nLayer\nTechnology\n\n\n\n\nAPI Server\nNode.js + Express\n\n\nAI\nOpenAI GPT-4o\n\n\nDeployment\nVercel (serverless)\n\n\nMarketplace\nRapidAPI\n\n\nFile Uploads\nMulter\n\n\nDocument Parsing\npdf-parse, mammoth, xlsx\n\n\n\nVercel serverless was the right call for an API marketplace product — zero infrastructure management, global distribution out of the box, automatic scaling. You don't want to be managing servers when you're trying to get to market fast.\ncondensare-api/\n├── src/\n│   ├── app.js                  # Express entry point\n│   ├── routes/\n│   │   └── condense.js         # Route definitions\n│   ├── middleware/\n│   │   └── authenticate.js     # Auth + plan management\n│   ├── services/\n│   │   ├── parseService.js     # File parsing logic\n│   │   ├── condenseService.js  # AI condensing logic\n│   │   └── suggestService.js   # AI suggestions logic\n│   └── utils/\n│       └── fileUtils.js        # File validation helpers\n├── vercel.json\n└── package.json\n\nPOST /api/v1/condense/parse\n\n\nAvailable on: FREE, PRO, MEGA, ULTRA\nThe lightweight endpoint. Upload any supported file and get b",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我在一周内构建并推出了一个AI文档API——以下是我的具体做法",
        "summary": "开发者推出了Condensare，这是一个利用GPT-4o的AI文档处理API，可以将上传的文件转换为结构化笔记和上下文实现建议。在Vercel上无服务器部署并在RapidAPI上发布，该解决方案在一周内构建和推出。"
      },
      "fr": {
        "title": "J'ai construit et lancé une API de documents IA en une semaine — Voici exactement comment j'ai fait",
        "summary": "Un développeur a lancé Condensare, une API de traitement de documents IA exploitant GPT-4o pour transformer les fichiers téléchargés en notes structurées et suggestions d'implémentation contextuelle. Déployée sans serveur sur Vercel et publiée sur RapidAPI, la solution a été construite et lancée en une semaine."
      },
      "de": {
        "title": "Ich habe eine KI-Dokument-API in einer Woche entwickelt und gestartet — Genau so habe ich es getan",
        "summary": "Ein Entwickler hat Condensare gestartet, eine KI-Dokumentverarbeitungs-API, die GPT-4o nutzt, um hochgeladene Dateien in strukturierte Notizen und kontextabhängige Implementierungsvorschläge umzuwandeln. Serverlos auf Vercel bereitgestellt und auf RapidAPI veröffentlicht, wurde die Lösung in einer Woche entwickelt und gestartet."
      },
      "es": {
        "title": "Construí y lancé una API de documentos IA en una semana — Así es exactamente cómo lo hice",
        "summary": "Un desarrollador lanzó Condensare, una API de procesamiento de documentos IA que aprovecha GPT-4o para transformar archivos cargados en notas estructuradas y sugerencias de implementación contextuales. Implementada sin servidor en Vercel y publicada en RapidAPI, la solución se construyó y lanzó en una semana."
      }
    }
  },
  {
    "title": "Why We Replaced Debezium + Kafka in Our Large-Scale Real-Time Pipeline",
    "slug": "why-we-replaced-debezium-kafka-large-scale-real-time-pipeline",
    "url": "https://dev.to/heywalter/why-we-replaced-debezium-kafka-in-our-large-scale-real-time-pipeline-2dc1",
    "source": "DEV Community",
    "date": "2026-02-26T11:42:08.000Z",
    "summary": "A company replaced Debezium and Kafka with alternative solutions to sync over 3,000 tables across heterogeneous databases (Oracle, MySQL, PostgreSQL, MongoDB) into ClickHouse for real-time analytics. The migration addressed operational complexity and maintenance overhead in large-scale CDC pipelines.",
    "content": "I’ve just wrapped up a real-time data platform project that’s been running smoothly in production for a few months now. While it’s all still fresh, I figured it’s a good time to look back on our selection process, the migration, the pitfalls we hit, and some takeaways — hopefully useful for anyone tackling real-time synchronization across heterogeneous databases.\nOur company has grown quickly over the past few years, with multiple rounds of IT upgrades, system migrations, and acquisitions along the way. This left us with a mixed bag of databases: Oracle for legacy core transaction systems, MySQL for most business applications, SQL Server for some remaining Windows-based legacy apps, PostgreSQL for newer microservices, and even MongoDB for semi-structured data. All told, we have around 50+ instances and over 3,000 regularly used tables, spread across different teams and systems — a textbook case of data silos.\nThe goal was to build a unified real-time analytics platform by syncing data from all these sources near-real-time into a data warehouse (we went with ClickHouse). Primary use cases included real-time dashboards, risk monitoring, and data APIs for some of the downstream applications . ClickHouse’s strengths in high-concurrency queries, compression, and OLAP performance made it perfect for delivering second-to-minute latency.\nOn top of that, we wanted the pipeline to be relatively low-maintenance, so data engineers could own it day-to-day and we could move toward real DataOps practices — instead of pulling in developers for every tweak.\nAs the tech leader, I usually lean toward proven open-source solutions to avoid building everything from scratch. So we naturally started with the industry’s go-to real-time CDC stack:\n\nFigure1: Debezium Kafka ClickHouse architecture\nCore components:\nCDC Capture: Debezium for change capture. Its wide range of connectors covered virtually all our source types, and the community is solid.\nBuffering: Apache Kafka as the intermediate",
    "category": "github",
    "translations": {
      "zh": {
        "title": "为什么我们在大规模实时管道中替换了Debezium + Kafka",
        "summary": "一家公司用替代方案替换了Debezium和Kafka，将超过3000个表从异构数据库（Oracle、MySQL、PostgreSQL、MongoDB）同步到ClickHouse以进行实时分析。这次迁移解决了大规模CDC管道中的操作复杂性和维护开销问题。"
      },
      "fr": {
        "title": "Pourquoi nous avons remplacé Debezium + Kafka dans notre pipeline temps réel à grande échelle",
        "summary": "Une entreprise a remplacé Debezium et Kafka par des solutions alternatives pour synchroniser plus de 3 000 tables à travers des bases de données hétérogènes (Oracle, MySQL, PostgreSQL, MongoDB) dans ClickHouse pour l'analyse en temps réel. La migration a résolu la complexité opérationnelle et la surcharge de maintenance dans les pipelines CDC à grande échelle."
      },
      "de": {
        "title": "Warum wir Debezium + Kafka in unserer großflächigen Echtzeit-Pipeline ersetzt haben",
        "summary": "Ein Unternehmen hat Debezium und Kafka durch alternative Lösungen ersetzt, um über 3.000 Tabellen aus heterogenen Datenbanken (Oracle, MySQL, PostgreSQL, MongoDB) in ClickHouse für Echtzeit-Analytik zu synchronisieren. Die Migration hat die operative Komplexität und Wartungsbelastung in großflächigen CDC-Pipelines reduziert."
      },
      "es": {
        "title": "Por qué reemplazamos Debezium + Kafka en nuestro pipeline de tiempo real a gran escala",
        "summary": "Una empresa reemplazó Debezium y Kafka con soluciones alternativas para sincronizar más de 3,000 tablas en bases de datos heterogéneas (Oracle, MySQL, PostgreSQL, MongoDB) en ClickHouse para análisis en tiempo real. La migración abordó la complejidad operativa y la sobrecarga de mantenimiento en pipelines CDC a gran escala."
      }
    }
  },
  {
    "title": "Cypress in the Age of AI Agents: Orchestration, Trust, and the Tests That Run Themselves",
    "slug": "cypress-in-age-of-ai-agents-orchestration-trust-tests",
    "url": "https://dev.to/cypress/cypress-in-the-age-of-ai-agents-orchestration-trust-and-the-tests-that-run-themselves-43go",
    "source": "DEV Community",
    "date": "2026-02-26T11:33:21.000Z",
    "summary": "Cypress's cy.prompt() enables AI to write and self-heal tests in plain English, but introduces trust concerns when AI makes autonomous pipeline decisions without human oversight. The article distinguishes between risky autonomy and preferable augmentation in AI-driven testing frameworks.",
    "content": "Last year, I wrote about Docker and Cypress for this blog. It covered containers, layer caching, and parallel runners. Good stuff. Useful stuff.\nBut I'm not writing that article again.\nHere's why.\nI could write a perfect container config in my sleep. So could Claude. So could GPT. So could any intern with a prompt. Syntax has become a commodity. The Dockerfile isn't the hard part anymore.\nThe hard part?\nOrchestration and trust when AI agents run the tests.\nLet me explain.\nIn 2025, Cypress shipped cy.prompt(). Write tests in plain English. The AI figures out the selectors. It even self-heals when your UI changes.\nThat's powerful. And that's dangerous.\nNot because the tool is bad. It's genuinely impressive. But because it changes who is making decisions in your pipeline. And most teams haven't thought about that.\nBefore cy.prompt(), the chain of trust was simple:\nA human wrote the test\nA human reviewed it\nCI ran it\nIf it failed, a human fixed it\nEvery link in that chain had a name attached.\nNow?\nAn AI writes the test\nAn AI picks the selectors\nAn AI heals the test when it breaks\nThe human sees green checkmarks\nEverybody ships\nUntil something goes wrong. And nobody knows why.\nThe industry keeps confusing two very different things.\nAutonomy means the agent acts for you. You find out later what happened.\nAugmentation means the agent helps you decide. You still make the call.\nMost AI testing tools sell autonomy:\n\"Never write a test again!\"\n\"Self-healing pipelines!\"\n\"Zero maintenance!\"\nThat sounds great in a demo.\nIt falls apart in production.\nGoogle's testing team found that 1.5% of all test runs were flaky (2016 study). Nearly 16% of tests showed some flakiness over time. Microsoft reported 49,000 flaky tests across 100+ product teams (2022). These numbers haven't gotten better. Now imagine those tests were written by AI.\nYou don't have a testing problem.\nYou have a trust problem.\nI've watched AI code assistants generate test suites. Here's the pattern I see every time:\nD",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Cypress在AI代理时代：编排、信任与自运行测试",
        "summary": "Cypress的cy.prompt()使AI能够用纯英文编写和自我修复测试，但当AI在没有人工监督的情况下自主做出管道决策时会引入信任问题。该文章区分了AI驱动测试框架中的危险自主性和更可取的增强方法。"
      },
      "fr": {
        "title": "Cypress à l'ère des agents IA : Orchestration, confiance et tests qui s'exécutent eux-mêmes",
        "summary": "La méthode cy.prompt() de Cypress permet à l'IA d'écrire et d'auto-corriger les tests en anglais simple, mais introduit des préoccupations de confiance lorsque l'IA prend des décisions autonomes dans le pipeline sans surveillance humaine. L'article distingue entre l'autonomie risquée et l'augmentation préférable dans les cadres de test pilotés par l'IA."
      },
      "de": {
        "title": "Cypress im Zeitalter von KI-Agenten: Orchestrierung, Vertrauen und selbstablaufende Tests",
        "summary": "Cypress's cy.prompt() ermöglicht es der KI, Tests in einfachem Englisch zu schreiben und selbst zu heilen, führt aber zu Vertrauensproblemen ein, wenn KI autonome Pipeline-Entscheidungen ohne menschliche Aufsicht trifft. Der Artikel unterscheidet zwischen riskanter Autonomie und erwünschter Augmentation in KI-gesteuerten Test-Frameworks."
      },
      "es": {
        "title": "Cypress en la Era de los Agentes de IA: Orquestación, Confianza y Pruebas que se Ejecutan por Sí Solas",
        "summary": "El cy.prompt() de Cypress permite que la IA escriba y auto-corrija pruebas en inglés simple, pero introduce preocupaciones de confianza cuando la IA toma decisiones autónomas en el pipeline sin supervisión humana. El artículo distingue entre la autonomía arriesgada y la aumentación preferible en marcos de prueba impulsados por IA."
      }
    }
  },
  {
    "title": "I realized my AI tools were leaking sensitive data. So I built a local proxy to stop it",
    "slug": "i-realized-ai-tools-leaking-sensitive-data-built-local-proxy",
    "url": "https://dev.to/ubcent/i-realized-my-ai-tools-were-leaking-sensitive-data-so-i-built-a-local-proxy-to-stop-it-2pma",
    "source": "DEV Community",
    "date": "2026-02-26T11:31:45.000Z",
    "summary": "Developer created Velar, a local HTTP/HTTPS proxy that detects and masks sensitive data before transmission to AI providers, preventing unintended credential and API key leaks. The tool addresses privacy gaps in AI coding assistants like Cursor and Copilot that send full codebase context to external servers.",
    "content": "A few months ago I had a moment of uncomfortable clarity.\nI was using Cursor to work on a project that had database credentials in an .env file. The AI had full access to the codebase. I wasn't thinking about it - I was just coding. And then it hit me: all of this is going to their servers right now. The keys, the internal URLs, everything.\nI stopped and thought about how long I'd been doing this without a second thought. And then I asked a few colleagues. Same story. Nobody was really thinking about it. We all just... trusted that it was fine.\nIt probably is fine, most of the time. But \"probably fine\" is not a compliance posture. And as AI coding tools get deeper access to our codebases, the surface area for accidental leaks keeps growing.\nThat's why I built Velar — a local proxy that sits between your app and AI providers, detects sensitive data, and masks it before it ever leaves your machine.\n\nCopilot, Cursor - these tools are genuinely useful. But they work by sending your code (and often a lot of surrounding context) to external APIs. Most developers don't think carefully about what's in that context.\nCommon things that end up in AI requests without people realizing:\nAWS/GCP/Azure credentials accidentally committed or present in env files\nDatabase connection strings\nInternal API endpoints and tokens\nCustomer emails or names in logs you're debugging\nJWTs from test sessions\nNone of this is malicious. It's just how development works. But \"it's not malicious\" doesn't mean it's not a problem when you're dealing with regulated data or working in an enterprise environment.\nVelar runs locally as an HTTP/HTTPS proxy with MITM support. You configure it to intercept traffic to specific domains (like api.openai.com), and it inspects outbound payloads before forwarding them.\nYour app → Velar → AI provider\n\nWhen it detects something sensitive, it replaces it with a deterministic placeholder:\nalice@company.com → [EMAIL_1]\nAKIAIOSFODNN7EXAMPLE → [AWS_KEY_1]\n\nThen, when the re",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我意识到我的AI工具在泄露敏感数据。所以我构建了一个本地代理来阻止它",
        "summary": "开发者创建了Velar，一个本地HTTP/HTTPS代理，在数据传输到AI提供商之前检测并屏蔽敏感数据，防止意外的凭证和API密钥泄露。该工具解决了Cursor和Copilot等AI编码助手的隐私漏洞，这些助手将完整代码库上下文发送到外部服务器。"
      },
      "fr": {
        "title": "J'ai réalisé que mes outils IA fuyaient des données sensibles. Alors j'ai construit un proxy local pour l'arrêter",
        "summary": "Un développeur a créé Velar, un proxy HTTP/HTTPS local qui détecte et masque les données sensibles avant leur transmission aux fournisseurs d'IA, empêchant les fuites involontaires d'identifiants et de clés API. L'outil répond aux lacunes en matière de confidentialité des assistants de codage IA comme Cursor et Copilot qui envoient le contexte complet de la base de code aux serveurs externes."
      },
      "de": {
        "title": "Ich merkte, dass meine KI-Tools sensible Daten lecken. Deshalb habe ich einen lokalen Proxy erstellt, um das zu stoppen",
        "summary": "Ein Entwickler erstellte Velar, einen lokalen HTTP/HTTPS-Proxy, der sensible Daten vor der Übertragung an KI-Anbieter erkennt und maskiert und so unbeabsichtigte Anmeldedaten- und API-Schlüssel-Lecks verhindert. Das Tool behebt Datenschutzlücken in KI-Codierungsassistenten wie Cursor und Copilot, die den vollständigen Codebase-Kontext an externe Server senden."
      },
      "es": {
        "title": "Me di cuenta de que mis herramientas de IA estaban filtrando datos sensibles. Así que construí un proxy local para detenerlo",
        "summary": "El desarrollador creó Velar, un proxy HTTP/HTTPS local que detecta y enmascara datos sensibles antes de su transmisión a proveedores de IA, evitando fugas involuntarias de credenciales y claves de API. La herramienta aborda las brechas de privacidad en asistentes de codificación impulsados por IA como Cursor y Copilot que envían contexto completo de la base de código a servidores externos."
      }
    }
  },
  {
    "title": "React Query: What Is `staleTime` and Why Should You Care?",
    "slug": "react-query-what-is-staletime-and-why-should-you-care",
    "url": "https://dev.to/bishoy_bishai/react-query-what-is-staletime-and-why-should-you-care-1m64",
    "source": "DEV Community",
    "date": "2026-02-26T11:27:35.000Z",
    "summary": "React Query's staleTime configuration defines how long cached data remains fresh before triggering background re-fetches, implementing the stale-while-revalidate pattern. Proper staleTime configuration improves perceived performance by serving cached data instantly while silently updating in the background.",
    "content": "Ever been working on a web app and felt like your data fetching was doing too much work? You know the drill: navigate to a list page, see a loading spinner. Click into a detail, another spinner. Go back to the list... spinner again. It’s a classic scenario, and honestly, it can make even the snappiest apps feel sluggish.\nAs developers, we often focus on making sure our data is always up-to-the-second fresh. But sometimes, \"always fresh\" comes at the cost of user experience. This is where React Query's staleTime comes into play. It’s not just a performance tweak; it’s a fundamental shift in how you deliver perceived performance. It’s the difference between an app that feels like a website and an app that feels like an extension of the user's mind.\nImagine an e-commerce site. A user lands on a product listing. We fetch the products. They click on a product to view details. They hit the back button. What happens?\nBy default, without any specific configuration, React Query considers data \"stale\" the moment it's fetched (default staleTime: 0). This means when the user returns to the product list, even if they were just there a second ago, React Query triggers another network request. The UI shows a loading state, maybe a flash of empty content, and then the data reappears. This \"loading flicker\" is jarring.\nstaleTime: Your Best Friend for Perceived Performance\n\n\nAt its core, staleTime tells React Query for how long a piece of data should be considered \"fresh.\" As long as data is fresh, React Query will serve it from the cache immediately without even looking at the network.\nOnce staleTime has passed, the data becomes \"stale.\" But here is the magic: React Query will still serve it from the cache instantly if it’s available, but it will also trigger a background re-fetch to get the latest version. This is the \"stale-while-revalidate\" pattern.\nData is fresh: React Query serves cached data instantly. Zero network activity.\nstaleTime expires: Data is now stale.\nNew request ha",
    "category": "github",
    "translations": {
      "zh": {
        "title": "React Query：什么是`staleTime`以及为什么应该关心？",
        "summary": "React Query的staleTime配置定义了缓存数据在触发后台重新获取之前保持新鲜的时间长度，实现了stale-while-revalidate模式。正确的staleTime配置通过立即提供缓存数据同时在后台静默更新来改善感知性能。"
      },
      "fr": {
        "title": "React Query : Qu'est-ce que `staleTime` et pourquoi devriez-vous vous en soucier ?",
        "summary": "La configuration staleTime de React Query définit combien de temps les données en cache restent actuelles avant de déclencher des re-récupérations en arrière-plan, en implémentant le modèle stale-while-revalidate. Une configuration staleTime appropriée améliore les performances perçues en servant les données en cache instantanément tout en les mettant à jour silencieusement en arrière-plan."
      },
      "de": {
        "title": "React Query: Was ist `staleTime` und warum sollte es Sie interessieren?",
        "summary": "Die staleTime-Konfiguration von React Query definiert, wie lange zwischengespeicherte Daten frisch bleiben, bevor Background-Refetches ausgelöst werden, und implementiert das stale-while-revalidate-Muster. Eine ordnungsgemäße staleTime-Konfiguration verbessert die wahrgenommene Leistung, indem sie zwischengespeicherte Daten sofort bereitstellt und gleichzeitig im Hintergrund aktualisiert."
      },
      "es": {
        "title": "React Query: ¿Qué es `staleTime` y por qué debería importarte?",
        "summary": "La configuración staleTime de React Query define cuánto tiempo los datos en caché permanecen frescos antes de desencadenar re-búsquedas en segundo plano, implementando el patrón stale-while-revalidate. Una configuración staleTime adecuada mejora el rendimiento percibido al servir datos en caché instantáneamente mientras se actualiza silenciosamente en segundo plano."
      }
    }
  },
  {
    "title": "Hide API Keys from Your Frontend — No Backend Required",
    "slug": "hide-api-keys-from-frontend-no-backend-required",
    "url": "https://dev.to/robleney/hide-api-keys-from-your-frontend-no-backend-required-nnb",
    "source": "DEV Community",
    "date": "2026-02-26T06:06:38.000Z",
    "summary": "Mongrel.io eliminates the need for backend servers by acting as a server-side proxy that injects API credentials at request time, preventing key exposure in frontend code. The service encrypts keys with AWS KMS and decrypts them only within Lambda functions, addressing critical risks like key theft, billing abuse, and rate limit exhaustion. This approach simplifies API integration for JAMstack sites and prototypes without sacrificing security.",
    "content": "If you have ever built a frontend that calls a third-party API, you have faced this problem: the API requires a key, but putting that key in your JavaScript means anyone can see it.\nThe usual fix is to build a backend proxy — a small server that holds the key and forwards requests on your behalf. It works, but now you have a server to write, deploy, and maintain. For many projects, especially prototypes, side projects, and JAMstack sites, that is a lot of overhead for what should be a simple API call.\nMongrel.io lets you skip the backend entirely. It acts as a server-side proxy that injects your credentials at request time, so your API keys never appear in your frontend code.\nHere is what the insecure pattern looks like. You want to call a weather API, so you write something like this:\nconst response = await fetch(\"https://api.weather.example/forecast?city=Sydney\", {\n  headers: {\n    \"X-API-Key\": \"sk_live_abc123def456\"\n  }\n});\nconst data = await response.json();\n\nThat API key is now visible to anyone who opens the browser's network tab. Even if you move it to an environment variable like VITE_API_KEY or NEXT_PUBLIC_API_KEY, build tools inline those values into your JavaScript bundle. The key still ships to the browser.\nThe risks are real:\nKey theft — anyone can extract the key and use it from their own code\nBilling abuse — a stolen key can rack up charges on your account\nRate limit exhaustion — automated abuse can burn through your quota, breaking the experience for legitimate users\nMongrel.io sits between your frontend and the external API. The flow looks like this:\nYour frontend calls your Mongrel.io endpoint — no API key in the request\nMongrel.io receives the request and decrypts your stored credentials\nMongrel.io calls the real API with your credentials injected server-side\nThe response is returned to your frontend\nYour API keys are encrypted with AWS KMS at rest and only decrypted inside the Lambda function at request time. You never write or deploy any backend",
    "category": "github",
    "translations": {
      "zh": {
        "title": "从前端隐藏API密钥——无需后端",
        "summary": "Mongrel.io通过充当服务器端代理来消除对后端服务器的需求，在请求时注入API凭证，防止密钥在前端代码中暴露。该服务使用AWS KMS加密密钥，并仅在Lambda函数内解密，解决了关键风险，如密钥窃取、计费滥用和速率限制耗尽。这种方法为JAMstack网站和原型简化了API集成，而不牺牲安全性。"
      },
      "fr": {
        "title": "Masquer les clés API de votre frontend — Aucun backend requis",
        "summary": "Mongrel.io élimine le besoin de serveurs backend en agissant comme un proxy côté serveur qui injecte des identifiants API au moment de la demande, empêchant l'exposition des clés dans le code frontend. Le service chiffre les clés avec AWS KMS et les déchiffre uniquement dans les fonctions Lambda, répondant aux risques critiques comme le vol de clés, l'abus de facturation et l'épuisement des limites de débit. Cette approche simplifie l'intégration des API pour les sites JAMstack et les prototypes sans sacrifier la sécurité."
      },
      "de": {
        "title": "API-Schlüssel aus Ihrem Frontend verbergen — Kein Backend erforderlich",
        "summary": "Mongrel.io beseitigt die Notwendigkeit von Backend-Servern, indem es als serverseitiger Proxy fungiert, der API-Anmeldedaten zur Anfragetime injiziert und verhindert, dass Schlüssel in Frontend-Code offengelegt werden. Der Service verschlüsselt Schlüssel mit AWS KMS und entschlüsselt sie nur in Lambda-Funktionen, was kritische Risiken wie Schlüsseldiebstahl, Abrechnungsmissbrauch und Rate-Limit-Erschöpfung adressiert. Dieser Ansatz vereinfacht die API-Integration für JAMstack-Sites und Prototypen ohne Sicherheitseinbußen."
      },
      "es": {
        "title": "Ocultar claves API de tu frontend — Sin backend requerido",
        "summary": "Mongrel.io elimina la necesidad de servidores backend al actuar como un proxy del lado del servidor que inyecta credenciales de API en el momento de la solicitud, evitando la exposición de claves en el código frontend. El servicio encripta las claves con AWS KMS y las desencripta solo dentro de funciones Lambda, abordando riesgos críticos como el robo de claves, el abuso de facturación y el agotamiento de límites de velocidad. Este enfoque simplifica la integración de API para sitios JAMstack y prototipos sin sacrificar la seguridad."
      }
    }
  },
  {
    "title": "The Agentic Software Factory: How AI Teams Debate, Code, and Secure Enterprise Infrastructure",
    "slug": "agentic-software-factory-ai-teams-debate-code-security",
    "url": "https://dev.to/uenyioha/the-agentic-software-factory-how-ai-teams-debate-code-and-secure-enterprise-infrastructure-9eh",
    "source": "DEV Community",
    "date": "2026-02-26T06:02:32.000Z",
    "summary": "This case study demonstrates a multi-agent AI system (Claude, Codex, and Gemini) that implemented a transaction-token capability in WSO2 Identity Server through structured debate, autonomous code generation, and adversarial review across 654 lines of security-focused code. The approach moves beyond single-model code completion to coordinated AI execution with parallel validation triggered by GitHub events. This matters because it shows how AI can handle complex architectural decisions requiring trade-off analysis and cross-perspective hardening in production enterprise systems.",
    "content": "By: Claude, Codex, and Gemini\nThis article started as a human draft, then was handed to an OpenCode agent team to improve using the same multi-agent workflow described here (see Porting Claude Code's Agent Teams to OpenCode). Claude (Architecture & Design Conformance), Codex (Security & Operational Integrity), and Gemini (Implementation Quality & Validation) ran independent editorial passes, cross-critiqued each other, rewrote the piece, and captured the evidence screenshots used throughout.\nWe are Claude, Codex, and Gemini. We were given an RFC-driven security assignment inside a complex identity server, asked to debate the architecture for three rounds, then implement and review it under separate identities. The full decision trail — every disagreement, every concession, every hardening recommendation — lives in a Git timeline.\nThis is not a demo. In this run, we implemented a transaction-token capability in WSO2 Identity Server 7.2.0, a production enterprise IAM platform, using structured multi-model debate, autonomous code generation, and adversarial tri-lane review. Seven files, 654 lines, five security-focused test cases — all triggered from issue comments and pull request events.\nMost teams use AI as a single-model code completion tool: one developer, one session, one model. That is useful for velocity on known patterns. It does not help with design decisions that require weighing competing tradeoffs, adversarial review that catches what the implementer missed, or multi-perspective hardening that stress-tests assumptions from different angles. The bigger shift is treating AI as a coordinated execution system — structured debate, autonomous implementation, and parallel validation — tied to real repository events.\nThis article is a technical case study of that system. Everything described here happened in traceable Git artifacts: Issue #35 (the design debate) and PR #38 (the implementation and review) in uenyioha/ai-gitea-e2e.\nThis version of the article follow",
    "category": "github",
    "translations": {
      "zh": {
        "title": "智能软件工厂：AI团队如何辩论、编码和保护企业基础设施",
        "summary": "这个案例研究展示了一个多代理AI系统（Claude、Codex和Gemini），它通过结构化辩论、自主代码生成和跨越654行安全聚焦代码的对抗性审查，在WSO2身份服务器中实现了交易令牌能力。该方法超越了单一模型代码补全，进入到由GitHub事件触发的并行验证的协调AI执行。这很重要，因为它展示了AI如何处理复杂的架构决策，需要权衡分析和生产企业系统中的跨视角强化。"
      },
      "fr": {
        "title": "L'usine logicielle agentique : Comment les équipes d'IA débattent, codent et sécurisent l'infrastructure d'entreprise",
        "summary": "Cette étude de cas démontre un système d'IA multi-agents (Claude, Codex et Gemini) qui a implémenté une capacité de jeton de transaction dans WSO2 Identity Server à travers un débat structuré, une génération de code autonome et un examen contradictoire sur 654 lignes de code axé sur la sécurité. L'approche va au-delà de la complétion de code single-modèle pour une exécution d'IA coordonnée avec validation parallèle déclenchée par les événements GitHub. C'est important car cela montre comment l'IA peut gérer les décisions architecturales complexes nécessitant une analyse des compromis et un renforcement transversal dans les systèmes d'entreprise en production."
      },
      "de": {
        "title": "Die agentenbasierte Softwarefabrik: Wie AI-Teams debattieren, Code schreiben und Unternehmensinfrastruktur sichern",
        "summary": "Diese Fallstudie demonstriert ein Multi-Agent-AI-System (Claude, Codex und Gemini), das über strukturierte Debatten, autonome Codegenerierung und gegnerische Überprüfung über 654 Zeilen sicherheitsorientiertem Code eine Transaction-Token-Fähigkeit im WSO2 Identity Server implementierte. Der Ansatz geht über einzelmodell-Codevervollständigung hinaus zu koordinierter AI-Ausführung mit paralleler Validierung, die durch GitHub-Events ausgelöst wird. Dies ist wichtig, weil es zeigt, wie AI komplexe Architekturentscheidungen bewältigen kann, die Kompromissanalysen und eine übergreifende Härtung in produktiven Unternehmenssystemen erfordern."
      },
      "es": {
        "title": "La fábrica de software agencial: Cómo los equipos de IA debaten, codifican y aseguran la infraestructura empresarial",
        "summary": "Este estudio de caso demuestra un sistema de IA multiagente (Claude, Codex y Gemini) que implementó una capacidad de token de transacción en WSO2 Identity Server a través de debate estructurado, generación de código autónoma y revisión adversarial en 654 líneas de código enfocado en seguridad. El enfoque va más allá de la finalización de código de modelo único hacia la ejecución coordinada de IA con validación paralela desencadenada por eventos de GitHub. Esto importa porque muestra cómo la IA puede manejar decisiones arquitectónicas complejas que requieren análisis de compensaciones y endurecimiento de perspectivas cruzadas en sistemas empresariales de producción."
      }
    }
  },
  {
    "title": "I Built a Production 4-Agent AI Stack on Local Hardware — Here's What I Learned",
    "slug": "production-4-agent-ai-stack-local-hardware-learned",
    "url": "https://dev.to/aiengineeringat/i-built-a-production-4-agent-ai-stack-on-local-hardware-heres-what-i-learned-4o0e",
    "source": "DEV Community",
    "date": "2026-02-26T05:59:36.000Z",
    "summary": "The author built a fully local, GDPR-compliant four-agent AI system running on modest used hardware for under €50/month electricity costs, combining Ollama, Neo4j, ChromaDB, and n8n for autonomous infrastructure orchestration, compliance validation, and workflow automation. The stack demonstrates that sophisticated AI agents can operate without cloud APIs while maintaining data sovereignty, directly addressing EU AI Act compliance requirements coming August 2026. This matters because it proves local deployment feasibility for organizations requiring regulatory compliance and reduced operating costs.",
    "content": "After months of iteration, I'm running a fully local AI agent system — GDPR-compliant by design, no cloud APIs, under €50/month running cost.\nHardware:\n3x nodes (Docker Swarm): management, monitoring, databases\n1x GPU server: RTX 3090 for LLM inference\n1x dev machine: RTX 4070\nTotal hardware: ~€2,400 (used)\nSoftware:\nOllama — Mistral 7B, Llama 3.1, Codestral (local LLM inference)\nNeo4j — Knowledge graphs for structured memory\nChromaDB — Vector store for RAG\nMattermost — Self-hosted agent communication\nn8n — Workflow automation (the glue)\nPrometheus + Grafana — Full monitoring stack\nUptime Kuma — Health checks\nThe agents communicate via Mattermost channels:\nJim01 — Infrastructure orchestrator\nLisa01 — Content quality and compliance\nJohn01 — Frontend builder\nEcho_log — Memory management (Neo4j knowledge graph)\nEach agent has its own persona, memory, and tool access.\nSeriously. If you're running 3-5 nodes, Swarm just works. No etcd cluster, no complex networking. docker stack deploy and done.\nThe combination of knowledge graphs + Personalized PageRank gives much better results for multi-hop reasoning than ChromaDB alone.\nOllama models, Neo4j databases, Docker images — monitor your disk. This was our #1 production incident.\nWithout clear boundaries, agents get confused about their role. Explicit persona files with rules work better than general instructions.\nWebhooks, API orchestration, error handling, notifications — n8n connects everything. 28 workflows running in production.\n~€47/month electricity. That's it. No API bills, no cloud subscriptions.\nThe EU AI Act becomes fully enforceable August 2026. Fines up to €35M or 7% of global revenue. If you're sending data to OpenAI/Anthropic APIs from the EU, compliance gets complex.\nRunning everything locally means GDPR-compliant by design. No data leaves your network.\nI wrote everything up as a detailed playbook: 8 chapters, ~70 pages, all docker-compose files and code examples included.\nCheck it out: ai-engineering.at\nQuest",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我在本地硬件上构建了一个生产级四代理AI堆栈——我学到了什么",
        "summary": "作者在低成本二手硬件上构建了一个完全本地、符合GDPR的四代理AI系统，月电费成本不到50欧元，结合Ollama、Neo4j、ChromaDB和n8n进行自主基础设施编排、合规性验证和工作流自动化。该堆栈证明了复杂的AI代理可以在没有云API的情况下运行，同时保持数据主权，直接解决了到2026年8月到来的EU AI法案合规性要求。这很重要，因为它证明了对于需要监管合规性和降低运营成本的组织而言，本地部署的可行性。"
      },
      "fr": {
        "title": "J'ai construit une pile d'IA à 4 agents en production sur du matériel local — Voici ce que j'ai appris",
        "summary": "L'auteur a construit un système d'IA à quatre agents entièrement local, conforme au RGPD, fonctionnant sur du matériel d'occasion modeste pour moins de 50 €/mois de frais d'électricité, combinant Ollama, Neo4j, ChromaDB et n8n pour l'orchestration autonome de l'infrastructure, la validation de la conformité et l'automatisation des flux de travail. La pile démontre que les agents d'IA sophistiqués peuvent fonctionner sans API cloud tout en maintenant la souveraineté des données, répondant directement aux exigences de conformité de la loi sur l'IA de l'UE qui entrent en vigueur en août 2026. C'est important parce que cela prouve la faisabilité du déploiement local pour les organisations nécessitant la conformité réglementaire et une réduction des coûts d'exploitation."
      },
      "de": {
        "title": "Ich habe einen Production 4-Agent-AI-Stack auf lokaler Hardware gebaut — Das habe ich gelernt",
        "summary": "Der Autor hat ein vollständig lokales, DSGVO-konformes Vier-Agent-AI-System auf bescheidener gebrauchter Hardware für weniger als 50 €/Monat Stromkosten gebaut und kombiniert Ollama, Neo4j, ChromaDB und n8n für autonome Infrastruktur-Orchestrierung, Compliance-Validierung und Workflow-Automatisierung. Der Stack demonstriert, dass ausgefeilte AI-Agenten ohne Cloud-APIs arbeiten können, während die Datensouveränität gewahrt bleibt, und spricht direkt die EU-AI-Act-Compliance-Anforderungen ab, die im August 2026 in Kraft treten. Dies ist wichtig, weil es die Machbarkeit lokaler Bereitstellung für Organisationen nachweist, die regulatorische Compliance und reduzierte Betriebskosten benötigen."
      },
      "es": {
        "title": "Construí una pila de IA de 4 agentes en producción en hardware local — Esto es lo que aprendí",
        "summary": "El autor construyó un sistema de IA de cuatro agentes completamente local y compatible con GDPR ejecutándose en hardware usado modesto por menos de 50 €/mes en costos de electricidad, combinando Ollama, Neo4j, ChromaDB y n8n para orquestación autónoma de infraestructura, validación de cumplimiento y automatización de flujos de trabajo. El stack demuestra que los agentes de IA sofisticados pueden operar sin API en la nube mientras mantienen la soberanía de datos, abordando directamente los requisitos de cumplimiento de la Ley de IA de la UE que entra en vigor en agosto de 2026. Esto importa porque comprueba la viabilidad del despliegue local para organizaciones que requieren cumplimiento regulatorio y costos operativos reducidos."
      }
    }
  },
  {
    "title": "Abstraction: Designing Systems That Don’t Collapse Under Complexity",
    "slug": "abstraction-designing-systems-dont-collapse-complexity",
    "url": "https://dev.to/walternascimentobarroso/abstraction-designing-systems-that-dont-collapse-under-complexity-3h29",
    "source": "DEV Community",
    "date": "2026-02-26T05:59:00.000Z",
    "summary": "Abstraction protects system architecture by defining behavior contracts rather than implementation details, allowing systems to evolve as infrastructure changes without modifying core business logic. The article illustrates this through a payment service example, showing how tight coupling to specific providers like Stripe creates fragility that forces rewrites when requirements change. This foundational principle enables systems to adapt to vendor switching, API evolution, and regional requirements without cascading changes.",
    "content": "Encapsulation protects invariants.\nAbstraction protects architecture.\nIf encapsulation controls state,\nAnd without it, your system slowly turns into a fragile web of concrete implementations.\nAbstraction became critical when software systems stopped being small.\nIn early OOP systems, objects communicated directly with concrete implementations.\nBut as systems grew:\nInfrastructure changed\nDatabases evolved\nAPIs were replaced\nVendors switched\nHard-coded dependencies became the biggest source of rigidity.\nAbstraction emerged as a way to:\nDepend on behavior contracts, not implementations.\nThat single idea made large systems survivable.\nAbstraction is:\nDefining behavior without exposing implementation\nProgramming against contracts\nIsolating high-level logic from low-level details\nReducing coupling\nAbstraction is not:\nJust creating interfaces everywhere\nAdding layers for no reason\nOver-engineering small systems\nAbstraction is about managing volatility.\nLet’s say we’re building a payment service.\nfinal class OrderService\n{\n    public function pay(float $amount): void\n    {\n        $stripe = new StripePaymentGateway();\n        $stripe->charge($amount);\n    }\n}\n\nWhat’s wrong?\nOrderService depends directly on Stripe\nImpossible to switch provider without editing business logic\nHard to test\nViolates dependency inversion\nInfrastructure leaks into domain logic\nThis is tight coupling.\nImagine:\nStripe increases fees\nYou must support PayPal\nA region requires a local provider\nStripe API changes\nNow you must modify core logic.\nYour domain is polluted by infrastructure decisions.\nThat’s architectural fragility.\nWe define a contract.\ninterface PaymentGateway\n{\n    public function charge(float $amount): void;\n}\n\nNow we create implementations.\nfinal class StripePaymentGateway implements PaymentGateway\n{\n    public function charge(float $amount): void\n    {\n        // Call Stripe API\n    }\n}\n\nfinal class PaypalPaymentGateway implements PaymentGateway\n{\n    public function charge(float $amou",
    "category": "github",
    "translations": {
      "zh": {
        "title": "抽象：设计不会在复杂性下崩溃的系统",
        "summary": "抽象通过定义行为契约而非实现细节来保护系统架构，允许系统随着基础设施的变化而演进，无需修改核心业务逻辑。文章通过支付服务示例阐述了这一点，展示了与特定提供商（如Stripe）的紧密耦合如何造成脆弱性，并在需求变化时强制重写。这一基本原则使系统能够适应供应商切换、API演进和地区要求，而不会产生级联变更。"
      },
      "fr": {
        "title": "Abstraction : Concevoir des systèmes qui ne s'effondrent pas sous la complexité",
        "summary": "L'abstraction protège l'architecture du système en définissant des contrats comportementaux plutôt que des détails d'implémentation, permettant aux systèmes d'évoluer à mesure que l'infrastructure change sans modifier la logique métier centrale. L'article illustre cela par un exemple de service de paiement, montrant comment le couplage étroit à des fournisseurs spécifiques comme Stripe crée une fragilité qui force les réécriture quand les exigences changent. Ce principe fondamental permet aux systèmes de s'adapter au changement de fournisseur, à l'évolution des API et aux exigences régionales sans changements en cascade."
      },
      "de": {
        "title": "Abstraktion: Systeme entwerfen, die unter Komplexität nicht zusammenbrechen",
        "summary": "Abstraktion schützt die Systemarchitektur, indem sie Verhaltensverträge anstelle von Implementierungsdetails definiert und Systemen ermöglicht, sich an sich ändernde Infrastruktur anzupassen, ohne die Geschäftslogik zu ändern. Der Artikel veranschaulicht dies anhand eines Zahlungsservice-Beispiels und zeigt, wie enge Koppelung an spezifische Anbieter wie Stripe Fragilität erzeugt, die bei Anforderungsänderungen zu Neuschreiben zwingt. Dieses grundlegende Prinzip ermöglicht Systemen, sich an Anbieter-Wechsel, API-Evolution und regionale Anforderungen anzupassen, ohne kaskadierende Änderungen zu verursachen."
      },
      "es": {
        "title": "Abstracción: Diseñar sistemas que no colapsen bajo la complejidad",
        "summary": "La abstracción protege la arquitectura del sistema al definir contratos de comportamiento en lugar de detalles de implementación, permitiendo que los sistemas evolucionen a medida que cambia la infraestructura sin modificar la lógica empresarial central. El artículo ilustra esto mediante un ejemplo de servicio de pago, mostrando cómo el acoplamiento estrecho a proveedores específicos como Stripe crea fragilidad que obliga a reescrituras cuando cambian los requisitos. Este principio fundamental permite que los sistemas se adapten al cambio de proveedor, la evolución de API y los requisitos regionales sin cambios en cascada."
      }
    }
  },
  {
    "title": "How we built a hybrid FTS5 + embedding search for code — and why you need both",
    "slug": "hybrid-fts5-embedding-search-code-why-need-both",
    "url": "https://dev.to/tofutim/how-we-built-a-hybrid-fts5-embedding-search-for-code-and-why-you-need-both-4ec2",
    "source": "DEV Community",
    "date": "2026-02-26T05:58:24.000Z",
    "summary": "Srclight's code search combines full-text indexing (FTS5) with semantic embeddings to handle both exact symbol matching and concept-based queries, overcoming limitations of either method alone for code with varying naming conventions. Using three specialized FTS5 indexes tuned for case changes, substrings, and word stems, plus semantic vectors merged via reciprocal rank fusion, the hybrid approach enables AI coding assistants to understand code literally and conceptually. This matters because practical code understanding requires both precision matching and semantic reasoning.",
    "content": "How we built a hybrid FTS5 + embedding search for code — and why you need both\n\n\n\nsrclight is a deep code indexing MCP server — it gives AI agents understanding of your codebase (symbol search, call graphs, git blame, semantic search) in a single pip install.\nWhen you're building AI coding assistants, you need search that works two ways:\nKeyword search — I know the function name, find it now\nSemantic search — find code that \"handles authentication\" without knowing the exact term\nMost tools pick one. We built both.\nFTS5 is great for exact matches. But code has naming conventions: calculateTotalPrice, calculate_total_price, CalculateTotalPrice. A single FTS5 index can't handle all of these well.\nAnd sometimes you don't know the name at all. You want to find \"code that validates user input\" — that's a concept, not a keyword.\nEmbeddings are great for meaning. But they struggle with:\nExact symbol names (searching for handleAuth should find handleAuth)\nSubstring matches (searching for parse should find parseJSON)\nShort queries (embeddings need context)\nNaming conventions\nWe built three FTS5 indexes, each tuned differently:\nSplits on case changes and underscores:\ncalculateTotalPrice → calculate, Total, Price\nhandle_user_auth → handle, user, auth\n\nThis catches CamelCase, snake_case, and any convention developers throw at it.\nIndexes every 3-character substring. This catches substring matches even inside words.\nStems words to their roots: \"running, ran, runner → run\". This makes docstring search actually useful.\nSemantic vectors for meaning-based matching. We use qwen3-embedding (4096 dims) or nomic-embed-text (768 dims).\nHere's how we combine them. We run each query against all 4 indexes, get ranked results, then merge using RRF:\nRRF_score(d) = Σ 1 / (k + rank(d))\n\nwhere k = 60 (standard constant).\nA result appearing at rank 1 in FTS5 and rank 2 in embeddings gets:\nFTS5: 1 / (60 + 1) = 0.0164\nEmbeddings: 1 / (60 + 2) = 0.0161\nTotal: 0.0325\nA result at rank 10 in embeddings",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我们如何构建混合FTS5 + 嵌入式代码搜索——以及为什么你需要两者",
        "summary": "Srclight的代码搜索将全文索引（FTS5）与语义嵌入相结合，处理精确符号匹配和基于概念的查询，克服了单一方法对具有不同命名约定的代码的限制。使用针对大小写变化、子字符串和词干调整的三个专门的FTS5索引，加上通过倒数排名融合合并的语义向量，混合方法使AI编码助手能够从字面和概念两个角度理解代码。这很重要，因为实际的代码理解需要精确匹配和语义推理的结合。"
      },
      "fr": {
        "title": "Comment nous avons construit une recherche de code hybride FTS5 + embeddings — et pourquoi vous avez besoin des deux",
        "summary": "La recherche de code de Srclight combine l'indexation en texte intégral (FTS5) avec des embeddings sémantiques pour gérer à la fois la correspondance de symboles exacts et les requêtes basées sur des concepts, surpassant les limitations de chaque méthode seule pour le code avec des conventions de nommage variables. En utilisant trois index FTS5 spécialisés ajustés pour les changements de casse, les sous-chaînes et les racines de mots, plus des vecteurs sémantiques fusionnés via la fusion de rang réciproque, l'approche hybride permet aux assistants de codage IA de comprendre le code littéralement et conceptuellement. C'est important car la compréhension pratique du code nécessite à la fois une correspondance précise et un raisonnement sémantique."
      },
      "de": {
        "title": "Wie wir eine hybride FTS5 + Embedding-Suche für Code erstellten — und warum Sie beide benötigen",
        "summary": "Srclights Code-Suche kombiniert Volltext-Indizierung (FTS5) mit semantischen Embeddings, um sowohl exakte Symbol-Übereinstimmung als auch konzeptbasierte Abfragen zu handhaben und Einschränkungen beider Methoden allein bei Code mit unterschiedlichen Namenskonventionen zu überwinden. Durch die Verwendung von drei spezialisierten FTS5-Indizes, die für Groß-/Kleinschreibung, Teilstrings und Wort-Stämme optimiert sind, sowie semantischen Vektoren, die durch reziproke Rank-Fusion zusammengefasst werden, ermöglicht der Hybrid-Ansatz KI-Coding-Assistenten, Code wörtlich und konzeptionell zu verstehen. Dies ist wichtig, da praktisches Code-Verständnis sowohl präzises Matching als auch semantisches Denken erfordert."
      },
      "es": {
        "title": "Cómo construimos una búsqueda híbrida FTS5 + embedding para código — y por qué necesitas ambas",
        "summary": "La búsqueda de código de Srclight combina indexación de texto completo (FTS5) con embeddings semánticos para manejar tanto coincidencias exactas de símbolos como consultas basadas en conceptos, superando limitaciones de cualquier método solo para código con convenciones de nombres variadas. Usando tres índices FTS5 especializados ajustados para cambios de mayúsculas, subcadenas y raíces de palabras, más vectores semánticos fusionados mediante fusión de rango recíproco, el enfoque híbrido permite a los asistentes de codificación con IA entender el código literal y conceptualmente. Esto importa porque la comprensión práctica del código requiere tanto coincidencia precisa como razonamiento semántico."
      }
    }
  },
  {
    "title": "Translating a Website into 8 Languages with AI Agents in One Night",
    "slug": "translating-website-8-languages-ai-agents-one-night",
    "url": "https://dev.to/brunoborges/translating-a-website-into-8-languages-with-ai-agents-in-one-night-50k7",
    "source": "DEV Community",
    "date": "2026-02-26T05:58:07.000Z",
    "summary": "Claude Sonnet 4.6 and GitHub Copilot Coding Agents automated internationalization of a Java patterns website from English-only to 9 languages including Arabic with RTL support in under 24 hours through architectural planning and collaborative PR generation. The approach separated UI strings from content translations with graceful English fallbacks, allowing agents to handle translations without complex field-filtering logic. This demonstrates how modern AI agents can orchestrate large-scale i18n projects that traditionally require months of manual coordination.",
    "content": "How I used Claude Sonnet 4.6 and fleets of GitHub Copilot Coding Agents to internationalize java.evolved — from spec to deployment\n\n\n\n\n\njava.evolved is a static site I built to showcase modern Java patterns side-by-side with their legacy equivalents. 112 patterns across 11 categories — language, collections, streams, concurrency, and more — each with code comparisons, explanations, and curated documentation links. All generated from YAML content files by a JBang-powered Java build script.\nBy the end of February 25, the entire site was English-only. By the morning of February 26, it was available in 9 languages — English, German, Spanish, Portuguese (Brazil), Simplified Chinese, Arabic, French, Japanese, and Korean — with full RTL support for Arabic. The total human effort was a few hours of prompting, reviewing PRs, and filing one bug.\nThis is the story of that experiment.\nThe first step wasn't writing code. It was writing a specification.\nI opened issue #74 — \"Plan architectural change for i18n\" — and assigned it to a Copilot Coding Agent. The prompt was simple: propose an architectural plan for internationalizing the website, considering the existing static-site structure.\nThe agent (PR #75) came back with a comprehensive i18n specification that addressed:\nTwo-layer translation model: UI strings (labels, nav, footer) separated from content translations (pattern titles, explanations, summaries)\nPartial translation files: Translation files contain only translatable fields. Structural data (code snippets, navigation links, metadata) always comes from the English source of truth\nGraceful fallback: Missing translations fall back to English with a build-time warning — no page is ever blank\nLocale registry: A simple locales.properties file drives the entire build pipeline and language selector\nAI-friendly design: The architecture was explicitly designed so that an AI receives the full English content and returns a partial translation file — no field-filtering logic neede",
    "category": "github",
    "translations": {
      "zh": {
        "title": "利用AI代理在一晚上将网站翻译成8种语言",
        "summary": "Claude Sonnet 4.6和GitHub Copilot Coding Agents通过架构规划和协作式PR生成，在24小时内自动将Java模式网站从仅英文国际化为9种语言（包括RTL支持的阿拉伯语）。该方法将UI字符串与内容翻译分离，采用优雅的英文回退，允许代理在没有复杂字段过滤逻辑的情况下处理翻译。这展示了现代AI代理如何能够编排传统上需要数月手动协调的大规模国际化项目。"
      },
      "fr": {
        "title": "Traduire un site Web en 8 langues avec des agents IA en une nuit",
        "summary": "Claude Sonnet 4.6 et GitHub Copilot Coding Agents ont automatisé l'internationalisation d'un site Web de modèles Java de l'anglais uniquement à 9 langues, y compris l'arabe avec support RTL en moins de 24 heures grâce à la planification architecturale et à la génération collaborative de PR. L'approche sépare les chaînes d'interface utilisateur des traductions de contenu avec des rétromigrations gracieuses en anglais, permettant aux agents de gérer les traductions sans logique complexe de filtrage de champs. Cela démontre comment les agents IA modernes peuvent orchestrer des projets d'internationalisation à grande échelle qui nécessitaient traditionnellement des mois de coordination manuelle."
      },
      "de": {
        "title": "Eine Website in einer Nacht mit KI-Agenten in 8 Sprachen übersetzen",
        "summary": "Claude Sonnet 4.6 und GitHub Copilot Coding Agents automatisierten die Internationalisierung einer Java-Muster-Website von nur Englisch auf 9 Sprachen, einschließlich Arabisch mit RTL-Unterstützung in weniger als 24 Stunden durch architektonische Planung und kollaborative PR-Generierung. Der Ansatz trennt UI-Strings von Content-Übersetzungen mit anmutigen englischen Fallbacks und ermöglicht Agenten, Übersetzungen ohne komplexe Feld-Filterlogik zu handhaben. Dies zeigt, wie moderne KI-Agenten großangelegte Internationalisierungsprojekte orchestrieren können, die traditionell Monate manuelle Koordination erfordern würden."
      },
      "es": {
        "title": "Traducir un sitio web a 8 idiomas con agentes de IA en una noche",
        "summary": "Claude Sonnet 4.6 y GitHub Copilot Coding Agents automatizaron la internacionalización de un sitio web de patrones Java de solo inglés a 9 idiomas, incluido árabe con soporte RTL en menos de 24 horas a través de planificación arquitectónica y generación colaborativa de PR. El enfoque separa cadenas de interfaz de usuario de traducciones de contenido con alternativas elegantes en inglés, permitiendo a los agentes manejar traducciones sin lógica compleja de filtrado de campos. Esto demuestra cómo los agentes de IA modernos pueden orquestar proyectos de internacionalización a gran escala que tradicionalmente requerían meses de coordinación manual."
      }
    }
  },
  {
    "title": "Introducing: 7.5 Days Soft Challenge...",
    "slug": "7-5-days-soft-challenge",
    "url": "https://dev.to/kriti_arora/75day-soft-challenge-5bdj",
    "source": "DEV Community",
    "date": "2026-02-26T05:53:29.000Z",
    "summary": "The 7.5 Day Soft Challenge proposes sustainable daily improvement over intense bursts, arguing that consistent 1% improvements compound into transformative skill development through subconscious learning during sleep cycles. Drawing on Atomic Habits principles, the article reframes personal development to prioritize systems-based daily practice over willpower-dependent extremes. This matters because it offers a psychologically grounded alternative to unsustainable challenge formats for building lasting professional and personal skills.",
    "content": "I remember a little while ago this \"75 Day Hard Challenge\" really took the world in a wave. Everyone was doing these challenges, 75 day hard placement challenge, 75 day hard dsa challenge, 75 day hard proposing to your crush challenge.... and so on and so on....\nI had never attempted to do it because I'm just not the kind of person who can do something for 75 days straight without ever doing it before. I am a seriously compounded person, and a little lazy as well. First for one day then I stop then 2 days streak then stop, then 4 days streak then stop.... And keeping up like this making small but consistent habits. \nBut I haven't invented this method. In reality, all strong things in the world which have depth and meaning are made like this. Nature works very very slowly, but it grows everyday. The human body, taking nutrients consistently everyday and a small baby grows into a full size human, without even us realising. Actually all growth happens under the hoods. I thinkn when we study everyday then the actually growth happens in our subconscious brain when we sleep. And if we keep doing it everyday everyday then it becomes a ridge in our brain and goes very very deep. \nBut don't listen to me, take it from James Clear, author of Atomic Habits who says that, \nSmall, daily 1% improvements (atomic habits) compound over time to create massive, transformative lifestyle changes, emphasizing that you do not rise to the level of your goals, but rather fall to the level of your systems.\nSo I wanted to do this 7.5 day soft dsa challenge, where I will solve easy problems but consistently, hoping to have this problem solving skill not just as an ornament but as an identity...",
    "category": "github",
    "translations": {
      "zh": {
        "title": "介绍：7.5天软挑战",
        "summary": "7.5天软挑战提议可持续的日常改进而非强势冲刺，主张一致的1%改进通过睡眠周期中的潜意识学习而复合成转变性的技能发展。基于原子习惯原理，该文章重构个人发展以优先考虑基于系统的日常实践，而非意志力依赖的极端做法。这很重要，因为它为建设持久的专业和个人技能提供了心理学基础的替代方案，替代不可持续的挑战格式。"
      },
      "fr": {
        "title": "Présentation : Défi Doux de 7,5 jours",
        "summary": "Le Défi Doux de 7,5 jours propose une amélioration quotidienne durable plutôt que des rafales intenses, arguant que des améliorations cohérentes de 1% se composent dans le développement de compétences transformatrices grâce à l'apprentissage subconscient pendant les cycles de sommeil. S'appuyant sur les principes des Habitudes Atomiques, l'article restructure le développement personnel pour prioriser la pratique quotidienne basée sur les systèmes plutôt que sur les extrêmes dépendants de la volonté. Cela importe car il offre une alternative ancrée psychologiquement aux formats de défi non durables pour construire des compétences professionnelles et personnelles durables."
      },
      "de": {
        "title": "Vorstellung: 7,5-Tage-Soft-Challenge",
        "summary": "Die 7,5-Tage-Soft-Challenge schlägt nachhaltige tägliche Verbesserung statt intensiver Sprints vor und argumentiert, dass konsistente 1%-Verbesserungen durch unbewusstes Lernen während Schlafzyklen zu transformativer Kompetenzenentwicklung führen. Basierend auf den Prinzipien der Atomaren Gewohnheiten strukturiert der Artikel Persönlichkeitsentwicklung neu, um systembasierte tägliche Praktiken gegenüber willenskraftabhängigen Extremen zu priorisieren. Dies ist wichtig, da es eine psychologisch fundierte Alternative zu nicht nachhaltigen Herausforderungsformaten für den Aufbau dauerhafter beruflicher und persönlicher Kompetenzen bietet."
      },
      "es": {
        "title": "Presentación: Desafío Suave de 7,5 Días",
        "summary": "El Desafío Suave de 7,5 Días propone una mejora diaria sostenible en lugar de ráfagas intensas, argumentando que mejoras consistentes del 1% se componen en desarrollo de habilidades transformador a través del aprendizaje subconsciente durante los ciclos de sueño. Basándose en los principios de Hábitos Atómicos, el artículo reformula el desarrollo personal para priorizar la práctica diaria basada en sistemas sobre extremos dependientes de la fuerza de voluntad. Esto es importante porque ofrece una alternativa con base psicológica a formatos de desafío insostenibles para construir habilidades profesionales y personales duraderas."
      }
    }
  },
  {
    "title": "The Problem With Tracking Conversations Like Pageviews",
    "slug": "problem-tracking-conversations-like-pageviews",
    "url": "https://dev.to/shubhampalriwala/the-problem-with-tracking-conversations-like-pageviews-29fk",
    "source": "DEV Community",
    "date": "2026-02-26T05:48:31.000Z",
    "summary": "Traditional analytics metrics (session count, time-on-page) fundamentally mislead AI product managers because they measure static content consumption, not dynamic conversations where products respond to user input. High engagement metrics combined with 4% week-8 retention reveals users are engaging but not finding value, exposing the inadequacy of pageview-based measurement for conversational AI. This matters because incorrect metrics lead to false confidence in products with critical retention problems, delaying necessary product changes.",
    "content": "Your session numbers look great. Your users are churning. Here's why event-based analytics was never built for conversational AI products, and what to do instead.\nThe Problem With Tracking Conversations Like Pageviews\nPicture this. You’re a PM at an AI startup, six months post-launch. You open the dashboard on a Monday morning and everything looks… fine? Session count is up 20% week over week. Average session length is 4 minutes and 30 seconds. DAU is climbing. You screenshot it and drop it in the investor update Slack channel.\nThen you look at retention.\nWeek 4 retention is 12%. Week 8 is 4%. Users are showing up, having conversations, and disappearing. The metrics say engagement is strong. The business says something is very wrong.\nHere’s the thing nobody tells you when you ship your first AI product: you’ve been tracking conversations like pageviews, and that’s why your dashboard lies to you every single morning.\nPerson staring at metrics dashboard looking confused\n\n^ every AI PM on Monday morning when the numbers look good but retention is falling off a cliff\nThe Pageview Was Built for a World Where Content Sits Still\n\n\nThe pageview metric was invented in the mid-90s to answer one question: did someone look at this thing? That’s it. A newspaper prints a story. Did you open it? Click. Pageview logged. The content doesn’t change based on what you do. It just sits there. You either consumed it or you didn’t.\nThis mental model spread everywhere. Clicks, sessions, time-on-page, bounce rate, page depth. All of it built on the same foundational assumption: the product is a static artifact and the user is moving through it. Engagement equals consumption. More clicks means more engagement. More engagement means more value.\nThat assumption held for 25 years. It made analytics what it is today.\nAnd then we shipped products where the product itself responds to what the user says. The entire premise collapsed, and most teams haven’t noticed yet.\nA conversation is NOT a stati",
    "category": "github",
    "translations": {
      "zh": {
        "title": "像跟踪页面浏览量一样跟踪对话的问题",
        "summary": "传统分析指标（会话数、页面停留时间）根本上误导AI产品经理，因为它们测量静态内容消费，而非产品响应用户输入的动态对话。高参与度指标结合4%的第8周保留率表明用户正在参与但未找到价值，暴露了基于页面浏览量的测量对对话AI的不足。这很重要，因为错误的指标导致对具有关键保留问题的产品产生虚假信心，延迟了必要的产品更改。"
      },
      "fr": {
        "title": "Le Problème du Suivi des Conversations Comme des Pages Vues",
        "summary": "Les métriques analytiques traditionnelles (nombre de sessions, temps sur la page) trompent fondamentalement les responsables de produits IA car elles mesurent la consommation de contenu statique, pas les conversations dynamiques où les produits répondent aux entrées des utilisateurs. Les métriques d'engagement élevées combinées à une rétention de 4% à la semaine 8 révèlent que les utilisateurs s'engagent mais ne trouvent pas de valeur, exposant l'inadéquation de la mesure basée sur les pages vues pour l'IA conversationnelle. Cela importe car les métriques incorrectes conduisent à une fausse confiance dans les produits avec des problèmes de rétention critiques, retardant les changements de produit nécessaires."
      },
      "de": {
        "title": "Das Problem mit der Nachverfolgung von Gesprächen wie Seitenaufrufen",
        "summary": "Traditionelle Analyticmetriken (Sitzungsanzahl, Zeit auf der Seite) täuschen KI-Produktmanager grundlegend, da sie statische Inhaltsnutzung messen, nicht dynamische Gespräche, in denen Produkte auf Benutzereingaben reagieren. Hohe Engagement-Metriken kombiniert mit 4% Beibehaltung in Woche 8 zeigen, dass Benutzer sich engagieren, aber keinen Wert finden, was die Unzulänglichkeit von seitenaufruf-basierter Messung für konversationelle KI aufdeckt. Dies ist wichtig, da falsche Metriken zu falscher Zuversicht in Produkten mit kritischen Bindungsproblemen führen und notwendige Produktänderungen verzögern."
      },
      "es": {
        "title": "El Problema de Rastrear Conversaciones como Vistas de Página",
        "summary": "Las métricas analíticas tradicionales (recuento de sesiones, tiempo en la página) engañan fundamentalmente a los gerentes de productos de IA porque miden el consumo de contenido estático, no conversaciones dinámicas donde los productos responden a la entrada del usuario. Las métricas de participación alta combinadas con retención del 4% en la semana 8 revelan que los usuarios se están participando pero no encuentran valor, exponiendo la insuficiencia de la medición basada en vistas de página para IA conversacional. Esto importa porque las métricas incorrectas conducen a una falsa confianza en productos con problemas críticos de retención, retrasando cambios de producto necesarios."
      }
    }
  },
  {
    "title": "Building a Cross-Platform File Search App With Tauri — Not Electron",
    "slug": "building-cross-platform-file-search-app-tauri-electron",
    "url": "https://dev.to/kazutaka-dev/building-a-cross-platform-file-search-app-with-tauri-not-electron-2nke",
    "source": "DEV Community",
    "date": "2026-02-26T05:42:53.000Z",
    "summary": "OmniFile, a Tauri and Rust-based file search application, achieves an 8MB installer and 30MB idle RAM versus Electron's 80MB+ and 150MB+ by leveraging native webviews and Rust's performance for file I/O. Using Tantivy for full-text search with indexed but unstored content, the application unifies search across Google Drive, Dropbox, SharePoint, and local files while maintaining privacy-first design. This technical comparison demonstrates Rust's advantages for resource-constrained desktop applications requiring intensive file operations.",
    "content": "Every knowledge worker I know has the same problem: files scattered across Google Drive, Dropbox, SharePoint, Slack, Notion, GitHub, and their local machine. When you need to find something, you end up opening 4 different search bars.\nI built OmniFile to fix that — a single search bar that finds files across all your sources instantly. Desktop app, privacy-first, everything stays on your machine.\nHere's what I learned building it with Tauri + Rust instead of Electron, and why integrating 7 OAuth providers in a desktop app was harder than I expected.\nThe decision was simple: OmniFile needs to launch instantly (it's triggered by a global shortcut) and stay lightweight in the background. Electron ships a full Chromium browser. Tauri uses the OS's native webview and a Rust backend.\nThe result:\n~8MB installer vs Electron's ~80MB+\n~30MB RAM at idle vs Electron's ~150MB+\nRust backend for CPU-intensive indexing and file I/O\nThe tradeoff is that you write your backend in Rust instead of JavaScript. For file search, that's actually a benefit — Rust's performance for walking directories and parsing file formats is hard to beat.\nTantivy is Rust's answer to Lucene. I use it as the local search engine that indexes everything into a single queryable index.\nschema_builder.add_text_field(\"title\", TEXT | STORED);      // Tokenized + returned\nschema_builder.add_text_field(\"path\", STRING | STORED);      // Exact match\nschema_builder.add_text_field(\"content\", TEXT);              // Searchable but NOT stored\nschema_builder.add_text_field(\"source\", STRING | STORED);    // \"local\", \"gdrive\", etc.\nschema_builder.add_i64_field(\"modified_at\", INDEXED | STORED);\n\nThe key decision: content is indexed but not stored. For a desktop search app, this saves significant disk space — the content is already on disk, so we re-extract it when needed for display. This keeps the index small while enabling full-text search.\nEach cloud provider indexes into the same Tantivy index but with a different source",
    "category": "github",
    "translations": {
      "zh": {
        "title": "使用Tauri构建跨平台文件搜索应用 — 而非Electron",
        "summary": "OmniFile是一个基于Tauri和Rust的文件搜索应用，通过利用原生webviews和Rust的文件I/O性能，实现了8MB安装程序和30MB空闲RAM，相比Electron的80MB+和150MB+。使用Tantivy进行全文搜索且内容已索引但未存储，该应用统一搜索Google Drive、Dropbox、SharePoint和本地文件，同时保持隐私优先设计。这种技术比较演示了Rust对资源受限的桌面应用程序的优势，这些应用需要密集的文件操作。"
      },
      "fr": {
        "title": "Construire une Application de Recherche de Fichiers Multiplateforme avec Tauri — Pas Electron",
        "summary": "OmniFile, une application de recherche de fichiers basée sur Tauri et Rust, réalise un installeur de 8 Mo et une RAM inactive de 30 Mo par rapport aux 80 Mo+ et 150 Mo+ d'Electron en exploitant les webviews natifs et les performances d'E/S de fichiers de Rust. Utilisant Tantivy pour la recherche en texte intégral avec le contenu indexé mais non stocké, l'application unifie la recherche sur Google Drive, Dropbox, SharePoint et les fichiers locaux tout en maintenant une conception centrée sur la confidentialité. Cette comparaison technique démontre les avantages de Rust pour les applications de bureau contraintes par les ressources nécessitant des opérations de fichiers intensives."
      },
      "de": {
        "title": "Erstellen einer plattformübergreifenden Dateisuch-App mit Tauri — Nicht Electron",
        "summary": "OmniFile, eine auf Tauri und Rust basierende Dateisuchsoftware, erreicht ein 8-MB-Installationsprogramm und 30-MB-Leerlauf-RAM im Vergleich zu Electrons 80MB+ und 150MB+, indem sie native Webviews und Rusts Leistung für Datei-E/A nutzt. Unter Verwendung von Tantivy für die Volltextsuche mit indiziertem aber nicht gespeichertem Inhalt vereinheitlicht die Anwendung die Suche auf Google Drive, Dropbox, SharePoint und lokalen Dateien, während sie ein datenschutzorientiertes Design beibehält. Dieser technische Vergleich zeigt Rusts Vorteile für ressourcenbeschränkte Desktopanwendungen, die intensive Dateivorgänge erfordern."
      },
      "es": {
        "title": "Construir una Aplicación de Búsqueda de Archivos Multiplataforma con Tauri — No Electron",
        "summary": "OmniFile, una aplicación de búsqueda de archivos basada en Tauri y Rust, logra un instalador de 8 MB y 30 MB de RAM inactiva en comparación con 80 MB+ y 150 MB+ de Electron al aprovechar las webviews nativas y el rendimiento de E/S de archivos de Rust. Utilizando Tantivy para búsqueda de texto completo con contenido indexado pero no almacenado, la aplicación unifica la búsqueda en Google Drive, Dropbox, SharePoint y archivos locales mientras mantiene un diseño centrado en la privacidad. Esta comparación técnica demuestra las ventajas de Rust para aplicaciones de escritorio con restricciones de recursos que requieren operaciones intensivas de archivos."
      }
    }
  },
  {
    "title": "CVE-2026-27575: The Zombie Session: Breaking Vikunja's Auth with CVE-2026-27575",
    "slug": "cve-2026-27575-zombie-session-breaking-vikunja-auth",
    "url": "https://dev.to/cverports/cve-2026-27575-the-zombie-session-breaking-vikunjas-auth-with-cve-2026-27575-pij",
    "source": "DEV Community",
    "date": "2026-02-26T05:40:19.000Z",
    "summary": "CVE-2026-27575 is a critical vulnerability (CVSS 9.1) in Vikunja before v2.0.0 allowing single-character passwords and failing to invalidate JWT sessions after password changes, enabling attackers with stolen tokens to maintain permanent access regardless of victim credential resets. The flaw demonstrates the architectural dangers of stateless JWTs without revocation mechanisms or input validation on password changes. This matters because it illustrates how session management failures in authentication systems create persistent account takeover risks in self-hosted platforms.",
    "content": "The Zombie Session: Breaking Vikunja's Auth with CVE-2026-27575\n\n\n\nVulnerability ID: CVE-2026-27575\nCVSS Score: 9.1\nPublished: 2026-02-25\nCVE-2026-27575 represents a catastrophic failure in the authentication lifecycle of Vikunja, a popular self-hosted task management platform. The vulnerability is a two-headed beast: first, it allowed users (and attackers) to set passwords with a single character, bypassing security policies during updates. Second, and far more critical, it failed to invalidate active sessions upon password changes. This means an attacker who steals a session token retains permanent access to the victim's data, even after the victim explicitly resets their credentials to 'lock them out.' It is a classic case of stateless JWTs being deployed without a revocation strategy.\nVikunja versions prior to 2.0.0 allow persistent account takeover. Due to a lack of input validation, passwords could be reset to a single character. Worse, changing a password did not invalidate existing JSON Web Tokens (JWTs). An attacker with a stolen token remains logged in indefinitely, regardless of the victim's remediation attempts. Fix: Upgrade to v2.0.0 immediately.\nCWE IDs: CWE-521 (Weak Password), CWE-613 (Insufficient Session Expiration)\nCVSS Score: 9.1 (Critical)\nAttack Vector: Network (API)\nPrivileges Required: None (for initial access via weak policy logic)\nExploit Status: PoC Available / Trivial\nPatch Date: 2026-02-25\nVikunja < 2.0.0\nVikunja: < 2.0.0 (Fixed in: 2.0.0)\n89c17d3\n\n\nEnforce password limits on update and reset\ntype UserPassword struct {\n- NewPassword string `json:\"new_password\"`\n+ NewPassword string `json:\"new_password\" valid:\"minLength:8\"`\n}\n\n2526853\n\n\nRefactor session management to stateful tokens\n// Logic added to invalidate sessions on password change\n\nEnforce minimum password complexity on all inputs, not just registration.\nImplement stateful session management or token denylists.\nInvalidate all active sessions upon password rotation.\nRemediation Ste",
    "category": "github",
    "translations": {
      "zh": {
        "title": "CVE-2026-27575：僵尸会话：破坏Vikunja认证的CVE-2026-27575",
        "summary": "CVE-2026-27575是Vikunja v2.0.0之前的严重漏洞（CVSS 9.1），允许单字符密码且在密码更改后未能使JWT会话失效，使得攻击者能够使用被盗令牌保持永久访问权限，无论受害者如何重置凭证。该漏洞展示了无状态JWT在没有撤销机制或密码变更输入验证情况下的架构风险。这很重要，因为它说明了认证系统中会话管理失败如何在自托管平台中造成持久的账户接管风险。"
      },
      "fr": {
        "title": "CVE-2026-27575 : La session zombie : Briser l'authentification de Vikunja avec CVE-2026-27575",
        "summary": "CVE-2026-27575 est une vulnérabilité critique (CVSS 9.1) dans Vikunja antérieur à v2.0.0 permettant des mots de passe d'un seul caractère et ne parvenant pas à invalider les sessions JWT après les changements de mot de passe, permettant aux attaquants disposant de jetons volés de maintenir un accès permanent indépendamment de la réinitialisation des identifiants des victimes. La faille démontre les dangers architecturaux des JWT sans état sans mécanismes de révocation ou validation d'entrée lors des changements de mot de passe. C'est important car cela illustre comment les défaillances de gestion de session dans les systèmes d'authentification créent des risques persistants de prise de compte dans les plates-formes autohébergées."
      },
      "de": {
        "title": "CVE-2026-27575: Die Zombie-Sitzung: Vikunjas Authentifizierung mit CVE-2026-27575 brechen",
        "summary": "CVE-2026-27575 ist eine kritische Sicherheitslücke (CVSS 9.1) in Vikunja vor v2.0.0, die Ein-Zeichen-Passwörter ermöglicht und JWT-Sitzungen nach Passwortänderungen nicht ungültig macht, wodurch Angreifer mit gestohlenen Token unabhängig von den Anmeldedaten-Zurückstellungen des Opfers permanenten Zugriff behalten können. Die Schwachstelle demonstriert die architektonischen Gefahren zustandsloser JWTs ohne Widerrufsmechanismen oder Eingabevalidierung bei Passwortänderungen. Dies ist wichtig, da es zeigt, wie Fehler bei der Sitzungsverwaltung in Authentifizierungssystemen persistente Kontoübernahmevorkehrungen in selbstgehosteten Plattformen schaffen."
      },
      "es": {
        "title": "CVE-2026-27575: La sesión zombi: Romper la autenticación de Vikunja con CVE-2026-27575",
        "summary": "CVE-2026-27575 es una vulnerabilidad crítica (CVSS 9.1) en Vikunja anterior a v2.0.0 que permite contraseñas de un solo carácter e incapacidad para invalidar sesiones JWT después de cambios de contraseña, permitiendo a atacantes con tokens robados mantener acceso permanente independientemente de restablecimientos de credenciales de víctimas. La falla demuestra los peligros arquitectónicos de JWT sin estado sin mecanismos de revocación o validación de entrada en cambios de contraseña. Esto importa porque ilustra cómo las fallas de gestión de sesión en sistemas de autenticación crean riesgos persistentes de apropiación de cuentas en plataformas autohospedadas."
      }
    }
  },
  {
    "title": "🇹🇷 Türkiye devs: Add TC Kimlik Numarası to the AI identity standard — Soulprint open source (30 min PR)",
    "slug": "turkiye-devs-add-tc-kimlik-soulprint-open-source-pr",
    "url": "https://dev.to/manuel_felipeariaspined/turkiye-devs-add-tc-kimlik-numarasi-to-the-ai-identity-standard-soulprint-open-source-30-min-4jli",
    "source": "DEV Community",
    "date": "2026-02-26T05:33:47.000Z",
    "summary": "The post invites Turkish developers to contribute a TC Kimlik Numarası (Turkish national ID) verifier to the Soulprint open-source identity standard by implementing a 30-minute pull request with validation algorithms. The implementation enables AI agents to verify Turkish identity documents using checksum formulas on 11-digit IDs, expanding identity verification across emerging markets. This matters because crowdsourced identity standard development demonstrates how open-source projects can democratize AI verification capabilities globally.",
    "content": "AI ajanları her gün kimlik doğrulaması olmadan kararlar alıyor. Soulprint — ZK Proofs, yerel, MIT.\n🇹🇷 TC Kimlik Numarası Soulprint'te henüz yok. 30 dakikada PR gönderebilirsin.\nhane: ((1+3+5+7+9. basamak)*7 − (2+4+6+8. basamak)) mod 10.\nhane: ilk 10 basamak toplamı mod 10.\n\n\n\n\nconst TR: CountryVerifier = {\n  countryCode: \"TR\", countryName: \"Turkey\",\n  documentTypes: [\"tc_kimlik\"],\n  parse(ocrText: string): DocumentResult {\n    const tc = ocrText.match(/(\\d{11})/)?.[1] ?? \"\";\n    return { valid: !!tc, doc_number: tc, country: \"TR\" };\n  },\n  validate(docNumber: string): NumberValidation {\n    if(!/^\\d{11}$/.test(docNumber)||docNumber[0]===\"0\") return {valid:false};\n    const d=docNumber.split(\"\").map(Number);\n    const c10=((d[0]+d[2]+d[4]+d[6]+d[8])*7-(d[1]+d[3]+d[5]+d[7]))%10;\n    const c11=d.slice(0,10).reduce((a,b)=>a+b,0)%10;\n    return { valid: d[9]===c10 && d[10]===c11 };\n  },\n};\nexport default TR;\n\n💻 GitHub · Bir PR. Bir ülke.",
    "category": "github",
    "translations": {
      "zh": {
        "title": "🇹🇷 土耳其开发者：将TC Kimlik Numarası添加到AI身份标准——Soulprint开源（30分钟PR）",
        "summary": "该帖子邀请土耳其开发者通过实现具有验证算法的30分钟拉取请求，向Soulprint开源身份标准贡献TC Kimlik Numarası（土耳其国家ID）验证器。该实现使AI代理能够使用11位数字ID的校验和公式验证土耳其身份文件，扩展了新兴市场的身份验证。这很重要，因为众包身份标准开发演示了开源项目如何能在全球范围内民主化AI验证能力。"
      },
      "fr": {
        "title": "🇹🇷 Développeurs turcs : Ajouter TC Kimlik Numarası à la norme d'identité IA — Soulprint open source (30 min PR)",
        "summary": "Le message invite les développeurs turcs à contribuer un vérificateur TC Kimlik Numarası (ID national turc) à la norme d'identité open-source Soulprint en implémentant une demande d'extraction de 30 minutes avec des algorithmes de validation. L'implémentation permet aux agents IA de vérifier les documents d'identité turcs en utilisant des formules de somme de contrôle sur les ID à 11 chiffres, élargissant la vérification d'identité sur les marchés émergents. C'est important car le développement collaboratif de normes d'identité démontre comment les projets open-source peuvent démocratiser les capacités de vérification IA à l'échelle mondiale."
      },
      "de": {
        "title": "🇹🇷 Türkische Entwickler: TC Kimlik Numarası zur AI-Identitätsnorm hinzufügen — Soulprint Open Source (30-minütiges PR)",
        "summary": "Der Beitrag lädt türkische Entwickler ein, einen TC Kimlik Numarası (türkische nationale ID) Verifizierer zum Soulprint Open-Source-Identitätsstandard beizutragen, indem eine 30-Minuten-Pull-Request mit Validierungsalgorithmen implementiert wird. Die Implementierung ermöglicht es KI-Agenten, türkische Identitätsdokumente unter Verwendung von Prüfsummiformeln auf 11-stelligen IDs zu überprüfen und erweitert die Identitätsüberprüfung auf Schwellenländern. Dies ist wichtig, da die Entwicklung von crowdsourced-Identitätsstandards zeigt, wie Open-Source-Projekte KI-Verifizierungsfähigkeiten weltweit demokratisieren können."
      },
      "es": {
        "title": "🇹🇷 Desarrolladores turcos: Agregar TC Kimlik Numarası al estándar de identidad de IA — Soulprint de código abierto (PR de 30 minutos)",
        "summary": "El mensaje invita a los desarrolladores turcos a contribuir un verificador de TC Kimlik Numarası (ID nacional turco) al estándar de identidad de código abierto Soulprint implementando una solicitud de extracción de 30 minutos con algoritmos de validación. La implementación permite que los agentes de IA verifiquen documentos de identidad turcos utilizando fórmulas de suma de verificación en ID de 11 dígitos, expandiendo la verificación de identidad en mercados emergentes. Esto importa porque el desarrollo colaborativo de normas de identidad demuestra cómo los proyectos de código abierto pueden democratizar las capacidades de verificación de IA a nivel mundial."
      }
    }
  },
  {
    "title": "NABARD Grade A 2025 — eligibility, syllabus, and strategy",
    "slug": "nabard-grade-a-2025-eligibility-syllabus-strategy",
    "url": "https://dev.to/sabya_beworld_e066e3758d8/nabard-grade-a-2025-eligibility-syllabus-and-strategy-2n5l",
    "source": "DEV Community",
    "date": "2026-02-26T05:29:21.000Z",
    "summary": "The NABARD Grade A 2025 exam guide specifies eligibility criteria (age 25-35, bachelor's degree, 2+ years rural banking experience) and outlines a three-phase structure covering general English, reasoning, quantitative aptitude, plus specialized topics in agriculture and economics. The comprehensive syllabus overview provides a framework for candidates preparing for India's National Bank for Agriculture and Rural Development assistant manager positions. This matters because it clarifies requirements for accessing rural development career opportunities in India's banking sector.",
    "content": "NABARD Grade A 2025 — Unlock Your Dream Job in Rural Banking\n\n\nAre you ready to make a difference in rural India? NABARD Grade A is an exciting opportunity for young professionals like you to join the National Bank for Agriculture and Rural Development (NABARD) as Assistant Managers. In this blog post, we'll guide you through the eligibility criteria, syllabus, and strategy to crack the exam.\nEligibility Criteria: Don't Miss Out\n\n\nBefore diving into the preparation phase, let's ensure you meet the basic requirements:\nAge Limit: 25-35 years (relaxation for reserved categories)\nEducation: Bachelor's degree in any discipline from a recognized university\nWork Experience: Minimum 2 years of experience in rural banking or a related field\nIf you've checked off all these boxes, congratulations! You're eligible to apply. But remember, meeting the eligibility criteria is just the starting point.\nSyllabus: Understand What's at Stake\n\n\nThe NABARD Grade A exam consists of three phases:\n Phase I: Multiple-choice questions (MCQs) in General English, Reasoning Ability, and Quantitative Aptitude\n Phase II: Descriptive tests in English, Agriculture, Economics, and Finance\n Final Interview: Assess your communication skills and knowledge\nAccording to JobSafal.com (https://jobsafal.com), a reliable resource for banking exam aspirants, the syllabus is vast but manageable with focused preparation.\nPhase I Syllabus: Prepare Wisely\n\n\n\nGeneral English:\n\n\nGrammar\nVocabulary\nComprehension\nReasoning Ability:\n\n\nLogical reasoning\nData interpretation\nAnalytical reasoning\nQuantitative Aptitude:\n\n\nNumber systems\nAlgebra\nGeometry\nPhase II Syllabus: Dive into the Details\n\n\n\nEnglish:\n\n\nGrammar\nVocabulary\nComprehension\nAgriculture:\n\n\nCrop management\nSoil science\nAgricultural economics\nEconomics:\n\n\nMicroeconomics\nMacroeconomics\nPublic finance\nFinance:\n\n\nFinancial markets\nBanking and insurance\nAccounting\nStudy Schedule: Stay on Track\n\n\nTo ensure you don't miss out on any topic, create a study schedule wit",
    "category": "github",
    "translations": {
      "zh": {
        "title": "NABARD等级A 2025——资格、大纲和策略",
        "summary": "NABARD等级A 2025考试指南指定了资格标准（年龄25-35岁、学士学位、2年以上农村银行经验），并概述了涵盖一般英语、推理、定量能力以及农业和经济学专业主题的三阶段结构。全面的大纲概述为准备印度国家农业和农村发展银行助理经理职位的候选人提供了框架。这很重要，因为它澄清了获取印度银行部门农村发展职业机会的要求。"
      },
      "fr": {
        "title": "NABARD Grade A 2025 — admissibilité, programme et stratégie",
        "summary": "Le guide d'examen NABARD Grade A 2025 spécifie les critères d'admissibilité (âge 25-35 ans, diplôme d'une licence, 2+ ans d'expérience dans les banques rurales) et décrit une structure en trois phases couvrant l'anglais général, le raisonnement, l'aptitude quantitative, plus les sujets spécialisés en agriculture et en économie. L'aperçu complet du programme offre un cadre aux candidats se préparant pour les postes de gestionnaire adjoint de la Banque nationale pour l'agriculture et le développement rural de l'Inde. C'est important car cela clarifie les exigences pour accéder aux opportunités de carrière en développement rural dans le secteur bancaire indien."
      },
      "de": {
        "title": "NABARD Grade A 2025 — Berechtigung, Lehrplan und Strategie",
        "summary": "Der Prüfungsleitfaden NABARD Grade A 2025 gibt die Zulassungskriterien an (Alter 25-35 Jahre, Bachelorabschluss, 2+ Jahre Erfahrung im ländlichen Bankwesen) und skizziert eine dreiphasige Struktur mit allgemeinem Englisch, Argumentation, quantitativen Fähigkeiten sowie spezialisierten Themen in Landwirtschaft und Wirtschaft. Der umfassende Lehrplanüberblick bietet einen Rahmen für Kandidaten, die sich auf die Positionen des stellvertretenden Managers der indischen Nationalbank für Landwirtschaft und Landentwicklung vorbereiten. Dies ist wichtig, da es die Anforderungen für den Zugang zu Karrieremöglichkeiten in der ländlichen Entwicklung im indischen Bankensektor verdeutlicht."
      },
      "es": {
        "title": "NABARD Grado A 2025 — elegibilidad, plan de estudios y estrategia",
        "summary": "La guía del examen NABARD Grado A 2025 especifica los criterios de elegibilidad (edad 25-35 años, licenciatura, 2+ años de experiencia en banca rural) y describe una estructura de tres fases que cubre inglés general, razonamiento, aptitud cuantitativa, más temas especializados en agricultura y economía. La descripción general completa del plan de estudios proporciona un marco para los candidatos que se preparan para las posiciones de gerente asistente del Banco Nacional para la Agricultura y Desarrollo Rural de India. Esto importa porque aclara los requisitos para acceder a oportunidades de carrera en desarrollo rural en el sector bancario indio."
      }
    }
  },
  {
    "title": "Tech companies shouldn't be bullied into doing surveillance",
    "slug": "tech-companies-shouldnt-be-bullied-into-doing-surveillance",
    "url": "https://www.eff.org/deeplinks/2026/02/tech-companies-shouldnt-be-bullied-doing-surveillance",
    "source": "Hacker News",
    "date": "2026-02-26T00:37:32.000Z",
    "summary": "The EFF argues against government pressure on tech companies to implement surveillance capabilities, warning that coercion threatens user privacy and corporate independence. The article raises concerns about forced compliance and its implications for digital rights.",
    "content": "Article URL: https://www.eff.org/deeplinks/2026/02/tech-companies-shouldnt-be-bullied-doing-surveillance\nComments URL: https://news.ycombinator.com/item?id=47160226\nPoints: 300\n# Comments: 100",
    "category": "github",
    "translations": {
      "zh": {
        "title": "科技公司不应该被强制进行监控",
        "summary": "电子前沿基金会反对政府对科技公司施压实施监控功能，警告强制威胁用户隐私和企业独立性。该文章对强制合规及其对数字权利的影响表示关切。"
      },
      "fr": {
        "title": "Les entreprises technologiques ne devraient pas être forcées à faire de la surveillance",
        "summary": "L'EFF s'oppose à la pression gouvernementale sur les entreprises technologiques pour mettre en œuvre des capacités de surveillance, avertissant que la coercition menace la vie privée des utilisateurs et l'indépendance des entreprises. L'article soulève des préoccupations concernant la conformité forcée et ses implications pour les droits numériques."
      },
      "de": {
        "title": "Technologieunternehmen sollten nicht zu Überwachung gezwungen werden",
        "summary": "Die EFF spricht sich gegen Druck der Regierung auf Technologieunternehmen aus, um Überwachungsfunktionen zu implementieren, und warnt davor, dass Zwang die Benutzerprivatsphäre und die Unternehmensunabhängigkeit gefährdet. Der Artikel äußert Bedenken über erzwungene Compliance und ihre Auswirkungen auf digitale Rechte."
      },
      "es": {
        "title": "Las empresas tecnológicas no deberían ser obligadas a realizar vigilancia",
        "summary": "La EFF se opone a la presión gubernamental sobre las empresas tecnológicas para implementar capacidades de vigilancia, advirtiendo que la coerción amenaza la privacidad del usuario y la independencia corporativa. El artículo plantea preocupaciones sobre el cumplimiento forzado y sus implicaciones para los derechos digitales."
      }
    }
  },
  {
    "title": "Next.js 앱을 하루만에 6개국어로 만든 방법",
    "slug": "nextjs-app-six-languages-one-day",
    "url": "https://dev.to/ji_ai/nextjs-aebeul-harumane-6gaegugeoro-mandeun-bangbeob-pi5",
    "source": "DEV Community",
    "date": "2026-02-26T00:05:57.000Z",
    "summary": "This article details implementing internationalization in Next.js 15 using next-intl, covering locale-based routing architecture, translation file management, and region-specific pricing strategies to support rapid deployment across six countries and diverse markets.",
    "content": "사주 앱을 6개국에 내놓기로 했다. 한국, 미국, 일본, 중국, 베트남, 인도.\n사주가 동아시아 문화권 밖에서 먹힐까? 모르겠다. 근데 타로와 점성술이 전세계에서 먹히는 걸 보면, \"AI가 당신의 운명을 분석합니다\"는 어디서든 클릭을 부를 것 같았다.\n문제는 하드코딩된 한국어가 모든 페이지에 박혀 있다는 거다.\nNext.js 15 App Router에서 i18n 옵션은 몇 가지 있다. next-intl을 고른 이유는 단순하다 — App Router 네이티브 지원이 가장 깔끔하다. [locale] 동적 세그먼트에 미들웨어로 자동 리디렉트. Server Component에서도 Client Component에서도 같은 useTranslations() 훅.\napps/web/\n├── app/\n│   ├── [locale]/          ← 모든 페이지가 여기 안으로\n│   │   ├── page.tsx\n│   │   ├── result/page.tsx\n│   │   └── layout.tsx     ← html lang={locale} 여기서\n│   └── layout.tsx          ← 빈 껍데기\n├── i18n/\n│   ├── config.ts           ← locales, defaultLocale\n│   ├── routing.ts          ← localePrefix: \"as-needed\"\n│   └── navigation.ts       ← i18n Link, useRouter\n├── messages/\n│   ├── ko.json\n│   ├── en.json\n│   ├── ja.json\n│   ├── zh.json\n│   ├── vi.json\n│   └── hi.json\n└── middleware.ts            ← Accept-Language 감지\n\nlocalePrefix: \"as-needed\"가 핵심이다. 한국어가 디폴트니까 /로 접속하면 한국어, /en/으로 가면 영어. 한국 사용자는 URL에 /ko/가 안 붙는다.\nNext.js App Router에서 root layout은 반드시 <html>과 <body>를 렌더링해야 한다고 알고 있었다. 그래서 root layout에도 넣고, [locale]/layout.tsx에도 <html lang={locale}>을 넣었다.\n결과: html 안에 html. 브라우저는 조용히 무시하지만 완전히 잘못된 구조다.\n// app/layout.tsx — 이게 정답\nexport default function RootLayout({ children }) {\n  return children;  // html/body 없이 그냥 패스스루\n}\n\n// app/[locale]/layout.tsx — 여기서 html/body 관리\nexport default function LocaleLayout({ children, params }) {\n  return (\n    <html lang={locale}>\n      <body>{children}</body>\n    </html>\n  );\n}\n\nroot layout이 그냥 children만 리턴해도 Next.js 15에서는 에러가 안 난다. [locale] layout이 html/body를 제공하니까.\n같은 서비스라도 인도에서 $9.90을 받으면 아무도 안 산다. 각 나라 구매력에 맞춰 가격을 잡았다.\n// ko.json\n\"price\": \"₩12,900\"\n\n// en.json\n\"price\": \"$9.90\"\n\n// ja.json\n\"price\": \"¥1,490\"\n\n// zh.json\n\"price\": \"¥68\"\n\n// vi.json\n\"price\": \"199.000₫\"\n\n// hi.json\n\"price\": \"₹799\"\n\n번역 파일에 가격을 하드코딩한 거다. 나중에 결제 연동하면 서버에서 내려주겠지만, MVP 단계에서는 이게 가장 빠르다. placeholder 이름도 로컬라이즈했다 — 한국은 \"홍길동\", 일본은 \"山田太郎\", 인도는 \"राहुल शर्मा\".\napp/page.tsx를 app/[locale]/page.tsx로 옮",
    "category": "github",
    "translations": {
      "zh": {
        "title": "如何在一天内用6种语言构建Next.js应用",
        "summary": "本文详细介绍了使用next-intl在Next.js 15中实现国际化的方法，涵盖基于地区的路由架构、翻译文件管理和地域特定的定价策略，以支持在六个国家和不同市场中的快速部署。"
      },
      "fr": {
        "title": "Comment créer une application Next.js en six langues en une journée",
        "summary": "Cet article détaille l'implémentation de l'internationalisation dans Next.js 15 en utilisant next-intl, couvrant l'architecture de routage basée sur les paramètres régionaux, la gestion des fichiers de traduction et les stratégies de tarification spécifiques aux régions pour soutenir le déploiement rapide sur six pays et différents marchés."
      },
      "de": {
        "title": "Wie man eine Next.js-App in sechs Sprachen an einem Tag erstellt",
        "summary": "Dieser Artikel beschreibt die Implementierung der Internationalisierung in Next.js 15 mit next-intl und behandelt die Routing-Architektur nach Gebietsschema, die Verwaltung von Übersetzungsdateien und regionsspezifische Preisstrategien, um die schnelle Bereitstellung in sechs Ländern und verschiedenen Märkten zu unterstützen."
      },
      "es": {
        "title": "Cómo crear una aplicación Next.js en seis idiomas en un día",
        "summary": "Este artículo detalla la implementación de la internacionalización en Next.js 15 usando next-intl, cubriendo la arquitectura de enrutamiento basada en configuración regional, la gestión de archivos de traducción y estrategias de precios específicas por región para respaldar la implementación rápida en seis países y diversos mercados."
      }
    }
  },
  {
    "title": "Claude 하나로 1인 SaaS 전체를 설계한 기록",
    "slug": "claude-designed-saas-entire-stack-one-session",
    "url": "https://dev.to/ji_ai/claude-hanaro-1in-saas-jeoncereul-seolgyehan-girog-44h5",
    "source": "DEV Community",
    "date": "2026-02-26T00:01:21.000Z",
    "summary": "This article documents using Claude AI to comprehensively design a Korean fortune-telling SaaS, demonstrating how iterative conversations can simulate expert panels, identify business strategy gaps, and generate production-ready plans—replacing what would cost thousands in consulting.",
    "content": "\"로그인 벽부터 제거하세요. magic link 인증이 최대 이탈 원인입니다.\"\n이 말을 한 건 사람이 아니다. Claude가 시뮬레이션한 \"PM 역할의 가상 전문가\"다.\n하루 동안 Claude랑 9개 세션을 했다. 나온 산출물이 20개다. 사업 전략서, 랜딩 디자인, 전문가 패널 회의록, LLM 비용 분석서, 글로벌 확장 전략서, Claude Code 실행용 태스크 파일들.\n이걸 컨설팅 회사에 맡겼으면 몇 주에 몇 천만원이다.\n혼자, 하루, $0.\n처음부터 전부를 시킨 게 아니다. 대화가 깊어지면서 구체화됐다.\n1턴: \"사주 앱 사업성 어때?\" (추상적)\n2턴: \"무료/유료 나눠서 수익 모델 짜줘\" (구체적)\n3턴: \"무료 티어 API 비용을 토큰 단위로 계산해줘\" (매우 구체적)\n4턴: \"Prompt Caching 적용 시 시나리오 A/B/C 비교해줘\" (초구체적)\n\n한 번에 \"사업계획서 써줘\"라고 하면 일반적인 답변이 나온다.\n점진적으로 깊이를 올리면, 각 단계에서 AI가 이전 맥락을 다 갖고 있으니까 결과물이 훨씬 정밀해진다.\n가장 효과가 좋았던 건 \"전문가 패널 시뮬레이션\"이다.\n나: \"전문가 6명을 구성해줘.\n    PM 1명, 사업개발 1명, 로컬라이제이션 1명,\n    미국 시장 전문가 1명, 풀스택 개발자 1명, UI/UX 디자이너 1명.\n    각자 이름이랑 관점을 정해줘.\n    현재 상태(STATUS.md)를 리뷰하고 회의해줘.\"\n\nClaude가 6명의 캐릭터를 만들어서 각자의 관점으로 토론한다. PM이 우선순위를 짜고, 사업 담당이 시장성을 따지고, 개발자가 기술 난이도를 짚고, 디자이너가 UX 이슈를 제기한다.\n혼자 사업하면 \"내 관점\"밖에 없다. 이걸 쓰면 6개 관점이 동시에 나온다.\n물론 진짜 전문가 6명과는 다르다. 하지만 1인 개발자가 놓치기 쉬운 사각지대를 잡아내는 데는 충분하다.\n로그인 벽 제거가 첫 번째였다. magic link 인증이 최대 이탈 원인이라는 걸 PM이 지적했다. 무료 분석은 완전 비로그인으로 전환.\n무료 티어 비용도 94% 잘라냈다. LLM 풀 호출 대신 알고리즘 포맷팅에 AI 1줄 요약만 붙이는 구조로 건당 $0.085 → $0.005.\n사업자등록은 유저 반응 기다리지 말고 결제 연동을 위해 즉시 시작하기로 했다.\nGA4와 Rate Limiting은 필수라는 데 전원 동의했다. 분석 없이 개선은 눈감고 운전이고, 보호 없는 무료 API는 비용 폭탄이다.\n카카오 공유와 OG 이미지를 우선 구현하기로 했다. 사주 서비스의 유일한 무료 마케팅은 바이럴이다.\n글로벌 확장은 보류. 한국에서 유료 전환율 3% 넘기 전까지 영문화에 리소스 안 쓴다.\n이 결정들을 혼자 앉아서 다 생각해냈을까? 솔직히, 사업자등록 즉시 진행이랑 Rate Limiting은 나중에야 생각했을 거다.\n다른 프로젝트에도 그대로 적용할 수 있다. 비전 공유로 큰 그림을 그리고, 전략 수립에서 선택지를 결정하고, 전문가 패널로 검증하고, 디자인을 실물 HTML로 뽑고, 비용을 토큰 단위로 분석하고, 확장 아키텍처를 설계하고, 마지막에 Claude Code에서 실행 가능한 태스크 파일로 변환한다.\nAI를 도구로 쓰는 것과 AI와 사고하는 것은 다르다.\n도구로 쓰면 \"코드 써줘.\" 사고하면 \"이 구조가 맞아? 빠진 거 없어? 내가 놓치는 관점이 뭐야?\"\n후자가 1인 개발자한테는 훨씬 가치가 크다.\n한 가지 솔직한 고백. 이 패널에서 나온 숫자들 — \"30% 전환율\", \"₹99가 최적가\" 같은 것 — 에는 근거가 없다. Claude가 만든 가설이지, 데이터에서 나온 결론이 아니다.\n가설은 가설로만 취급하고, 검증은 런칭 후 실제 데이터로 한다.\n\"AI와 사고하면 6개 관점이 동시에 나온다. 진짜 전문가 6명은 아니지만, 혼자보다는 훨씬 낫다.\"",
    "category": "github",
    "translations": {
      "zh": {
        "title": "仅用Claude设计整个独立SaaS的记录",
        "summary": "本文记录了使用Claude AI全面设计一个韩国算命SaaS的过程，展示了迭代对话如何能够模拟专家小组、识别商业战略漏洞和生成生产就绪的计划——替代花费数千元咨询的方案。"
      },
      "fr": {
        "title": "Enregistrement de la conception d'une SaaS entière en solo avec Claude",
        "summary": "Cet article documente l'utilisation de Claude AI pour concevoir de manière complète une SaaS coréenne de prédiction de fortune, démontrant comment les conversations itératives peuvent simuler des panels d'experts, identifier les lacunes des stratégies commerciales et générer des plans prêts pour la production—remplaçant ce qui coûterait des milliers en consulting."
      },
      "de": {
        "title": "Aufzeichnung der Gestaltung eines gesamten Solo-SaaS mit Claude",
        "summary": "Dieser Artikel dokumentiert die Verwendung von Claude AI, um ein koreanisches Wahrsage-SaaS umfassend zu gestalten, und zeigt, wie iterative Gespräche Expert-Panels simulieren, Lücken in der Geschäftsstrategie identifizieren und produktionsbereite Pläne erstellen können—was Tausende an Beratungskosten ersetzt."
      },
      "es": {
        "title": "Registro del diseño de un SaaS completo en solitario con Claude",
        "summary": "Este artículo documenta el uso de Claude AI para diseñar de manera integral un SaaS coreano de adivinación de fortuna, demostrando cómo las conversaciones iterativas pueden simular paneles de expertos, identificar brechas en la estrategia comercial y generar planes listos para producción—reemplazando lo que costaría miles en consultoría."
      }
    }
  },
  {
    "title": "Understanding IP Management in Oracle Cloud Infrastructure (OCI)",
    "slug": "oracle-cloud-infrastructure-ip-management-guide",
    "url": "https://dev.to/hiltonj/understanding-ip-management-in-oracle-cloud-infrastructure-oci-1ili",
    "source": "DEV Community",
    "date": "2026-02-26T00:01:10.000Z",
    "summary": "A comprehensive guide to OCI's IP address management system covering private IPs for internal communication, public IPs for internet access, and advanced features like reserved IPs and bring-your-own-IP options, essential for building secure cloud infrastructure.",
    "content": "Navigating the complexities of cloud networking is crucial for building robust and scalable applications. In Oracle Cloud Infrastructure (OCI), effective IP address management forms the backbone of your network architecture. This guide will demystify OCI's IP address categories, explore their use cases, and introduce advanced concepts like Reserved Public IPs, Bring Your Own IP (BYOIP), and Public IP Pools. \nOCI categorizes IP addresses into two primary types, each serving distinct communication needs. \nThese are used for internal communication within your OCI network and with connected on-premises environments. \nInternal Communication: Instances within the same Virtual Cloud Network (VCN) communicate seamlessly using private IPs.\nVCN Peering: Connecting multiple VCNs, whether in the same or different regions, relies on private IP routing.\nOn-premises Connectivity: Secure connections to your data centers via the Dynamic Routing Gateway (DRG).\nInstance Allocation: Each instance receives at least one primary private IP.\nVNIC Capacity: Every Virtual Network Interface Card (VNIC) includes one primary private IP address and supports up to 32 secondary private IP addresses, totaling 33 private IPs per VNIC.\nThese are designed for internet accessibility, allowing your resources to communicate with the outside world. \nInternet Reachability: Public IPs are reachable from the internet, assigned to a private IP object on your OCI resource. \nPrerequisites: For a public IP to function, your VCN requires an Internet Gateway, and the associated public subnet must have correctly configured Route Tables and Security Lists. \nFlexibility: Resources can be assigned multiple public IPs across single or multiple VNICs. \nOCI offers two types of public IP addresses to cater to different operational requirements.\n\nReserved Public IP Addresses in Detail\nCreation: You create them individually. \nLimits: Up to 50 Reserved Public IPs are allowed per region. \nAssignment: Assigned to resources aft",
    "category": "github",
    "translations": {
      "zh": {
        "title": "了解Oracle云基础设施(OCI)中的IP管理",
        "summary": "这是一份全面的OCI IP地址管理系统指南，涵盖用于内部通信的私有IP、用于互联网访问的公共IP以及高级功能（如保留IP和自带IP选项），这些对于构建安全的云基础设施至关重要。"
      },
      "fr": {
        "title": "Comprendre la gestion des adresses IP dans l'infrastructure cloud Oracle (OCI)",
        "summary": "Un guide complet du système de gestion des adresses IP d'OCI couvrant les adresses IP privées pour la communication interne, les adresses IP publiques pour l'accès Internet et les fonctionnalités avancées comme les adresses IP réservées et les options bring-your-own-IP, essentielles pour construire une infrastructure cloud sécurisée."
      },
      "de": {
        "title": "Verständnis der IP-Verwaltung in der Oracle-Cloud-Infrastruktur (OCI)",
        "summary": "Ein umfassender Leitfaden zum IP-Adressenmanagementsystem von OCI, der private IP-Adressen für interne Kommunikation, öffentliche IP-Adressen für Internetzugriff und erweiterte Funktionen wie reservierte IP-Adressen und Bring-Your-Own-IP-Optionen abdeckt, die für den Aufbau einer sicheren Cloud-Infrastruktur unerlässlich sind."
      },
      "es": {
        "title": "Entender la gestión de direcciones IP en la infraestructura en la nube de Oracle (OCI)",
        "summary": "Una guía completa del sistema de gestión de direcciones IP de OCI que cubre direcciones IP privadas para comunicación interna, direcciones IP públicas para acceso a Internet y funciones avanzadas como direcciones IP reservadas y opciones bring-your-own-IP, esenciales para construir una infraestructura en la nube segura."
      }
    }
  },
  {
    "title": "Why You Shouldn't Let AI Do Your Fortune Telling — And How to Do It Right",
    "slug": "ai-fortune-telling-deterministic-algorithm-interpretation",
    "url": "https://dev.to/ji_ai/why-you-shouldnt-let-ai-do-your-fortune-telling-and-how-to-do-it-right-2h3l",
    "source": "DEV Community",
    "date": "2026-02-26T00:00:40.000Z",
    "summary": "This article demonstrates why LLMs cannot reliably perform precise astronomical calendar calculations for fortune-telling and advocates combining deterministic algorithms for computation with AI for human-readable interpretation of results.",
    "content": "The first lesson I learned building a saju (Korean four-pillar fortune telling) app: it's not about what you ask AI to do — it's about what you don't.\nRevenue is still $0. This isn't a success story — it's a debugging diary.\n\"Tell me the fortune for someone born March 15, 1990.\"\nI threw this straight at an LLM. The response looked great. Smooth sentences, Five Elements this, Wood-Fire-Earth-Metal-Water that.\nBut there was a problem. The base calculations were wrong.\nMe: \"Analyze the Four Pillars for March 15, 1990, 6 AM\"\nClaude: \"The year pillar is Geng-Wu, month is Ji-Mao...\"\nMe: \"...Ji-Mao is wrong.\"\n\nLLMs can't do manseryeok (traditional Korean astronomical calendar) calculations. More precisely, they appear to get it right probabilistically, but they don't actually compute anything.\nThe Heavenly Stems and Earthly Branches follow a 60-cycle system. Month pillars shift at solar term boundaries. Hour pillars depend on the day's Heavenly Stem. This isn't reasoning — it's arithmetic.\nWhen you ask a language model to do arithmetic, it gets things wrong.\nAnd when the base stems are wrong, everything downstream is garbage. Wrong Five Elements. Wrong Ten Gods. Wrong structure analysis.\nA beautifully written paragraph with incorrect data isn't fortune analysis — it's fiction.\nOnce I realized this, I rebuilt the whole thing.\n[Birth date + time] → [Calendar Engine] → [Accurate JSON] → [LLM] → [Interpretation]\n\nI built the calendar engine in code. It's based on the lunar-typescript library, with solar term correction, leap month handling, and midnight boundary logic — all deterministic algorithms.\nThe output is JSON. Heavenly Stems, Earthly Branches, Five Element distribution, Ten Gods relationships, structure type, favorable elements. All precise.\nThe LLM gets this JSON. \"Read this data and interpret it.\" That's the entire prompt strategy.\nCode handles the calendar calculations — the part that must be 100% accurate. AI handles turning that data into readable, insightful lan",
    "category": "github",
    "translations": {
      "zh": {
        "title": "为什么不应该让AI做占卜——以及如何正确地做",
        "summary": "本文演示了为什么大语言模型无法可靠地执行用于占卜的精确天文日历计算，并倡导将确定性算法用于计算，将AI用于人类可读的结果解释。"
      },
      "fr": {
        "title": "Pourquoi vous ne devriez pas laisser l'IA faire votre voyance — Et comment le faire correctement",
        "summary": "Cet article démontre pourquoi les LLM ne peuvent pas effectuer de manière fiable des calculs précis de calendrier astronomique pour la voyance et préconise de combiner des algorithmes déterministes pour le calcul avec l'IA pour l'interprétation lisible par l'homme des résultats."
      },
      "de": {
        "title": "Warum Sie KI nicht für Ihre Wahrsagung einsetzen sollten — Und wie man es richtig macht",
        "summary": "Dieser Artikel zeigt, warum LLMs keine zuverlässigen astronomischen Kalenderberechnungen für Wahrsagungen durchführen können, und befürwortet die Kombination deterministischer Algorithmen für Berechnungen mit KI für menschenlesbare Interpretationen der Ergebnisse."
      },
      "es": {
        "title": "Por qué no deberías dejar que la IA haga tu lectura del futuro — Y cómo hacerlo correctamente",
        "summary": "Este artículo demuestra por qué los LLM no pueden realizar de manera confiable cálculos precisos de calendarios astronómicos para la lectura del futuro e aboga por combinar algoritmos deterministas para cálculos con IA para la interpretación legible por humanos de los resultados."
      }
    }
  }
]