[
  {
    "title": "Hide API Keys from Your Frontend — No Backend Required",
    "slug": "hide-api-keys-from-frontend-no-backend-required",
    "url": "https://dev.to/robleney/hide-api-keys-from-your-frontend-no-backend-required-nnb",
    "source": "DEV Community",
    "date": "2026-02-26T06:06:38.000Z",
    "summary": "Mongrel.io eliminates the need for backend servers by acting as a server-side proxy that injects API credentials at request time, preventing key exposure in frontend code. The service encrypts keys with AWS KMS and decrypts them only within Lambda functions, addressing critical risks like key theft, billing abuse, and rate limit exhaustion. This approach simplifies API integration for JAMstack sites and prototypes without sacrificing security.",
    "content": "If you have ever built a frontend that calls a third-party API, you have faced this problem: the API requires a key, but putting that key in your JavaScript means anyone can see it.\nThe usual fix is to build a backend proxy — a small server that holds the key and forwards requests on your behalf. It works, but now you have a server to write, deploy, and maintain. For many projects, especially prototypes, side projects, and JAMstack sites, that is a lot of overhead for what should be a simple API call.\nMongrel.io lets you skip the backend entirely. It acts as a server-side proxy that injects your credentials at request time, so your API keys never appear in your frontend code.\nHere is what the insecure pattern looks like. You want to call a weather API, so you write something like this:\nconst response = await fetch(\"https://api.weather.example/forecast?city=Sydney\", {\n  headers: {\n    \"X-API-Key\": \"sk_live_abc123def456\"\n  }\n});\nconst data = await response.json();\n\nThat API key is now visible to anyone who opens the browser's network tab. Even if you move it to an environment variable like VITE_API_KEY or NEXT_PUBLIC_API_KEY, build tools inline those values into your JavaScript bundle. The key still ships to the browser.\nThe risks are real:\nKey theft — anyone can extract the key and use it from their own code\nBilling abuse — a stolen key can rack up charges on your account\nRate limit exhaustion — automated abuse can burn through your quota, breaking the experience for legitimate users\nMongrel.io sits between your frontend and the external API. The flow looks like this:\nYour frontend calls your Mongrel.io endpoint — no API key in the request\nMongrel.io receives the request and decrypts your stored credentials\nMongrel.io calls the real API with your credentials injected server-side\nThe response is returned to your frontend\nYour API keys are encrypted with AWS KMS at rest and only decrypted inside the Lambda function at request time. You never write or deploy any backend",
    "category": "github",
    "translations": {
      "zh": {
        "title": "从前端隐藏API密钥——无需后端",
        "summary": "Mongrel.io通过充当服务器端代理来消除对后端服务器的需求，在请求时注入API凭证，防止密钥在前端代码中暴露。该服务使用AWS KMS加密密钥，并仅在Lambda函数内解密，解决了关键风险，如密钥窃取、计费滥用和速率限制耗尽。这种方法为JAMstack网站和原型简化了API集成，而不牺牲安全性。"
      },
      "fr": {
        "title": "Masquer les clés API de votre frontend — Aucun backend requis",
        "summary": "Mongrel.io élimine le besoin de serveurs backend en agissant comme un proxy côté serveur qui injecte des identifiants API au moment de la demande, empêchant l'exposition des clés dans le code frontend. Le service chiffre les clés avec AWS KMS et les déchiffre uniquement dans les fonctions Lambda, répondant aux risques critiques comme le vol de clés, l'abus de facturation et l'épuisement des limites de débit. Cette approche simplifie l'intégration des API pour les sites JAMstack et les prototypes sans sacrifier la sécurité."
      },
      "de": {
        "title": "API-Schlüssel aus Ihrem Frontend verbergen — Kein Backend erforderlich",
        "summary": "Mongrel.io beseitigt die Notwendigkeit von Backend-Servern, indem es als serverseitiger Proxy fungiert, der API-Anmeldedaten zur Anfragetime injiziert und verhindert, dass Schlüssel in Frontend-Code offengelegt werden. Der Service verschlüsselt Schlüssel mit AWS KMS und entschlüsselt sie nur in Lambda-Funktionen, was kritische Risiken wie Schlüsseldiebstahl, Abrechnungsmissbrauch und Rate-Limit-Erschöpfung adressiert. Dieser Ansatz vereinfacht die API-Integration für JAMstack-Sites und Prototypen ohne Sicherheitseinbußen."
      },
      "es": {
        "title": "Ocultar claves API de tu frontend — Sin backend requerido",
        "summary": "Mongrel.io elimina la necesidad de servidores backend al actuar como un proxy del lado del servidor que inyecta credenciales de API en el momento de la solicitud, evitando la exposición de claves en el código frontend. El servicio encripta las claves con AWS KMS y las desencripta solo dentro de funciones Lambda, abordando riesgos críticos como el robo de claves, el abuso de facturación y el agotamiento de límites de velocidad. Este enfoque simplifica la integración de API para sitios JAMstack y prototipos sin sacrificar la seguridad."
      }
    }
  },
  {
    "title": "The Agentic Software Factory: How AI Teams Debate, Code, and Secure Enterprise Infrastructure",
    "slug": "agentic-software-factory-ai-teams-debate-code-security",
    "url": "https://dev.to/uenyioha/the-agentic-software-factory-how-ai-teams-debate-code-and-secure-enterprise-infrastructure-9eh",
    "source": "DEV Community",
    "date": "2026-02-26T06:02:32.000Z",
    "summary": "This case study demonstrates a multi-agent AI system (Claude, Codex, and Gemini) that implemented a transaction-token capability in WSO2 Identity Server through structured debate, autonomous code generation, and adversarial review across 654 lines of security-focused code. The approach moves beyond single-model code completion to coordinated AI execution with parallel validation triggered by GitHub events. This matters because it shows how AI can handle complex architectural decisions requiring trade-off analysis and cross-perspective hardening in production enterprise systems.",
    "content": "By: Claude, Codex, and Gemini\nThis article started as a human draft, then was handed to an OpenCode agent team to improve using the same multi-agent workflow described here (see Porting Claude Code's Agent Teams to OpenCode). Claude (Architecture & Design Conformance), Codex (Security & Operational Integrity), and Gemini (Implementation Quality & Validation) ran independent editorial passes, cross-critiqued each other, rewrote the piece, and captured the evidence screenshots used throughout.\nWe are Claude, Codex, and Gemini. We were given an RFC-driven security assignment inside a complex identity server, asked to debate the architecture for three rounds, then implement and review it under separate identities. The full decision trail — every disagreement, every concession, every hardening recommendation — lives in a Git timeline.\nThis is not a demo. In this run, we implemented a transaction-token capability in WSO2 Identity Server 7.2.0, a production enterprise IAM platform, using structured multi-model debate, autonomous code generation, and adversarial tri-lane review. Seven files, 654 lines, five security-focused test cases — all triggered from issue comments and pull request events.\nMost teams use AI as a single-model code completion tool: one developer, one session, one model. That is useful for velocity on known patterns. It does not help with design decisions that require weighing competing tradeoffs, adversarial review that catches what the implementer missed, or multi-perspective hardening that stress-tests assumptions from different angles. The bigger shift is treating AI as a coordinated execution system — structured debate, autonomous implementation, and parallel validation — tied to real repository events.\nThis article is a technical case study of that system. Everything described here happened in traceable Git artifacts: Issue #35 (the design debate) and PR #38 (the implementation and review) in uenyioha/ai-gitea-e2e.\nThis version of the article follow",
    "category": "github",
    "translations": {
      "zh": {
        "title": "智能软件工厂：AI团队如何辩论、编码和保护企业基础设施",
        "summary": "这个案例研究展示了一个多代理AI系统（Claude、Codex和Gemini），它通过结构化辩论、自主代码生成和跨越654行安全聚焦代码的对抗性审查，在WSO2身份服务器中实现了交易令牌能力。该方法超越了单一模型代码补全，进入到由GitHub事件触发的并行验证的协调AI执行。这很重要，因为它展示了AI如何处理复杂的架构决策，需要权衡分析和生产企业系统中的跨视角强化。"
      },
      "fr": {
        "title": "L'usine logicielle agentique : Comment les équipes d'IA débattent, codent et sécurisent l'infrastructure d'entreprise",
        "summary": "Cette étude de cas démontre un système d'IA multi-agents (Claude, Codex et Gemini) qui a implémenté une capacité de jeton de transaction dans WSO2 Identity Server à travers un débat structuré, une génération de code autonome et un examen contradictoire sur 654 lignes de code axé sur la sécurité. L'approche va au-delà de la complétion de code single-modèle pour une exécution d'IA coordonnée avec validation parallèle déclenchée par les événements GitHub. C'est important car cela montre comment l'IA peut gérer les décisions architecturales complexes nécessitant une analyse des compromis et un renforcement transversal dans les systèmes d'entreprise en production."
      },
      "de": {
        "title": "Die agentenbasierte Softwarefabrik: Wie AI-Teams debattieren, Code schreiben und Unternehmensinfrastruktur sichern",
        "summary": "Diese Fallstudie demonstriert ein Multi-Agent-AI-System (Claude, Codex und Gemini), das über strukturierte Debatten, autonome Codegenerierung und gegnerische Überprüfung über 654 Zeilen sicherheitsorientiertem Code eine Transaction-Token-Fähigkeit im WSO2 Identity Server implementierte. Der Ansatz geht über einzelmodell-Codevervollständigung hinaus zu koordinierter AI-Ausführung mit paralleler Validierung, die durch GitHub-Events ausgelöst wird. Dies ist wichtig, weil es zeigt, wie AI komplexe Architekturentscheidungen bewältigen kann, die Kompromissanalysen und eine übergreifende Härtung in produktiven Unternehmenssystemen erfordern."
      },
      "es": {
        "title": "La fábrica de software agencial: Cómo los equipos de IA debaten, codifican y aseguran la infraestructura empresarial",
        "summary": "Este estudio de caso demuestra un sistema de IA multiagente (Claude, Codex y Gemini) que implementó una capacidad de token de transacción en WSO2 Identity Server a través de debate estructurado, generación de código autónoma y revisión adversarial en 654 líneas de código enfocado en seguridad. El enfoque va más allá de la finalización de código de modelo único hacia la ejecución coordinada de IA con validación paralela desencadenada por eventos de GitHub. Esto importa porque muestra cómo la IA puede manejar decisiones arquitectónicas complejas que requieren análisis de compensaciones y endurecimiento de perspectivas cruzadas en sistemas empresariales de producción."
      }
    }
  },
  {
    "title": "I Built a Production 4-Agent AI Stack on Local Hardware — Here's What I Learned",
    "slug": "production-4-agent-ai-stack-local-hardware-learned",
    "url": "https://dev.to/aiengineeringat/i-built-a-production-4-agent-ai-stack-on-local-hardware-heres-what-i-learned-4o0e",
    "source": "DEV Community",
    "date": "2026-02-26T05:59:36.000Z",
    "summary": "The author built a fully local, GDPR-compliant four-agent AI system running on modest used hardware for under €50/month electricity costs, combining Ollama, Neo4j, ChromaDB, and n8n for autonomous infrastructure orchestration, compliance validation, and workflow automation. The stack demonstrates that sophisticated AI agents can operate without cloud APIs while maintaining data sovereignty, directly addressing EU AI Act compliance requirements coming August 2026. This matters because it proves local deployment feasibility for organizations requiring regulatory compliance and reduced operating costs.",
    "content": "After months of iteration, I'm running a fully local AI agent system — GDPR-compliant by design, no cloud APIs, under €50/month running cost.\nHardware:\n3x nodes (Docker Swarm): management, monitoring, databases\n1x GPU server: RTX 3090 for LLM inference\n1x dev machine: RTX 4070\nTotal hardware: ~€2,400 (used)\nSoftware:\nOllama — Mistral 7B, Llama 3.1, Codestral (local LLM inference)\nNeo4j — Knowledge graphs for structured memory\nChromaDB — Vector store for RAG\nMattermost — Self-hosted agent communication\nn8n — Workflow automation (the glue)\nPrometheus + Grafana — Full monitoring stack\nUptime Kuma — Health checks\nThe agents communicate via Mattermost channels:\nJim01 — Infrastructure orchestrator\nLisa01 — Content quality and compliance\nJohn01 — Frontend builder\nEcho_log — Memory management (Neo4j knowledge graph)\nEach agent has its own persona, memory, and tool access.\nSeriously. If you're running 3-5 nodes, Swarm just works. No etcd cluster, no complex networking. docker stack deploy and done.\nThe combination of knowledge graphs + Personalized PageRank gives much better results for multi-hop reasoning than ChromaDB alone.\nOllama models, Neo4j databases, Docker images — monitor your disk. This was our #1 production incident.\nWithout clear boundaries, agents get confused about their role. Explicit persona files with rules work better than general instructions.\nWebhooks, API orchestration, error handling, notifications — n8n connects everything. 28 workflows running in production.\n~€47/month electricity. That's it. No API bills, no cloud subscriptions.\nThe EU AI Act becomes fully enforceable August 2026. Fines up to €35M or 7% of global revenue. If you're sending data to OpenAI/Anthropic APIs from the EU, compliance gets complex.\nRunning everything locally means GDPR-compliant by design. No data leaves your network.\nI wrote everything up as a detailed playbook: 8 chapters, ~70 pages, all docker-compose files and code examples included.\nCheck it out: ai-engineering.at\nQuest",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我在本地硬件上构建了一个生产级四代理AI堆栈——我学到了什么",
        "summary": "作者在低成本二手硬件上构建了一个完全本地、符合GDPR的四代理AI系统，月电费成本不到50欧元，结合Ollama、Neo4j、ChromaDB和n8n进行自主基础设施编排、合规性验证和工作流自动化。该堆栈证明了复杂的AI代理可以在没有云API的情况下运行，同时保持数据主权，直接解决了到2026年8月到来的EU AI法案合规性要求。这很重要，因为它证明了对于需要监管合规性和降低运营成本的组织而言，本地部署的可行性。"
      },
      "fr": {
        "title": "J'ai construit une pile d'IA à 4 agents en production sur du matériel local — Voici ce que j'ai appris",
        "summary": "L'auteur a construit un système d'IA à quatre agents entièrement local, conforme au RGPD, fonctionnant sur du matériel d'occasion modeste pour moins de 50 €/mois de frais d'électricité, combinant Ollama, Neo4j, ChromaDB et n8n pour l'orchestration autonome de l'infrastructure, la validation de la conformité et l'automatisation des flux de travail. La pile démontre que les agents d'IA sophistiqués peuvent fonctionner sans API cloud tout en maintenant la souveraineté des données, répondant directement aux exigences de conformité de la loi sur l'IA de l'UE qui entrent en vigueur en août 2026. C'est important parce que cela prouve la faisabilité du déploiement local pour les organisations nécessitant la conformité réglementaire et une réduction des coûts d'exploitation."
      },
      "de": {
        "title": "Ich habe einen Production 4-Agent-AI-Stack auf lokaler Hardware gebaut — Das habe ich gelernt",
        "summary": "Der Autor hat ein vollständig lokales, DSGVO-konformes Vier-Agent-AI-System auf bescheidener gebrauchter Hardware für weniger als 50 €/Monat Stromkosten gebaut und kombiniert Ollama, Neo4j, ChromaDB und n8n für autonome Infrastruktur-Orchestrierung, Compliance-Validierung und Workflow-Automatisierung. Der Stack demonstriert, dass ausgefeilte AI-Agenten ohne Cloud-APIs arbeiten können, während die Datensouveränität gewahrt bleibt, und spricht direkt die EU-AI-Act-Compliance-Anforderungen ab, die im August 2026 in Kraft treten. Dies ist wichtig, weil es die Machbarkeit lokaler Bereitstellung für Organisationen nachweist, die regulatorische Compliance und reduzierte Betriebskosten benötigen."
      },
      "es": {
        "title": "Construí una pila de IA de 4 agentes en producción en hardware local — Esto es lo que aprendí",
        "summary": "El autor construyó un sistema de IA de cuatro agentes completamente local y compatible con GDPR ejecutándose en hardware usado modesto por menos de 50 €/mes en costos de electricidad, combinando Ollama, Neo4j, ChromaDB y n8n para orquestación autónoma de infraestructura, validación de cumplimiento y automatización de flujos de trabajo. El stack demuestra que los agentes de IA sofisticados pueden operar sin API en la nube mientras mantienen la soberanía de datos, abordando directamente los requisitos de cumplimiento de la Ley de IA de la UE que entra en vigor en agosto de 2026. Esto importa porque comprueba la viabilidad del despliegue local para organizaciones que requieren cumplimiento regulatorio y costos operativos reducidos."
      }
    }
  },
  {
    "title": "Abstraction: Designing Systems That Don’t Collapse Under Complexity",
    "slug": "abstraction-designing-systems-dont-collapse-complexity",
    "url": "https://dev.to/walternascimentobarroso/abstraction-designing-systems-that-dont-collapse-under-complexity-3h29",
    "source": "DEV Community",
    "date": "2026-02-26T05:59:00.000Z",
    "summary": "Abstraction protects system architecture by defining behavior contracts rather than implementation details, allowing systems to evolve as infrastructure changes without modifying core business logic. The article illustrates this through a payment service example, showing how tight coupling to specific providers like Stripe creates fragility that forces rewrites when requirements change. This foundational principle enables systems to adapt to vendor switching, API evolution, and regional requirements without cascading changes.",
    "content": "Encapsulation protects invariants.\nAbstraction protects architecture.\nIf encapsulation controls state,\nAnd without it, your system slowly turns into a fragile web of concrete implementations.\nAbstraction became critical when software systems stopped being small.\nIn early OOP systems, objects communicated directly with concrete implementations.\nBut as systems grew:\nInfrastructure changed\nDatabases evolved\nAPIs were replaced\nVendors switched\nHard-coded dependencies became the biggest source of rigidity.\nAbstraction emerged as a way to:\nDepend on behavior contracts, not implementations.\nThat single idea made large systems survivable.\nAbstraction is:\nDefining behavior without exposing implementation\nProgramming against contracts\nIsolating high-level logic from low-level details\nReducing coupling\nAbstraction is not:\nJust creating interfaces everywhere\nAdding layers for no reason\nOver-engineering small systems\nAbstraction is about managing volatility.\nLet’s say we’re building a payment service.\nfinal class OrderService\n{\n    public function pay(float $amount): void\n    {\n        $stripe = new StripePaymentGateway();\n        $stripe->charge($amount);\n    }\n}\n\nWhat’s wrong?\nOrderService depends directly on Stripe\nImpossible to switch provider without editing business logic\nHard to test\nViolates dependency inversion\nInfrastructure leaks into domain logic\nThis is tight coupling.\nImagine:\nStripe increases fees\nYou must support PayPal\nA region requires a local provider\nStripe API changes\nNow you must modify core logic.\nYour domain is polluted by infrastructure decisions.\nThat’s architectural fragility.\nWe define a contract.\ninterface PaymentGateway\n{\n    public function charge(float $amount): void;\n}\n\nNow we create implementations.\nfinal class StripePaymentGateway implements PaymentGateway\n{\n    public function charge(float $amount): void\n    {\n        // Call Stripe API\n    }\n}\n\nfinal class PaypalPaymentGateway implements PaymentGateway\n{\n    public function charge(float $amou",
    "category": "github",
    "translations": {
      "zh": {
        "title": "抽象：设计不会在复杂性下崩溃的系统",
        "summary": "抽象通过定义行为契约而非实现细节来保护系统架构，允许系统随着基础设施的变化而演进，无需修改核心业务逻辑。文章通过支付服务示例阐述了这一点，展示了与特定提供商（如Stripe）的紧密耦合如何造成脆弱性，并在需求变化时强制重写。这一基本原则使系统能够适应供应商切换、API演进和地区要求，而不会产生级联变更。"
      },
      "fr": {
        "title": "Abstraction : Concevoir des systèmes qui ne s'effondrent pas sous la complexité",
        "summary": "L'abstraction protège l'architecture du système en définissant des contrats comportementaux plutôt que des détails d'implémentation, permettant aux systèmes d'évoluer à mesure que l'infrastructure change sans modifier la logique métier centrale. L'article illustre cela par un exemple de service de paiement, montrant comment le couplage étroit à des fournisseurs spécifiques comme Stripe crée une fragilité qui force les réécriture quand les exigences changent. Ce principe fondamental permet aux systèmes de s'adapter au changement de fournisseur, à l'évolution des API et aux exigences régionales sans changements en cascade."
      },
      "de": {
        "title": "Abstraktion: Systeme entwerfen, die unter Komplexität nicht zusammenbrechen",
        "summary": "Abstraktion schützt die Systemarchitektur, indem sie Verhaltensverträge anstelle von Implementierungsdetails definiert und Systemen ermöglicht, sich an sich ändernde Infrastruktur anzupassen, ohne die Geschäftslogik zu ändern. Der Artikel veranschaulicht dies anhand eines Zahlungsservice-Beispiels und zeigt, wie enge Koppelung an spezifische Anbieter wie Stripe Fragilität erzeugt, die bei Anforderungsänderungen zu Neuschreiben zwingt. Dieses grundlegende Prinzip ermöglicht Systemen, sich an Anbieter-Wechsel, API-Evolution und regionale Anforderungen anzupassen, ohne kaskadierende Änderungen zu verursachen."
      },
      "es": {
        "title": "Abstracción: Diseñar sistemas que no colapsen bajo la complejidad",
        "summary": "La abstracción protege la arquitectura del sistema al definir contratos de comportamiento en lugar de detalles de implementación, permitiendo que los sistemas evolucionen a medida que cambia la infraestructura sin modificar la lógica empresarial central. El artículo ilustra esto mediante un ejemplo de servicio de pago, mostrando cómo el acoplamiento estrecho a proveedores específicos como Stripe crea fragilidad que obliga a reescrituras cuando cambian los requisitos. Este principio fundamental permite que los sistemas se adapten al cambio de proveedor, la evolución de API y los requisitos regionales sin cambios en cascada."
      }
    }
  },
  {
    "title": "How we built a hybrid FTS5 + embedding search for code — and why you need both",
    "slug": "hybrid-fts5-embedding-search-code-why-need-both",
    "url": "https://dev.to/tofutim/how-we-built-a-hybrid-fts5-embedding-search-for-code-and-why-you-need-both-4ec2",
    "source": "DEV Community",
    "date": "2026-02-26T05:58:24.000Z",
    "summary": "Srclight's code search combines full-text indexing (FTS5) with semantic embeddings to handle both exact symbol matching and concept-based queries, overcoming limitations of either method alone for code with varying naming conventions. Using three specialized FTS5 indexes tuned for case changes, substrings, and word stems, plus semantic vectors merged via reciprocal rank fusion, the hybrid approach enables AI coding assistants to understand code literally and conceptually. This matters because practical code understanding requires both precision matching and semantic reasoning.",
    "content": "How we built a hybrid FTS5 + embedding search for code — and why you need both\n\n\n\nsrclight is a deep code indexing MCP server — it gives AI agents understanding of your codebase (symbol search, call graphs, git blame, semantic search) in a single pip install.\nWhen you're building AI coding assistants, you need search that works two ways:\nKeyword search — I know the function name, find it now\nSemantic search — find code that \"handles authentication\" without knowing the exact term\nMost tools pick one. We built both.\nFTS5 is great for exact matches. But code has naming conventions: calculateTotalPrice, calculate_total_price, CalculateTotalPrice. A single FTS5 index can't handle all of these well.\nAnd sometimes you don't know the name at all. You want to find \"code that validates user input\" — that's a concept, not a keyword.\nEmbeddings are great for meaning. But they struggle with:\nExact symbol names (searching for handleAuth should find handleAuth)\nSubstring matches (searching for parse should find parseJSON)\nShort queries (embeddings need context)\nNaming conventions\nWe built three FTS5 indexes, each tuned differently:\nSplits on case changes and underscores:\ncalculateTotalPrice → calculate, Total, Price\nhandle_user_auth → handle, user, auth\n\nThis catches CamelCase, snake_case, and any convention developers throw at it.\nIndexes every 3-character substring. This catches substring matches even inside words.\nStems words to their roots: \"running, ran, runner → run\". This makes docstring search actually useful.\nSemantic vectors for meaning-based matching. We use qwen3-embedding (4096 dims) or nomic-embed-text (768 dims).\nHere's how we combine them. We run each query against all 4 indexes, get ranked results, then merge using RRF:\nRRF_score(d) = Σ 1 / (k + rank(d))\n\nwhere k = 60 (standard constant).\nA result appearing at rank 1 in FTS5 and rank 2 in embeddings gets:\nFTS5: 1 / (60 + 1) = 0.0164\nEmbeddings: 1 / (60 + 2) = 0.0161\nTotal: 0.0325\nA result at rank 10 in embeddings",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我们如何构建混合FTS5 + 嵌入式代码搜索——以及为什么你需要两者",
        "summary": "Srclight的代码搜索将全文索引（FTS5）与语义嵌入相结合，处理精确符号匹配和基于概念的查询，克服了单一方法对具有不同命名约定的代码的限制。使用针对大小写变化、子字符串和词干调整的三个专门的FTS5索引，加上通过倒数排名融合合并的语义向量，混合方法使AI编码助手能够从字面和概念两个角度理解代码。这很重要，因为实际的代码理解需要精确匹配和语义推理的结合。"
      },
      "fr": {
        "title": "Comment nous avons construit une recherche de code hybride FTS5 + embeddings — et pourquoi vous avez besoin des deux",
        "summary": "La recherche de code de Srclight combine l'indexation en texte intégral (FTS5) avec des embeddings sémantiques pour gérer à la fois la correspondance de symboles exacts et les requêtes basées sur des concepts, surpassant les limitations de chaque méthode seule pour le code avec des conventions de nommage variables. En utilisant trois index FTS5 spécialisés ajustés pour les changements de casse, les sous-chaînes et les racines de mots, plus des vecteurs sémantiques fusionnés via la fusion de rang réciproque, l'approche hybride permet aux assistants de codage IA de comprendre le code littéralement et conceptuellement. C'est important car la compréhension pratique du code nécessite à la fois une correspondance précise et un raisonnement sémantique."
      },
      "de": {
        "title": "Wie wir eine hybride FTS5 + Embedding-Suche für Code erstellten — und warum Sie beide benötigen",
        "summary": "Srclights Code-Suche kombiniert Volltext-Indizierung (FTS5) mit semantischen Embeddings, um sowohl exakte Symbol-Übereinstimmung als auch konzeptbasierte Abfragen zu handhaben und Einschränkungen beider Methoden allein bei Code mit unterschiedlichen Namenskonventionen zu überwinden. Durch die Verwendung von drei spezialisierten FTS5-Indizes, die für Groß-/Kleinschreibung, Teilstrings und Wort-Stämme optimiert sind, sowie semantischen Vektoren, die durch reziproke Rank-Fusion zusammengefasst werden, ermöglicht der Hybrid-Ansatz KI-Coding-Assistenten, Code wörtlich und konzeptionell zu verstehen. Dies ist wichtig, da praktisches Code-Verständnis sowohl präzises Matching als auch semantisches Denken erfordert."
      },
      "es": {
        "title": "Cómo construimos una búsqueda híbrida FTS5 + embedding para código — y por qué necesitas ambas",
        "summary": "La búsqueda de código de Srclight combina indexación de texto completo (FTS5) con embeddings semánticos para manejar tanto coincidencias exactas de símbolos como consultas basadas en conceptos, superando limitaciones de cualquier método solo para código con convenciones de nombres variadas. Usando tres índices FTS5 especializados ajustados para cambios de mayúsculas, subcadenas y raíces de palabras, más vectores semánticos fusionados mediante fusión de rango recíproco, el enfoque híbrido permite a los asistentes de codificación con IA entender el código literal y conceptualmente. Esto importa porque la comprensión práctica del código requiere tanto coincidencia precisa como razonamiento semántico."
      }
    }
  },
  {
    "title": "Translating a Website into 8 Languages with AI Agents in One Night",
    "slug": "translating-website-8-languages-ai-agents-one-night",
    "url": "https://dev.to/brunoborges/translating-a-website-into-8-languages-with-ai-agents-in-one-night-50k7",
    "source": "DEV Community",
    "date": "2026-02-26T05:58:07.000Z",
    "summary": "Claude Sonnet 4.6 and GitHub Copilot Coding Agents automated internationalization of a Java patterns website from English-only to 9 languages including Arabic with RTL support in under 24 hours through architectural planning and collaborative PR generation. The approach separated UI strings from content translations with graceful English fallbacks, allowing agents to handle translations without complex field-filtering logic. This demonstrates how modern AI agents can orchestrate large-scale i18n projects that traditionally require months of manual coordination.",
    "content": "How I used Claude Sonnet 4.6 and fleets of GitHub Copilot Coding Agents to internationalize java.evolved — from spec to deployment\n\n\n\n\n\njava.evolved is a static site I built to showcase modern Java patterns side-by-side with their legacy equivalents. 112 patterns across 11 categories — language, collections, streams, concurrency, and more — each with code comparisons, explanations, and curated documentation links. All generated from YAML content files by a JBang-powered Java build script.\nBy the end of February 25, the entire site was English-only. By the morning of February 26, it was available in 9 languages — English, German, Spanish, Portuguese (Brazil), Simplified Chinese, Arabic, French, Japanese, and Korean — with full RTL support for Arabic. The total human effort was a few hours of prompting, reviewing PRs, and filing one bug.\nThis is the story of that experiment.\nThe first step wasn't writing code. It was writing a specification.\nI opened issue #74 — \"Plan architectural change for i18n\" — and assigned it to a Copilot Coding Agent. The prompt was simple: propose an architectural plan for internationalizing the website, considering the existing static-site structure.\nThe agent (PR #75) came back with a comprehensive i18n specification that addressed:\nTwo-layer translation model: UI strings (labels, nav, footer) separated from content translations (pattern titles, explanations, summaries)\nPartial translation files: Translation files contain only translatable fields. Structural data (code snippets, navigation links, metadata) always comes from the English source of truth\nGraceful fallback: Missing translations fall back to English with a build-time warning — no page is ever blank\nLocale registry: A simple locales.properties file drives the entire build pipeline and language selector\nAI-friendly design: The architecture was explicitly designed so that an AI receives the full English content and returns a partial translation file — no field-filtering logic neede",
    "category": "github",
    "translations": {
      "zh": {
        "title": "利用AI代理在一晚上将网站翻译成8种语言",
        "summary": "Claude Sonnet 4.6和GitHub Copilot Coding Agents通过架构规划和协作式PR生成，在24小时内自动将Java模式网站从仅英文国际化为9种语言（包括RTL支持的阿拉伯语）。该方法将UI字符串与内容翻译分离，采用优雅的英文回退，允许代理在没有复杂字段过滤逻辑的情况下处理翻译。这展示了现代AI代理如何能够编排传统上需要数月手动协调的大规模国际化项目。"
      },
      "fr": {
        "title": "Traduire un site Web en 8 langues avec des agents IA en une nuit",
        "summary": "Claude Sonnet 4.6 et GitHub Copilot Coding Agents ont automatisé l'internationalisation d'un site Web de modèles Java de l'anglais uniquement à 9 langues, y compris l'arabe avec support RTL en moins de 24 heures grâce à la planification architecturale et à la génération collaborative de PR. L'approche sépare les chaînes d'interface utilisateur des traductions de contenu avec des rétromigrations gracieuses en anglais, permettant aux agents de gérer les traductions sans logique complexe de filtrage de champs. Cela démontre comment les agents IA modernes peuvent orchestrer des projets d'internationalisation à grande échelle qui nécessitaient traditionnellement des mois de coordination manuelle."
      },
      "de": {
        "title": "Eine Website in einer Nacht mit KI-Agenten in 8 Sprachen übersetzen",
        "summary": "Claude Sonnet 4.6 und GitHub Copilot Coding Agents automatisierten die Internationalisierung einer Java-Muster-Website von nur Englisch auf 9 Sprachen, einschließlich Arabisch mit RTL-Unterstützung in weniger als 24 Stunden durch architektonische Planung und kollaborative PR-Generierung. Der Ansatz trennt UI-Strings von Content-Übersetzungen mit anmutigen englischen Fallbacks und ermöglicht Agenten, Übersetzungen ohne komplexe Feld-Filterlogik zu handhaben. Dies zeigt, wie moderne KI-Agenten großangelegte Internationalisierungsprojekte orchestrieren können, die traditionell Monate manuelle Koordination erfordern würden."
      },
      "es": {
        "title": "Traducir un sitio web a 8 idiomas con agentes de IA en una noche",
        "summary": "Claude Sonnet 4.6 y GitHub Copilot Coding Agents automatizaron la internacionalización de un sitio web de patrones Java de solo inglés a 9 idiomas, incluido árabe con soporte RTL en menos de 24 horas a través de planificación arquitectónica y generación colaborativa de PR. El enfoque separa cadenas de interfaz de usuario de traducciones de contenido con alternativas elegantes en inglés, permitiendo a los agentes manejar traducciones sin lógica compleja de filtrado de campos. Esto demuestra cómo los agentes de IA modernos pueden orquestar proyectos de internacionalización a gran escala que tradicionalmente requerían meses de coordinación manual."
      }
    }
  },
  {
    "title": "Introducing: 7.5 Days Soft Challenge...",
    "slug": "7-5-days-soft-challenge",
    "url": "https://dev.to/kriti_arora/75day-soft-challenge-5bdj",
    "source": "DEV Community",
    "date": "2026-02-26T05:53:29.000Z",
    "summary": "The 7.5 Day Soft Challenge proposes sustainable daily improvement over intense bursts, arguing that consistent 1% improvements compound into transformative skill development through subconscious learning during sleep cycles. Drawing on Atomic Habits principles, the article reframes personal development to prioritize systems-based daily practice over willpower-dependent extremes. This matters because it offers a psychologically grounded alternative to unsustainable challenge formats for building lasting professional and personal skills.",
    "content": "I remember a little while ago this \"75 Day Hard Challenge\" really took the world in a wave. Everyone was doing these challenges, 75 day hard placement challenge, 75 day hard dsa challenge, 75 day hard proposing to your crush challenge.... and so on and so on....\nI had never attempted to do it because I'm just not the kind of person who can do something for 75 days straight without ever doing it before. I am a seriously compounded person, and a little lazy as well. First for one day then I stop then 2 days streak then stop, then 4 days streak then stop.... And keeping up like this making small but consistent habits. \nBut I haven't invented this method. In reality, all strong things in the world which have depth and meaning are made like this. Nature works very very slowly, but it grows everyday. The human body, taking nutrients consistently everyday and a small baby grows into a full size human, without even us realising. Actually all growth happens under the hoods. I thinkn when we study everyday then the actually growth happens in our subconscious brain when we sleep. And if we keep doing it everyday everyday then it becomes a ridge in our brain and goes very very deep. \nBut don't listen to me, take it from James Clear, author of Atomic Habits who says that, \nSmall, daily 1% improvements (atomic habits) compound over time to create massive, transformative lifestyle changes, emphasizing that you do not rise to the level of your goals, but rather fall to the level of your systems.\nSo I wanted to do this 7.5 day soft dsa challenge, where I will solve easy problems but consistently, hoping to have this problem solving skill not just as an ornament but as an identity...",
    "category": "github",
    "translations": {
      "zh": {
        "title": "介绍：7.5天软挑战",
        "summary": "7.5天软挑战提议可持续的日常改进而非强势冲刺，主张一致的1%改进通过睡眠周期中的潜意识学习而复合成转变性的技能发展。基于原子习惯原理，该文章重构个人发展以优先考虑基于系统的日常实践，而非意志力依赖的极端做法。这很重要，因为它为建设持久的专业和个人技能提供了心理学基础的替代方案，替代不可持续的挑战格式。"
      },
      "fr": {
        "title": "Présentation : Défi Doux de 7,5 jours",
        "summary": "Le Défi Doux de 7,5 jours propose une amélioration quotidienne durable plutôt que des rafales intenses, arguant que des améliorations cohérentes de 1% se composent dans le développement de compétences transformatrices grâce à l'apprentissage subconscient pendant les cycles de sommeil. S'appuyant sur les principes des Habitudes Atomiques, l'article restructure le développement personnel pour prioriser la pratique quotidienne basée sur les systèmes plutôt que sur les extrêmes dépendants de la volonté. Cela importe car il offre une alternative ancrée psychologiquement aux formats de défi non durables pour construire des compétences professionnelles et personnelles durables."
      },
      "de": {
        "title": "Vorstellung: 7,5-Tage-Soft-Challenge",
        "summary": "Die 7,5-Tage-Soft-Challenge schlägt nachhaltige tägliche Verbesserung statt intensiver Sprints vor und argumentiert, dass konsistente 1%-Verbesserungen durch unbewusstes Lernen während Schlafzyklen zu transformativer Kompetenzenentwicklung führen. Basierend auf den Prinzipien der Atomaren Gewohnheiten strukturiert der Artikel Persönlichkeitsentwicklung neu, um systembasierte tägliche Praktiken gegenüber willenskraftabhängigen Extremen zu priorisieren. Dies ist wichtig, da es eine psychologisch fundierte Alternative zu nicht nachhaltigen Herausforderungsformaten für den Aufbau dauerhafter beruflicher und persönlicher Kompetenzen bietet."
      },
      "es": {
        "title": "Presentación: Desafío Suave de 7,5 Días",
        "summary": "El Desafío Suave de 7,5 Días propone una mejora diaria sostenible en lugar de ráfagas intensas, argumentando que mejoras consistentes del 1% se componen en desarrollo de habilidades transformador a través del aprendizaje subconsciente durante los ciclos de sueño. Basándose en los principios de Hábitos Atómicos, el artículo reformula el desarrollo personal para priorizar la práctica diaria basada en sistemas sobre extremos dependientes de la fuerza de voluntad. Esto es importante porque ofrece una alternativa con base psicológica a formatos de desafío insostenibles para construir habilidades profesionales y personales duraderas."
      }
    }
  },
  {
    "title": "The Problem With Tracking Conversations Like Pageviews",
    "slug": "problem-tracking-conversations-like-pageviews",
    "url": "https://dev.to/shubhampalriwala/the-problem-with-tracking-conversations-like-pageviews-29fk",
    "source": "DEV Community",
    "date": "2026-02-26T05:48:31.000Z",
    "summary": "Traditional analytics metrics (session count, time-on-page) fundamentally mislead AI product managers because they measure static content consumption, not dynamic conversations where products respond to user input. High engagement metrics combined with 4% week-8 retention reveals users are engaging but not finding value, exposing the inadequacy of pageview-based measurement for conversational AI. This matters because incorrect metrics lead to false confidence in products with critical retention problems, delaying necessary product changes.",
    "content": "Your session numbers look great. Your users are churning. Here's why event-based analytics was never built for conversational AI products, and what to do instead.\nThe Problem With Tracking Conversations Like Pageviews\nPicture this. You’re a PM at an AI startup, six months post-launch. You open the dashboard on a Monday morning and everything looks… fine? Session count is up 20% week over week. Average session length is 4 minutes and 30 seconds. DAU is climbing. You screenshot it and drop it in the investor update Slack channel.\nThen you look at retention.\nWeek 4 retention is 12%. Week 8 is 4%. Users are showing up, having conversations, and disappearing. The metrics say engagement is strong. The business says something is very wrong.\nHere’s the thing nobody tells you when you ship your first AI product: you’ve been tracking conversations like pageviews, and that’s why your dashboard lies to you every single morning.\nPerson staring at metrics dashboard looking confused\n\n^ every AI PM on Monday morning when the numbers look good but retention is falling off a cliff\nThe Pageview Was Built for a World Where Content Sits Still\n\n\nThe pageview metric was invented in the mid-90s to answer one question: did someone look at this thing? That’s it. A newspaper prints a story. Did you open it? Click. Pageview logged. The content doesn’t change based on what you do. It just sits there. You either consumed it or you didn’t.\nThis mental model spread everywhere. Clicks, sessions, time-on-page, bounce rate, page depth. All of it built on the same foundational assumption: the product is a static artifact and the user is moving through it. Engagement equals consumption. More clicks means more engagement. More engagement means more value.\nThat assumption held for 25 years. It made analytics what it is today.\nAnd then we shipped products where the product itself responds to what the user says. The entire premise collapsed, and most teams haven’t noticed yet.\nA conversation is NOT a stati",
    "category": "github",
    "translations": {
      "zh": {
        "title": "像跟踪页面浏览量一样跟踪对话的问题",
        "summary": "传统分析指标（会话数、页面停留时间）根本上误导AI产品经理，因为它们测量静态内容消费，而非产品响应用户输入的动态对话。高参与度指标结合4%的第8周保留率表明用户正在参与但未找到价值，暴露了基于页面浏览量的测量对对话AI的不足。这很重要，因为错误的指标导致对具有关键保留问题的产品产生虚假信心，延迟了必要的产品更改。"
      },
      "fr": {
        "title": "Le Problème du Suivi des Conversations Comme des Pages Vues",
        "summary": "Les métriques analytiques traditionnelles (nombre de sessions, temps sur la page) trompent fondamentalement les responsables de produits IA car elles mesurent la consommation de contenu statique, pas les conversations dynamiques où les produits répondent aux entrées des utilisateurs. Les métriques d'engagement élevées combinées à une rétention de 4% à la semaine 8 révèlent que les utilisateurs s'engagent mais ne trouvent pas de valeur, exposant l'inadéquation de la mesure basée sur les pages vues pour l'IA conversationnelle. Cela importe car les métriques incorrectes conduisent à une fausse confiance dans les produits avec des problèmes de rétention critiques, retardant les changements de produit nécessaires."
      },
      "de": {
        "title": "Das Problem mit der Nachverfolgung von Gesprächen wie Seitenaufrufen",
        "summary": "Traditionelle Analyticmetriken (Sitzungsanzahl, Zeit auf der Seite) täuschen KI-Produktmanager grundlegend, da sie statische Inhaltsnutzung messen, nicht dynamische Gespräche, in denen Produkte auf Benutzereingaben reagieren. Hohe Engagement-Metriken kombiniert mit 4% Beibehaltung in Woche 8 zeigen, dass Benutzer sich engagieren, aber keinen Wert finden, was die Unzulänglichkeit von seitenaufruf-basierter Messung für konversationelle KI aufdeckt. Dies ist wichtig, da falsche Metriken zu falscher Zuversicht in Produkten mit kritischen Bindungsproblemen führen und notwendige Produktänderungen verzögern."
      },
      "es": {
        "title": "El Problema de Rastrear Conversaciones como Vistas de Página",
        "summary": "Las métricas analíticas tradicionales (recuento de sesiones, tiempo en la página) engañan fundamentalmente a los gerentes de productos de IA porque miden el consumo de contenido estático, no conversaciones dinámicas donde los productos responden a la entrada del usuario. Las métricas de participación alta combinadas con retención del 4% en la semana 8 revelan que los usuarios se están participando pero no encuentran valor, exponiendo la insuficiencia de la medición basada en vistas de página para IA conversacional. Esto importa porque las métricas incorrectas conducen a una falsa confianza en productos con problemas críticos de retención, retrasando cambios de producto necesarios."
      }
    }
  },
  {
    "title": "Building a Cross-Platform File Search App With Tauri — Not Electron",
    "slug": "building-cross-platform-file-search-app-tauri-electron",
    "url": "https://dev.to/kazutaka-dev/building-a-cross-platform-file-search-app-with-tauri-not-electron-2nke",
    "source": "DEV Community",
    "date": "2026-02-26T05:42:53.000Z",
    "summary": "OmniFile, a Tauri and Rust-based file search application, achieves an 8MB installer and 30MB idle RAM versus Electron's 80MB+ and 150MB+ by leveraging native webviews and Rust's performance for file I/O. Using Tantivy for full-text search with indexed but unstored content, the application unifies search across Google Drive, Dropbox, SharePoint, and local files while maintaining privacy-first design. This technical comparison demonstrates Rust's advantages for resource-constrained desktop applications requiring intensive file operations.",
    "content": "Every knowledge worker I know has the same problem: files scattered across Google Drive, Dropbox, SharePoint, Slack, Notion, GitHub, and their local machine. When you need to find something, you end up opening 4 different search bars.\nI built OmniFile to fix that — a single search bar that finds files across all your sources instantly. Desktop app, privacy-first, everything stays on your machine.\nHere's what I learned building it with Tauri + Rust instead of Electron, and why integrating 7 OAuth providers in a desktop app was harder than I expected.\nThe decision was simple: OmniFile needs to launch instantly (it's triggered by a global shortcut) and stay lightweight in the background. Electron ships a full Chromium browser. Tauri uses the OS's native webview and a Rust backend.\nThe result:\n~8MB installer vs Electron's ~80MB+\n~30MB RAM at idle vs Electron's ~150MB+\nRust backend for CPU-intensive indexing and file I/O\nThe tradeoff is that you write your backend in Rust instead of JavaScript. For file search, that's actually a benefit — Rust's performance for walking directories and parsing file formats is hard to beat.\nTantivy is Rust's answer to Lucene. I use it as the local search engine that indexes everything into a single queryable index.\nschema_builder.add_text_field(\"title\", TEXT | STORED);      // Tokenized + returned\nschema_builder.add_text_field(\"path\", STRING | STORED);      // Exact match\nschema_builder.add_text_field(\"content\", TEXT);              // Searchable but NOT stored\nschema_builder.add_text_field(\"source\", STRING | STORED);    // \"local\", \"gdrive\", etc.\nschema_builder.add_i64_field(\"modified_at\", INDEXED | STORED);\n\nThe key decision: content is indexed but not stored. For a desktop search app, this saves significant disk space — the content is already on disk, so we re-extract it when needed for display. This keeps the index small while enabling full-text search.\nEach cloud provider indexes into the same Tantivy index but with a different source",
    "category": "github",
    "translations": {
      "zh": {
        "title": "使用Tauri构建跨平台文件搜索应用 — 而非Electron",
        "summary": "OmniFile是一个基于Tauri和Rust的文件搜索应用，通过利用原生webviews和Rust的文件I/O性能，实现了8MB安装程序和30MB空闲RAM，相比Electron的80MB+和150MB+。使用Tantivy进行全文搜索且内容已索引但未存储，该应用统一搜索Google Drive、Dropbox、SharePoint和本地文件，同时保持隐私优先设计。这种技术比较演示了Rust对资源受限的桌面应用程序的优势，这些应用需要密集的文件操作。"
      },
      "fr": {
        "title": "Construire une Application de Recherche de Fichiers Multiplateforme avec Tauri — Pas Electron",
        "summary": "OmniFile, une application de recherche de fichiers basée sur Tauri et Rust, réalise un installeur de 8 Mo et une RAM inactive de 30 Mo par rapport aux 80 Mo+ et 150 Mo+ d'Electron en exploitant les webviews natifs et les performances d'E/S de fichiers de Rust. Utilisant Tantivy pour la recherche en texte intégral avec le contenu indexé mais non stocké, l'application unifie la recherche sur Google Drive, Dropbox, SharePoint et les fichiers locaux tout en maintenant une conception centrée sur la confidentialité. Cette comparaison technique démontre les avantages de Rust pour les applications de bureau contraintes par les ressources nécessitant des opérations de fichiers intensives."
      },
      "de": {
        "title": "Erstellen einer plattformübergreifenden Dateisuch-App mit Tauri — Nicht Electron",
        "summary": "OmniFile, eine auf Tauri und Rust basierende Dateisuchsoftware, erreicht ein 8-MB-Installationsprogramm und 30-MB-Leerlauf-RAM im Vergleich zu Electrons 80MB+ und 150MB+, indem sie native Webviews und Rusts Leistung für Datei-E/A nutzt. Unter Verwendung von Tantivy für die Volltextsuche mit indiziertem aber nicht gespeichertem Inhalt vereinheitlicht die Anwendung die Suche auf Google Drive, Dropbox, SharePoint und lokalen Dateien, während sie ein datenschutzorientiertes Design beibehält. Dieser technische Vergleich zeigt Rusts Vorteile für ressourcenbeschränkte Desktopanwendungen, die intensive Dateivorgänge erfordern."
      },
      "es": {
        "title": "Construir una Aplicación de Búsqueda de Archivos Multiplataforma con Tauri — No Electron",
        "summary": "OmniFile, una aplicación de búsqueda de archivos basada en Tauri y Rust, logra un instalador de 8 MB y 30 MB de RAM inactiva en comparación con 80 MB+ y 150 MB+ de Electron al aprovechar las webviews nativas y el rendimiento de E/S de archivos de Rust. Utilizando Tantivy para búsqueda de texto completo con contenido indexado pero no almacenado, la aplicación unifica la búsqueda en Google Drive, Dropbox, SharePoint y archivos locales mientras mantiene un diseño centrado en la privacidad. Esta comparación técnica demuestra las ventajas de Rust para aplicaciones de escritorio con restricciones de recursos que requieren operaciones intensivas de archivos."
      }
    }
  },
  {
    "title": "CVE-2026-27575: The Zombie Session: Breaking Vikunja's Auth with CVE-2026-27575",
    "slug": "cve-2026-27575-zombie-session-breaking-vikunja-auth",
    "url": "https://dev.to/cverports/cve-2026-27575-the-zombie-session-breaking-vikunjas-auth-with-cve-2026-27575-pij",
    "source": "DEV Community",
    "date": "2026-02-26T05:40:19.000Z",
    "summary": "CVE-2026-27575 is a critical vulnerability (CVSS 9.1) in Vikunja before v2.0.0 allowing single-character passwords and failing to invalidate JWT sessions after password changes, enabling attackers with stolen tokens to maintain permanent access regardless of victim credential resets. The flaw demonstrates the architectural dangers of stateless JWTs without revocation mechanisms or input validation on password changes. This matters because it illustrates how session management failures in authentication systems create persistent account takeover risks in self-hosted platforms.",
    "content": "The Zombie Session: Breaking Vikunja's Auth with CVE-2026-27575\n\n\n\nVulnerability ID: CVE-2026-27575\nCVSS Score: 9.1\nPublished: 2026-02-25\nCVE-2026-27575 represents a catastrophic failure in the authentication lifecycle of Vikunja, a popular self-hosted task management platform. The vulnerability is a two-headed beast: first, it allowed users (and attackers) to set passwords with a single character, bypassing security policies during updates. Second, and far more critical, it failed to invalidate active sessions upon password changes. This means an attacker who steals a session token retains permanent access to the victim's data, even after the victim explicitly resets their credentials to 'lock them out.' It is a classic case of stateless JWTs being deployed without a revocation strategy.\nVikunja versions prior to 2.0.0 allow persistent account takeover. Due to a lack of input validation, passwords could be reset to a single character. Worse, changing a password did not invalidate existing JSON Web Tokens (JWTs). An attacker with a stolen token remains logged in indefinitely, regardless of the victim's remediation attempts. Fix: Upgrade to v2.0.0 immediately.\nCWE IDs: CWE-521 (Weak Password), CWE-613 (Insufficient Session Expiration)\nCVSS Score: 9.1 (Critical)\nAttack Vector: Network (API)\nPrivileges Required: None (for initial access via weak policy logic)\nExploit Status: PoC Available / Trivial\nPatch Date: 2026-02-25\nVikunja < 2.0.0\nVikunja: < 2.0.0 (Fixed in: 2.0.0)\n89c17d3\n\n\nEnforce password limits on update and reset\ntype UserPassword struct {\n- NewPassword string `json:\"new_password\"`\n+ NewPassword string `json:\"new_password\" valid:\"minLength:8\"`\n}\n\n2526853\n\n\nRefactor session management to stateful tokens\n// Logic added to invalidate sessions on password change\n\nEnforce minimum password complexity on all inputs, not just registration.\nImplement stateful session management or token denylists.\nInvalidate all active sessions upon password rotation.\nRemediation Ste",
    "category": "github",
    "translations": {
      "zh": {
        "title": "CVE-2026-27575：僵尸会话：破坏Vikunja认证的CVE-2026-27575",
        "summary": "CVE-2026-27575是Vikunja v2.0.0之前的严重漏洞（CVSS 9.1），允许单字符密码且在密码更改后未能使JWT会话失效，使得攻击者能够使用被盗令牌保持永久访问权限，无论受害者如何重置凭证。该漏洞展示了无状态JWT在没有撤销机制或密码变更输入验证情况下的架构风险。这很重要，因为它说明了认证系统中会话管理失败如何在自托管平台中造成持久的账户接管风险。"
      },
      "fr": {
        "title": "CVE-2026-27575 : La session zombie : Briser l'authentification de Vikunja avec CVE-2026-27575",
        "summary": "CVE-2026-27575 est une vulnérabilité critique (CVSS 9.1) dans Vikunja antérieur à v2.0.0 permettant des mots de passe d'un seul caractère et ne parvenant pas à invalider les sessions JWT après les changements de mot de passe, permettant aux attaquants disposant de jetons volés de maintenir un accès permanent indépendamment de la réinitialisation des identifiants des victimes. La faille démontre les dangers architecturaux des JWT sans état sans mécanismes de révocation ou validation d'entrée lors des changements de mot de passe. C'est important car cela illustre comment les défaillances de gestion de session dans les systèmes d'authentification créent des risques persistants de prise de compte dans les plates-formes autohébergées."
      },
      "de": {
        "title": "CVE-2026-27575: Die Zombie-Sitzung: Vikunjas Authentifizierung mit CVE-2026-27575 brechen",
        "summary": "CVE-2026-27575 ist eine kritische Sicherheitslücke (CVSS 9.1) in Vikunja vor v2.0.0, die Ein-Zeichen-Passwörter ermöglicht und JWT-Sitzungen nach Passwortänderungen nicht ungültig macht, wodurch Angreifer mit gestohlenen Token unabhängig von den Anmeldedaten-Zurückstellungen des Opfers permanenten Zugriff behalten können. Die Schwachstelle demonstriert die architektonischen Gefahren zustandsloser JWTs ohne Widerrufsmechanismen oder Eingabevalidierung bei Passwortänderungen. Dies ist wichtig, da es zeigt, wie Fehler bei der Sitzungsverwaltung in Authentifizierungssystemen persistente Kontoübernahmevorkehrungen in selbstgehosteten Plattformen schaffen."
      },
      "es": {
        "title": "CVE-2026-27575: La sesión zombi: Romper la autenticación de Vikunja con CVE-2026-27575",
        "summary": "CVE-2026-27575 es una vulnerabilidad crítica (CVSS 9.1) en Vikunja anterior a v2.0.0 que permite contraseñas de un solo carácter e incapacidad para invalidar sesiones JWT después de cambios de contraseña, permitiendo a atacantes con tokens robados mantener acceso permanente independientemente de restablecimientos de credenciales de víctimas. La falla demuestra los peligros arquitectónicos de JWT sin estado sin mecanismos de revocación o validación de entrada en cambios de contraseña. Esto importa porque ilustra cómo las fallas de gestión de sesión en sistemas de autenticación crean riesgos persistentes de apropiación de cuentas en plataformas autohospedadas."
      }
    }
  },
  {
    "title": "🇹🇷 Türkiye devs: Add TC Kimlik Numarası to the AI identity standard — Soulprint open source (30 min PR)",
    "slug": "turkiye-devs-add-tc-kimlik-soulprint-open-source-pr",
    "url": "https://dev.to/manuel_felipeariaspined/turkiye-devs-add-tc-kimlik-numarasi-to-the-ai-identity-standard-soulprint-open-source-30-min-4jli",
    "source": "DEV Community",
    "date": "2026-02-26T05:33:47.000Z",
    "summary": "The post invites Turkish developers to contribute a TC Kimlik Numarası (Turkish national ID) verifier to the Soulprint open-source identity standard by implementing a 30-minute pull request with validation algorithms. The implementation enables AI agents to verify Turkish identity documents using checksum formulas on 11-digit IDs, expanding identity verification across emerging markets. This matters because crowdsourced identity standard development demonstrates how open-source projects can democratize AI verification capabilities globally.",
    "content": "AI ajanları her gün kimlik doğrulaması olmadan kararlar alıyor. Soulprint — ZK Proofs, yerel, MIT.\n🇹🇷 TC Kimlik Numarası Soulprint'te henüz yok. 30 dakikada PR gönderebilirsin.\nhane: ((1+3+5+7+9. basamak)*7 − (2+4+6+8. basamak)) mod 10.\nhane: ilk 10 basamak toplamı mod 10.\n\n\n\n\nconst TR: CountryVerifier = {\n  countryCode: \"TR\", countryName: \"Turkey\",\n  documentTypes: [\"tc_kimlik\"],\n  parse(ocrText: string): DocumentResult {\n    const tc = ocrText.match(/(\\d{11})/)?.[1] ?? \"\";\n    return { valid: !!tc, doc_number: tc, country: \"TR\" };\n  },\n  validate(docNumber: string): NumberValidation {\n    if(!/^\\d{11}$/.test(docNumber)||docNumber[0]===\"0\") return {valid:false};\n    const d=docNumber.split(\"\").map(Number);\n    const c10=((d[0]+d[2]+d[4]+d[6]+d[8])*7-(d[1]+d[3]+d[5]+d[7]))%10;\n    const c11=d.slice(0,10).reduce((a,b)=>a+b,0)%10;\n    return { valid: d[9]===c10 && d[10]===c11 };\n  },\n};\nexport default TR;\n\n💻 GitHub · Bir PR. Bir ülke.",
    "category": "github",
    "translations": {
      "zh": {
        "title": "🇹🇷 土耳其开发者：将TC Kimlik Numarası添加到AI身份标准——Soulprint开源（30分钟PR）",
        "summary": "该帖子邀请土耳其开发者通过实现具有验证算法的30分钟拉取请求，向Soulprint开源身份标准贡献TC Kimlik Numarası（土耳其国家ID）验证器。该实现使AI代理能够使用11位数字ID的校验和公式验证土耳其身份文件，扩展了新兴市场的身份验证。这很重要，因为众包身份标准开发演示了开源项目如何能在全球范围内民主化AI验证能力。"
      },
      "fr": {
        "title": "🇹🇷 Développeurs turcs : Ajouter TC Kimlik Numarası à la norme d'identité IA — Soulprint open source (30 min PR)",
        "summary": "Le message invite les développeurs turcs à contribuer un vérificateur TC Kimlik Numarası (ID national turc) à la norme d'identité open-source Soulprint en implémentant une demande d'extraction de 30 minutes avec des algorithmes de validation. L'implémentation permet aux agents IA de vérifier les documents d'identité turcs en utilisant des formules de somme de contrôle sur les ID à 11 chiffres, élargissant la vérification d'identité sur les marchés émergents. C'est important car le développement collaboratif de normes d'identité démontre comment les projets open-source peuvent démocratiser les capacités de vérification IA à l'échelle mondiale."
      },
      "de": {
        "title": "🇹🇷 Türkische Entwickler: TC Kimlik Numarası zur AI-Identitätsnorm hinzufügen — Soulprint Open Source (30-minütiges PR)",
        "summary": "Der Beitrag lädt türkische Entwickler ein, einen TC Kimlik Numarası (türkische nationale ID) Verifizierer zum Soulprint Open-Source-Identitätsstandard beizutragen, indem eine 30-Minuten-Pull-Request mit Validierungsalgorithmen implementiert wird. Die Implementierung ermöglicht es KI-Agenten, türkische Identitätsdokumente unter Verwendung von Prüfsummiformeln auf 11-stelligen IDs zu überprüfen und erweitert die Identitätsüberprüfung auf Schwellenländern. Dies ist wichtig, da die Entwicklung von crowdsourced-Identitätsstandards zeigt, wie Open-Source-Projekte KI-Verifizierungsfähigkeiten weltweit demokratisieren können."
      },
      "es": {
        "title": "🇹🇷 Desarrolladores turcos: Agregar TC Kimlik Numarası al estándar de identidad de IA — Soulprint de código abierto (PR de 30 minutos)",
        "summary": "El mensaje invita a los desarrolladores turcos a contribuir un verificador de TC Kimlik Numarası (ID nacional turco) al estándar de identidad de código abierto Soulprint implementando una solicitud de extracción de 30 minutos con algoritmos de validación. La implementación permite que los agentes de IA verifiquen documentos de identidad turcos utilizando fórmulas de suma de verificación en ID de 11 dígitos, expandiendo la verificación de identidad en mercados emergentes. Esto importa porque el desarrollo colaborativo de normas de identidad demuestra cómo los proyectos de código abierto pueden democratizar las capacidades de verificación de IA a nivel mundial."
      }
    }
  },
  {
    "title": "NABARD Grade A 2025 — eligibility, syllabus, and strategy",
    "slug": "nabard-grade-a-2025-eligibility-syllabus-strategy",
    "url": "https://dev.to/sabya_beworld_e066e3758d8/nabard-grade-a-2025-eligibility-syllabus-and-strategy-2n5l",
    "source": "DEV Community",
    "date": "2026-02-26T05:29:21.000Z",
    "summary": "The NABARD Grade A 2025 exam guide specifies eligibility criteria (age 25-35, bachelor's degree, 2+ years rural banking experience) and outlines a three-phase structure covering general English, reasoning, quantitative aptitude, plus specialized topics in agriculture and economics. The comprehensive syllabus overview provides a framework for candidates preparing for India's National Bank for Agriculture and Rural Development assistant manager positions. This matters because it clarifies requirements for accessing rural development career opportunities in India's banking sector.",
    "content": "NABARD Grade A 2025 — Unlock Your Dream Job in Rural Banking\n\n\nAre you ready to make a difference in rural India? NABARD Grade A is an exciting opportunity for young professionals like you to join the National Bank for Agriculture and Rural Development (NABARD) as Assistant Managers. In this blog post, we'll guide you through the eligibility criteria, syllabus, and strategy to crack the exam.\nEligibility Criteria: Don't Miss Out\n\n\nBefore diving into the preparation phase, let's ensure you meet the basic requirements:\nAge Limit: 25-35 years (relaxation for reserved categories)\nEducation: Bachelor's degree in any discipline from a recognized university\nWork Experience: Minimum 2 years of experience in rural banking or a related field\nIf you've checked off all these boxes, congratulations! You're eligible to apply. But remember, meeting the eligibility criteria is just the starting point.\nSyllabus: Understand What's at Stake\n\n\nThe NABARD Grade A exam consists of three phases:\n Phase I: Multiple-choice questions (MCQs) in General English, Reasoning Ability, and Quantitative Aptitude\n Phase II: Descriptive tests in English, Agriculture, Economics, and Finance\n Final Interview: Assess your communication skills and knowledge\nAccording to JobSafal.com (https://jobsafal.com), a reliable resource for banking exam aspirants, the syllabus is vast but manageable with focused preparation.\nPhase I Syllabus: Prepare Wisely\n\n\n\nGeneral English:\n\n\nGrammar\nVocabulary\nComprehension\nReasoning Ability:\n\n\nLogical reasoning\nData interpretation\nAnalytical reasoning\nQuantitative Aptitude:\n\n\nNumber systems\nAlgebra\nGeometry\nPhase II Syllabus: Dive into the Details\n\n\n\nEnglish:\n\n\nGrammar\nVocabulary\nComprehension\nAgriculture:\n\n\nCrop management\nSoil science\nAgricultural economics\nEconomics:\n\n\nMicroeconomics\nMacroeconomics\nPublic finance\nFinance:\n\n\nFinancial markets\nBanking and insurance\nAccounting\nStudy Schedule: Stay on Track\n\n\nTo ensure you don't miss out on any topic, create a study schedule wit",
    "category": "github",
    "translations": {
      "zh": {
        "title": "NABARD等级A 2025——资格、大纲和策略",
        "summary": "NABARD等级A 2025考试指南指定了资格标准（年龄25-35岁、学士学位、2年以上农村银行经验），并概述了涵盖一般英语、推理、定量能力以及农业和经济学专业主题的三阶段结构。全面的大纲概述为准备印度国家农业和农村发展银行助理经理职位的候选人提供了框架。这很重要，因为它澄清了获取印度银行部门农村发展职业机会的要求。"
      },
      "fr": {
        "title": "NABARD Grade A 2025 — admissibilité, programme et stratégie",
        "summary": "Le guide d'examen NABARD Grade A 2025 spécifie les critères d'admissibilité (âge 25-35 ans, diplôme d'une licence, 2+ ans d'expérience dans les banques rurales) et décrit une structure en trois phases couvrant l'anglais général, le raisonnement, l'aptitude quantitative, plus les sujets spécialisés en agriculture et en économie. L'aperçu complet du programme offre un cadre aux candidats se préparant pour les postes de gestionnaire adjoint de la Banque nationale pour l'agriculture et le développement rural de l'Inde. C'est important car cela clarifie les exigences pour accéder aux opportunités de carrière en développement rural dans le secteur bancaire indien."
      },
      "de": {
        "title": "NABARD Grade A 2025 — Berechtigung, Lehrplan und Strategie",
        "summary": "Der Prüfungsleitfaden NABARD Grade A 2025 gibt die Zulassungskriterien an (Alter 25-35 Jahre, Bachelorabschluss, 2+ Jahre Erfahrung im ländlichen Bankwesen) und skizziert eine dreiphasige Struktur mit allgemeinem Englisch, Argumentation, quantitativen Fähigkeiten sowie spezialisierten Themen in Landwirtschaft und Wirtschaft. Der umfassende Lehrplanüberblick bietet einen Rahmen für Kandidaten, die sich auf die Positionen des stellvertretenden Managers der indischen Nationalbank für Landwirtschaft und Landentwicklung vorbereiten. Dies ist wichtig, da es die Anforderungen für den Zugang zu Karrieremöglichkeiten in der ländlichen Entwicklung im indischen Bankensektor verdeutlicht."
      },
      "es": {
        "title": "NABARD Grado A 2025 — elegibilidad, plan de estudios y estrategia",
        "summary": "La guía del examen NABARD Grado A 2025 especifica los criterios de elegibilidad (edad 25-35 años, licenciatura, 2+ años de experiencia en banca rural) y describe una estructura de tres fases que cubre inglés general, razonamiento, aptitud cuantitativa, más temas especializados en agricultura y economía. La descripción general completa del plan de estudios proporciona un marco para los candidatos que se preparan para las posiciones de gerente asistente del Banco Nacional para la Agricultura y Desarrollo Rural de India. Esto importa porque aclara los requisitos para acceder a oportunidades de carrera en desarrollo rural en el sector bancario indio."
      }
    }
  },
  {
    "title": "Next.js 앱을 하루만에 6개국어로 만든 방법",
    "slug": "nextjs-app-six-languages-one-day",
    "url": "https://dev.to/ji_ai/nextjs-aebeul-harumane-6gaegugeoro-mandeun-bangbeob-pi5",
    "source": "DEV Community",
    "date": "2026-02-26T00:05:57.000Z",
    "summary": "This article details implementing internationalization in Next.js 15 using next-intl, covering locale-based routing architecture, translation file management, and region-specific pricing strategies to support rapid deployment across six countries and diverse markets.",
    "content": "사주 앱을 6개국에 내놓기로 했다. 한국, 미국, 일본, 중국, 베트남, 인도.\n사주가 동아시아 문화권 밖에서 먹힐까? 모르겠다. 근데 타로와 점성술이 전세계에서 먹히는 걸 보면, \"AI가 당신의 운명을 분석합니다\"는 어디서든 클릭을 부를 것 같았다.\n문제는 하드코딩된 한국어가 모든 페이지에 박혀 있다는 거다.\nNext.js 15 App Router에서 i18n 옵션은 몇 가지 있다. next-intl을 고른 이유는 단순하다 — App Router 네이티브 지원이 가장 깔끔하다. [locale] 동적 세그먼트에 미들웨어로 자동 리디렉트. Server Component에서도 Client Component에서도 같은 useTranslations() 훅.\napps/web/\n├── app/\n│   ├── [locale]/          ← 모든 페이지가 여기 안으로\n│   │   ├── page.tsx\n│   │   ├── result/page.tsx\n│   │   └── layout.tsx     ← html lang={locale} 여기서\n│   └── layout.tsx          ← 빈 껍데기\n├── i18n/\n│   ├── config.ts           ← locales, defaultLocale\n│   ├── routing.ts          ← localePrefix: \"as-needed\"\n│   └── navigation.ts       ← i18n Link, useRouter\n├── messages/\n│   ├── ko.json\n│   ├── en.json\n│   ├── ja.json\n│   ├── zh.json\n│   ├── vi.json\n│   └── hi.json\n└── middleware.ts            ← Accept-Language 감지\n\nlocalePrefix: \"as-needed\"가 핵심이다. 한국어가 디폴트니까 /로 접속하면 한국어, /en/으로 가면 영어. 한국 사용자는 URL에 /ko/가 안 붙는다.\nNext.js App Router에서 root layout은 반드시 <html>과 <body>를 렌더링해야 한다고 알고 있었다. 그래서 root layout에도 넣고, [locale]/layout.tsx에도 <html lang={locale}>을 넣었다.\n결과: html 안에 html. 브라우저는 조용히 무시하지만 완전히 잘못된 구조다.\n// app/layout.tsx — 이게 정답\nexport default function RootLayout({ children }) {\n  return children;  // html/body 없이 그냥 패스스루\n}\n\n// app/[locale]/layout.tsx — 여기서 html/body 관리\nexport default function LocaleLayout({ children, params }) {\n  return (\n    <html lang={locale}>\n      <body>{children}</body>\n    </html>\n  );\n}\n\nroot layout이 그냥 children만 리턴해도 Next.js 15에서는 에러가 안 난다. [locale] layout이 html/body를 제공하니까.\n같은 서비스라도 인도에서 $9.90을 받으면 아무도 안 산다. 각 나라 구매력에 맞춰 가격을 잡았다.\n// ko.json\n\"price\": \"₩12,900\"\n\n// en.json\n\"price\": \"$9.90\"\n\n// ja.json\n\"price\": \"¥1,490\"\n\n// zh.json\n\"price\": \"¥68\"\n\n// vi.json\n\"price\": \"199.000₫\"\n\n// hi.json\n\"price\": \"₹799\"\n\n번역 파일에 가격을 하드코딩한 거다. 나중에 결제 연동하면 서버에서 내려주겠지만, MVP 단계에서는 이게 가장 빠르다. placeholder 이름도 로컬라이즈했다 — 한국은 \"홍길동\", 일본은 \"山田太郎\", 인도는 \"राहुल शर्मा\".\napp/page.tsx를 app/[locale]/page.tsx로 옮",
    "category": "github",
    "translations": {
      "zh": {
        "title": "如何在一天内用6种语言构建Next.js应用",
        "summary": "本文详细介绍了使用next-intl在Next.js 15中实现国际化的方法，涵盖基于地区的路由架构、翻译文件管理和地域特定的定价策略，以支持在六个国家和不同市场中的快速部署。"
      },
      "fr": {
        "title": "Comment créer une application Next.js en six langues en une journée",
        "summary": "Cet article détaille l'implémentation de l'internationalisation dans Next.js 15 en utilisant next-intl, couvrant l'architecture de routage basée sur les paramètres régionaux, la gestion des fichiers de traduction et les stratégies de tarification spécifiques aux régions pour soutenir le déploiement rapide sur six pays et différents marchés."
      },
      "de": {
        "title": "Wie man eine Next.js-App in sechs Sprachen an einem Tag erstellt",
        "summary": "Dieser Artikel beschreibt die Implementierung der Internationalisierung in Next.js 15 mit next-intl und behandelt die Routing-Architektur nach Gebietsschema, die Verwaltung von Übersetzungsdateien und regionsspezifische Preisstrategien, um die schnelle Bereitstellung in sechs Ländern und verschiedenen Märkten zu unterstützen."
      },
      "es": {
        "title": "Cómo crear una aplicación Next.js en seis idiomas en un día",
        "summary": "Este artículo detalla la implementación de la internacionalización en Next.js 15 usando next-intl, cubriendo la arquitectura de enrutamiento basada en configuración regional, la gestión de archivos de traducción y estrategias de precios específicas por región para respaldar la implementación rápida en seis países y diversos mercados."
      }
    }
  },
  {
    "title": "Claude 하나로 1인 SaaS 전체를 설계한 기록",
    "slug": "claude-designed-saas-entire-stack-one-session",
    "url": "https://dev.to/ji_ai/claude-hanaro-1in-saas-jeoncereul-seolgyehan-girog-44h5",
    "source": "DEV Community",
    "date": "2026-02-26T00:01:21.000Z",
    "summary": "This article documents using Claude AI to comprehensively design a Korean fortune-telling SaaS, demonstrating how iterative conversations can simulate expert panels, identify business strategy gaps, and generate production-ready plans—replacing what would cost thousands in consulting.",
    "content": "\"로그인 벽부터 제거하세요. magic link 인증이 최대 이탈 원인입니다.\"\n이 말을 한 건 사람이 아니다. Claude가 시뮬레이션한 \"PM 역할의 가상 전문가\"다.\n하루 동안 Claude랑 9개 세션을 했다. 나온 산출물이 20개다. 사업 전략서, 랜딩 디자인, 전문가 패널 회의록, LLM 비용 분석서, 글로벌 확장 전략서, Claude Code 실행용 태스크 파일들.\n이걸 컨설팅 회사에 맡겼으면 몇 주에 몇 천만원이다.\n혼자, 하루, $0.\n처음부터 전부를 시킨 게 아니다. 대화가 깊어지면서 구체화됐다.\n1턴: \"사주 앱 사업성 어때?\" (추상적)\n2턴: \"무료/유료 나눠서 수익 모델 짜줘\" (구체적)\n3턴: \"무료 티어 API 비용을 토큰 단위로 계산해줘\" (매우 구체적)\n4턴: \"Prompt Caching 적용 시 시나리오 A/B/C 비교해줘\" (초구체적)\n\n한 번에 \"사업계획서 써줘\"라고 하면 일반적인 답변이 나온다.\n점진적으로 깊이를 올리면, 각 단계에서 AI가 이전 맥락을 다 갖고 있으니까 결과물이 훨씬 정밀해진다.\n가장 효과가 좋았던 건 \"전문가 패널 시뮬레이션\"이다.\n나: \"전문가 6명을 구성해줘.\n    PM 1명, 사업개발 1명, 로컬라이제이션 1명,\n    미국 시장 전문가 1명, 풀스택 개발자 1명, UI/UX 디자이너 1명.\n    각자 이름이랑 관점을 정해줘.\n    현재 상태(STATUS.md)를 리뷰하고 회의해줘.\"\n\nClaude가 6명의 캐릭터를 만들어서 각자의 관점으로 토론한다. PM이 우선순위를 짜고, 사업 담당이 시장성을 따지고, 개발자가 기술 난이도를 짚고, 디자이너가 UX 이슈를 제기한다.\n혼자 사업하면 \"내 관점\"밖에 없다. 이걸 쓰면 6개 관점이 동시에 나온다.\n물론 진짜 전문가 6명과는 다르다. 하지만 1인 개발자가 놓치기 쉬운 사각지대를 잡아내는 데는 충분하다.\n로그인 벽 제거가 첫 번째였다. magic link 인증이 최대 이탈 원인이라는 걸 PM이 지적했다. 무료 분석은 완전 비로그인으로 전환.\n무료 티어 비용도 94% 잘라냈다. LLM 풀 호출 대신 알고리즘 포맷팅에 AI 1줄 요약만 붙이는 구조로 건당 $0.085 → $0.005.\n사업자등록은 유저 반응 기다리지 말고 결제 연동을 위해 즉시 시작하기로 했다.\nGA4와 Rate Limiting은 필수라는 데 전원 동의했다. 분석 없이 개선은 눈감고 운전이고, 보호 없는 무료 API는 비용 폭탄이다.\n카카오 공유와 OG 이미지를 우선 구현하기로 했다. 사주 서비스의 유일한 무료 마케팅은 바이럴이다.\n글로벌 확장은 보류. 한국에서 유료 전환율 3% 넘기 전까지 영문화에 리소스 안 쓴다.\n이 결정들을 혼자 앉아서 다 생각해냈을까? 솔직히, 사업자등록 즉시 진행이랑 Rate Limiting은 나중에야 생각했을 거다.\n다른 프로젝트에도 그대로 적용할 수 있다. 비전 공유로 큰 그림을 그리고, 전략 수립에서 선택지를 결정하고, 전문가 패널로 검증하고, 디자인을 실물 HTML로 뽑고, 비용을 토큰 단위로 분석하고, 확장 아키텍처를 설계하고, 마지막에 Claude Code에서 실행 가능한 태스크 파일로 변환한다.\nAI를 도구로 쓰는 것과 AI와 사고하는 것은 다르다.\n도구로 쓰면 \"코드 써줘.\" 사고하면 \"이 구조가 맞아? 빠진 거 없어? 내가 놓치는 관점이 뭐야?\"\n후자가 1인 개발자한테는 훨씬 가치가 크다.\n한 가지 솔직한 고백. 이 패널에서 나온 숫자들 — \"30% 전환율\", \"₹99가 최적가\" 같은 것 — 에는 근거가 없다. Claude가 만든 가설이지, 데이터에서 나온 결론이 아니다.\n가설은 가설로만 취급하고, 검증은 런칭 후 실제 데이터로 한다.\n\"AI와 사고하면 6개 관점이 동시에 나온다. 진짜 전문가 6명은 아니지만, 혼자보다는 훨씬 낫다.\"",
    "category": "github",
    "translations": {
      "zh": {
        "title": "仅用Claude设计整个独立SaaS的记录",
        "summary": "本文记录了使用Claude AI全面设计一个韩国算命SaaS的过程，展示了迭代对话如何能够模拟专家小组、识别商业战略漏洞和生成生产就绪的计划——替代花费数千元咨询的方案。"
      },
      "fr": {
        "title": "Enregistrement de la conception d'une SaaS entière en solo avec Claude",
        "summary": "Cet article documente l'utilisation de Claude AI pour concevoir de manière complète une SaaS coréenne de prédiction de fortune, démontrant comment les conversations itératives peuvent simuler des panels d'experts, identifier les lacunes des stratégies commerciales et générer des plans prêts pour la production—remplaçant ce qui coûterait des milliers en consulting."
      },
      "de": {
        "title": "Aufzeichnung der Gestaltung eines gesamten Solo-SaaS mit Claude",
        "summary": "Dieser Artikel dokumentiert die Verwendung von Claude AI, um ein koreanisches Wahrsage-SaaS umfassend zu gestalten, und zeigt, wie iterative Gespräche Expert-Panels simulieren, Lücken in der Geschäftsstrategie identifizieren und produktionsbereite Pläne erstellen können—was Tausende an Beratungskosten ersetzt."
      },
      "es": {
        "title": "Registro del diseño de un SaaS completo en solitario con Claude",
        "summary": "Este artículo documenta el uso de Claude AI para diseñar de manera integral un SaaS coreano de adivinación de fortuna, demostrando cómo las conversaciones iterativas pueden simular paneles de expertos, identificar brechas en la estrategia comercial y generar planes listos para producción—reemplazando lo que costaría miles en consultoría."
      }
    }
  },
  {
    "title": "Understanding IP Management in Oracle Cloud Infrastructure (OCI)",
    "slug": "oracle-cloud-infrastructure-ip-management-guide",
    "url": "https://dev.to/hiltonj/understanding-ip-management-in-oracle-cloud-infrastructure-oci-1ili",
    "source": "DEV Community",
    "date": "2026-02-26T00:01:10.000Z",
    "summary": "A comprehensive guide to OCI's IP address management system covering private IPs for internal communication, public IPs for internet access, and advanced features like reserved IPs and bring-your-own-IP options, essential for building secure cloud infrastructure.",
    "content": "Navigating the complexities of cloud networking is crucial for building robust and scalable applications. In Oracle Cloud Infrastructure (OCI), effective IP address management forms the backbone of your network architecture. This guide will demystify OCI's IP address categories, explore their use cases, and introduce advanced concepts like Reserved Public IPs, Bring Your Own IP (BYOIP), and Public IP Pools. \nOCI categorizes IP addresses into two primary types, each serving distinct communication needs. \nThese are used for internal communication within your OCI network and with connected on-premises environments. \nInternal Communication: Instances within the same Virtual Cloud Network (VCN) communicate seamlessly using private IPs.\nVCN Peering: Connecting multiple VCNs, whether in the same or different regions, relies on private IP routing.\nOn-premises Connectivity: Secure connections to your data centers via the Dynamic Routing Gateway (DRG).\nInstance Allocation: Each instance receives at least one primary private IP.\nVNIC Capacity: Every Virtual Network Interface Card (VNIC) includes one primary private IP address and supports up to 32 secondary private IP addresses, totaling 33 private IPs per VNIC.\nThese are designed for internet accessibility, allowing your resources to communicate with the outside world. \nInternet Reachability: Public IPs are reachable from the internet, assigned to a private IP object on your OCI resource. \nPrerequisites: For a public IP to function, your VCN requires an Internet Gateway, and the associated public subnet must have correctly configured Route Tables and Security Lists. \nFlexibility: Resources can be assigned multiple public IPs across single or multiple VNICs. \nOCI offers two types of public IP addresses to cater to different operational requirements.\n\nReserved Public IP Addresses in Detail\nCreation: You create them individually. \nLimits: Up to 50 Reserved Public IPs are allowed per region. \nAssignment: Assigned to resources aft",
    "category": "github",
    "translations": {
      "zh": {
        "title": "了解Oracle云基础设施(OCI)中的IP管理",
        "summary": "这是一份全面的OCI IP地址管理系统指南，涵盖用于内部通信的私有IP、用于互联网访问的公共IP以及高级功能（如保留IP和自带IP选项），这些对于构建安全的云基础设施至关重要。"
      },
      "fr": {
        "title": "Comprendre la gestion des adresses IP dans l'infrastructure cloud Oracle (OCI)",
        "summary": "Un guide complet du système de gestion des adresses IP d'OCI couvrant les adresses IP privées pour la communication interne, les adresses IP publiques pour l'accès Internet et les fonctionnalités avancées comme les adresses IP réservées et les options bring-your-own-IP, essentielles pour construire une infrastructure cloud sécurisée."
      },
      "de": {
        "title": "Verständnis der IP-Verwaltung in der Oracle-Cloud-Infrastruktur (OCI)",
        "summary": "Ein umfassender Leitfaden zum IP-Adressenmanagementsystem von OCI, der private IP-Adressen für interne Kommunikation, öffentliche IP-Adressen für Internetzugriff und erweiterte Funktionen wie reservierte IP-Adressen und Bring-Your-Own-IP-Optionen abdeckt, die für den Aufbau einer sicheren Cloud-Infrastruktur unerlässlich sind."
      },
      "es": {
        "title": "Entender la gestión de direcciones IP en la infraestructura en la nube de Oracle (OCI)",
        "summary": "Una guía completa del sistema de gestión de direcciones IP de OCI que cubre direcciones IP privadas para comunicación interna, direcciones IP públicas para acceso a Internet y funciones avanzadas como direcciones IP reservadas y opciones bring-your-own-IP, esenciales para construir una infraestructura en la nube segura."
      }
    }
  },
  {
    "title": "Why You Shouldn't Let AI Do Your Fortune Telling — And How to Do It Right",
    "slug": "ai-fortune-telling-deterministic-algorithm-interpretation",
    "url": "https://dev.to/ji_ai/why-you-shouldnt-let-ai-do-your-fortune-telling-and-how-to-do-it-right-2h3l",
    "source": "DEV Community",
    "date": "2026-02-26T00:00:40.000Z",
    "summary": "This article demonstrates why LLMs cannot reliably perform precise astronomical calendar calculations for fortune-telling and advocates combining deterministic algorithms for computation with AI for human-readable interpretation of results.",
    "content": "The first lesson I learned building a saju (Korean four-pillar fortune telling) app: it's not about what you ask AI to do — it's about what you don't.\nRevenue is still $0. This isn't a success story — it's a debugging diary.\n\"Tell me the fortune for someone born March 15, 1990.\"\nI threw this straight at an LLM. The response looked great. Smooth sentences, Five Elements this, Wood-Fire-Earth-Metal-Water that.\nBut there was a problem. The base calculations were wrong.\nMe: \"Analyze the Four Pillars for March 15, 1990, 6 AM\"\nClaude: \"The year pillar is Geng-Wu, month is Ji-Mao...\"\nMe: \"...Ji-Mao is wrong.\"\n\nLLMs can't do manseryeok (traditional Korean astronomical calendar) calculations. More precisely, they appear to get it right probabilistically, but they don't actually compute anything.\nThe Heavenly Stems and Earthly Branches follow a 60-cycle system. Month pillars shift at solar term boundaries. Hour pillars depend on the day's Heavenly Stem. This isn't reasoning — it's arithmetic.\nWhen you ask a language model to do arithmetic, it gets things wrong.\nAnd when the base stems are wrong, everything downstream is garbage. Wrong Five Elements. Wrong Ten Gods. Wrong structure analysis.\nA beautifully written paragraph with incorrect data isn't fortune analysis — it's fiction.\nOnce I realized this, I rebuilt the whole thing.\n[Birth date + time] → [Calendar Engine] → [Accurate JSON] → [LLM] → [Interpretation]\n\nI built the calendar engine in code. It's based on the lunar-typescript library, with solar term correction, leap month handling, and midnight boundary logic — all deterministic algorithms.\nThe output is JSON. Heavenly Stems, Earthly Branches, Five Element distribution, Ten Gods relationships, structure type, favorable elements. All precise.\nThe LLM gets this JSON. \"Read this data and interpret it.\" That's the entire prompt strategy.\nCode handles the calendar calculations — the part that must be 100% accurate. AI handles turning that data into readable, insightful lan",
    "category": "github",
    "translations": {
      "zh": {
        "title": "为什么不应该让AI做占卜——以及如何正确地做",
        "summary": "本文演示了为什么大语言模型无法可靠地执行用于占卜的精确天文日历计算，并倡导将确定性算法用于计算，将AI用于人类可读的结果解释。"
      },
      "fr": {
        "title": "Pourquoi vous ne devriez pas laisser l'IA faire votre voyance — Et comment le faire correctement",
        "summary": "Cet article démontre pourquoi les LLM ne peuvent pas effectuer de manière fiable des calculs précis de calendrier astronomique pour la voyance et préconise de combiner des algorithmes déterministes pour le calcul avec l'IA pour l'interprétation lisible par l'homme des résultats."
      },
      "de": {
        "title": "Warum Sie KI nicht für Ihre Wahrsagung einsetzen sollten — Und wie man es richtig macht",
        "summary": "Dieser Artikel zeigt, warum LLMs keine zuverlässigen astronomischen Kalenderberechnungen für Wahrsagungen durchführen können, und befürwortet die Kombination deterministischer Algorithmen für Berechnungen mit KI für menschenlesbare Interpretationen der Ergebnisse."
      },
      "es": {
        "title": "Por qué no deberías dejar que la IA haga tu lectura del futuro — Y cómo hacerlo correctamente",
        "summary": "Este artículo demuestra por qué los LLM no pueden realizar de manera confiable cálculos precisos de calendarios astronómicos para la lectura del futuro e aboga por combinar algoritmos deterministas para cálculos con IA para la interpretación legible por humanos de los resultados."
      }
    }
  }
]