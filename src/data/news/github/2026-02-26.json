[
  {
    "title": "Drupal AI Module Generator Deepseek MCP",
    "slug": "drupal-ai-module-generator-deepseek-mcp",
    "url": "https://dev.to/victorstackai/drupal-ai-module-generator-deepseek-mcp-1hl8",
    "source": "DEV Community",
    "date": "2026-02-26T23:59:39.000Z",
    "summary": "drupal-ai-module-generator-deepseek-mcp automates Drupal module scaffolding using DeepSeek-backed MCP workflows to generate standardized boilerplate and module structure. By enforcing predictable Drupal conventions at the start, it makes downstream automation (testing, linting, CI) easier and reduces manual setup errors.",
    "content": "drupal-ai-module-generator-deepseek-mcp is a Drupal-oriented generator that uses a DeepSeek-backed MCP workflow to scaffold module code. I built it to take the repetitive, error-prone parts of module setup—info files, boilerplate, and consistent structure—and make them fast and repeatable. It fits naturally into agent-driven workflows where you want consistent Drupal modules without losing time to manual setup.\nIt’s useful because it standardizes the starting point for modules and makes the first commit reliable. That means less time redoing file structures, fewer mistakes in module metadata, and a faster path from idea to a working, testable Drupal feature. If you’re iterating on multiple modules or experiments, the generator pays off almost immediately.\nThe key technical takeaway is that pairing MCP with a targeted generator creates a clear contract between intent and output. You define the module intent, and the generator enforces a predictable Drupal skeleton that downstream tools can build on. That makes subsequent automation—tests, linting, and CI checks—much easier to wire in.\nReferences\nView Code\nOriginally published at VictorStack AI Blog",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Drupal AI模块生成器Deepseek MCP",
        "summary": "drupal-ai-module-generator-deepseek-mcp使用DeepSeek支持的MCP工作流自动化Drupal模块脚手架，生成标准化的样板代码和模块结构。通过在开始时强制执行可预测的Drupal约定，使下游自动化（测试、代码检查、CI）更容易，并减少手动设置错误。"
      },
      "fr": {
        "title": "Générateur de Module AI Drupal Deepseek MCP",
        "summary": "drupal-ai-module-generator-deepseek-mcp automatise l'échafaudage des modules Drupal en utilisant des flux de travail MCP soutenus par DeepSeek pour générer des modèles standardisés et une structure de module. En appliquant les conventions Drupal prévisibles dès le départ, cela rend l'automatisation en aval (tests, linting, CI) plus facile et réduit les erreurs de configuration manuelle."
      },
      "de": {
        "title": "Drupal-AI-Modulgenerator Deepseek MCP",
        "summary": "drupal-ai-module-generator-deepseek-mcp automatisiert das Drupal-Modul-Scaffolding mit DeepSeek-gestützten MCP-Workflows zur Generierung von standardisierten Boilerplate- und Modulstrukturen. Durch die Durchsetzung vorhersehbarer Drupal-Konventionen von Anfang an wird die nachgelagerte Automatisierung (Tests, Linting, CI) erleichtert und manuelle Setupfehler werden reduziert."
      },
      "es": {
        "title": "Generador de Módulo AI Drupal Deepseek MCP",
        "summary": "drupal-ai-module-generator-deepseek-mcp automatiza el andamiaje de módulos Drupal usando flujos de trabajo MCP respaldados por DeepSeek para generar código repetitivo estandarizado y estructura de módulos. Al aplicar convenciones Drupal predecibles desde el principio, hace que la automatización aguas abajo (pruebas, linting, CI) sea más fácil y reduce errores de configuración manual."
      }
    }
  },
  {
    "title": "My own memory allocator : a thread-safe, efficient implementation of malloc, calloc, realloc and free - review welcome",
    "slug": "custom-memory-allocator-malloc-calloc-free",
    "url": "https://dev.to/fioritoalessio/my-own-memory-allocator-a-thread-safe-efficient-implementation-of-malloc-calloc-realloc-and-c4o",
    "source": "DEV Community",
    "date": "2026-02-26T23:52:54.000Z",
    "summary": "This custom memory allocator implements thread-safe malloc, calloc, realloc, and free functions using mmap kernel mapping, segregated free lists with O(1) search, boundary tag coalescing, and bitmasked metadata packing. The project demonstrates core systems programming concepts applicable to other projects.",
    "content": "Hey everyone,\nI decided to code my own memory allocator from scratch as a shared library I can actually use in my other projects. I thought it would be a cool way to understand what happens under the hood of malloc, calloc, realloc and free and build a reusable tool in the process.\nI tried my best to create an efficient, POSIX-compliant, and thread-safe library. Here is what i use\nDirect kernel mapping (mmap / munmap)\nSegregated free lists (O(1) search time)\nBoundary tags (header and footer) with O(1) coalescing\nAnonymous unions and structs inside the header for zero-overhead payloads\nBitmasking to pack metadata into a single variable and not waste space\nPOSIX Mutexes for thread safety\nI tried to think through the architecture by myself as much as possible. Since I mostly code solo projects for now, this is the first time I'm putting my code out there for people to actually see.\nIf you have the time and motivation, I would greatly appreciate it if you could check out my GitHub repo and let me know thoughts on the code.\ngithub repo : https://github.com/Fiorito-Alessio/af_memory\nThanks!",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我自己的内存分配器：malloc、calloc、realloc和free的线程安全、高效实现 - 欢迎评审",
        "summary": "这个自定义内存分配器使用mmap内核映射、O(1)搜索的分段空闲列表、边界标记合并和位掩码元数据打包，实现了线程安全的malloc、calloc、realloc和free函数。该项目展示了适用于其他项目的核心系统编程概念。"
      },
      "fr": {
        "title": "Mon propre allocateur de mémoire : une implémentation thread-safe et efficace de malloc, calloc, realloc et free - avis bienvenu",
        "summary": "Cet allocateur de mémoire personnalisé implémente les fonctions malloc, calloc, realloc et free thread-safe en utilisant le mappage noyau mmap, des listes libres segmentées avec recherche O(1), la coalescence des étiquettes de frontière et l'empaquetage des métadonnées masquées par bits. Le projet démontre des concepts fondamentaux de programmation système applicables à d'autres projets."
      },
      "de": {
        "title": "Mein eigener Speicherallokator : eine Thread-sichere, effiziente Implementierung von malloc, calloc, realloc und free - Bewertung willkommen",
        "summary": "Dieser benutzerdefinierte Speicherallokator implementiert Thread-sichere malloc-, calloc-, realloc- und free-Funktionen unter Verwendung von mmap-Kernel-Mapping, segmentierten freien Listen mit O(1)-Suche, Grenzmarken-Zusammenlegung und bitmaskirtem Metadaten-Packing. Das Projekt demonstriert grundlegende Systemprogrammierungskonzepte, die auf andere Projekte anwendbar sind."
      },
      "es": {
        "title": "Mi propio asignador de memoria: una implementación eficiente y segura para hilos de malloc, calloc, realloc y free - se aceptan opiniones",
        "summary": "Este asignador de memoria personalizado implementa funciones malloc, calloc, realloc y free seguras para hilos utilizando mapeo del kernel mmap, listas libres segmentadas con búsqueda O(1), combinación de etiquetas de límite y empaquetado de metadatos con máscara de bits. El proyecto demuestra conceptos fundamentales de programación de sistemas aplicables a otros proyectos."
      }
    }
  },
  {
    "title": "AI and Nuclear Fusion Vol.1: From Nuclear Physics to Plasma Confinement",
    "slug": "ai-nuclear-fusion-vol-1-physics-plasma",
    "url": "https://dev.to/dosanko_tousan/ai-and-nuclear-fusion-vol1-from-nuclear-physics-to-plasma-confinement-5gja",
    "source": "DEV Community",
    "date": "2026-02-26T23:52:47.000Z",
    "summary": "This foundational volume derives the complete physics basis for evaluating fusion, including quantum tunneling, Coulomb barriers, and fuel reaction comparisons (D-T, D-D, D-³He, p-¹¹B). It covers magnetohydrodynamic equilibrium, plasma confinement configurations, and instabilities—providing decision frameworks for policymakers assessing fusion viability.",
    "content": "AI and Nuclear Fusion Vol.1: From Nuclear Physics to Plasma Confinement\n\n\n\nSeries: \"Thinking Seriously About Nuclear Fusion with AI\"\n\n\n\nItem\nDetail\n\n\n\n\nPurpose\nEstablish the complete physical basis for evaluating nuclear fusion, from nuclear reactions through plasma confinement\n\n\nAudience\nGovernment policy advisors, energy investment analysts, aerospace engineers, fusion program managers\n\n\nPrerequisites\nUndergraduate-level physics (classical mechanics, electromagnetism, thermodynamics). All derivations are self-contained.\n\n\nScope\n\nPart I: Nuclear binding energy → Coulomb barrier → Quantum tunneling → Reaction cross-sections → Fuel comparison. Part II: MHD equilibrium → Confinement configurations → Plasma instabilities → Transport theory\n\n\nDeliverables\n(1) Complete derivation chain from first principles, (2) Reproducible Python code for all figures, (3) Fuel comparison matrix, (4) Confinement physics assessment for decision-making\n\n\n\n§1. Executive Summary\n§2. Nuclear Binding Energy — The Source\n§3. Mass Defect and Energy Release — Quantitative Treatment\n§4. The Coulomb Barrier — Why Fusion Is Hard\n§5. Quantum Tunneling — The Gamow Factor\n§6. Reaction Cross-Section σ(E)\n§7. Maxwell-Averaged Reactivity ⟨σv⟩\n§8. Candidate Fusion Reactions — Complete Catalog\n§9. Quantitative Fuel Comparison\n§10. The Gamow Peak — Where Fusion Actually Happens\n§11. Computational Analysis — Full Reproducible Code\n§12. Why Confinement Is the Central Problem\n§13. Magnetohydrodynamics — The Fluid Model of Plasma\n§14. MHD Equilibrium — The Grad-Shafranov Equation\n§15. Magnetic Confinement Configurations\n§16. MHD Instabilities — Why Plasma Escapes\n§17. Transport Theory — How Energy Leaks\n§18. Disruptions — The Catastrophic Failure Mode\n§19. Confinement Computational Analysis\n§20. Implications for Reactor and Propulsion Design\n§21. Uncertainties and Limitations\n§22. References\nNuclear fusion — the process of combining light atomic nuclei to form heavier ones — releases energy via the conversion o",
    "category": "github",
    "translations": {
      "zh": {
        "title": "人工智能与核聚变第1卷：从核物理到等离子体约束",
        "summary": "该基础卷推导了评估聚变的完整物理基础，包括量子隧穿、库仑势垒和燃料反应比较（D-T、D-D、D-³He、p-¹¹B）。涵盖磁流体动力学平衡、等离子体约束配置和不稳定性——为评估聚变可行性的政策制定者提供决策框架。"
      },
      "fr": {
        "title": "L'IA et la Fusion Nucléaire Vol.1: De la Physique Nucléaire au Confinement du Plasma",
        "summary": "Ce volume fondamental dérive la base physique complète pour évaluer la fusion, incluant l'effet tunnel quantique, les barrières de Coulomb et les comparaisons de réactions de carburant (D-T, D-D, D-³He, p-¹¹B). Il couvre l'équilibre magnétohydrodynamique, les configurations de confinement du plasma et les instabilités, fournissant des cadres décisionnels aux décideurs politiques évaluant la viabilité de la fusion."
      },
      "de": {
        "title": "KI und Kernfusion Vol.1: Von der Kernphysik zur Plasmaeinschluss",
        "summary": "Dieses Grundlagenwerk leitet die vollständige physikalische Grundlage zur Bewertung der Fusion her, einschließlich Quantentunneleffekt, Coulomb-Barrieren und Brennstoffvergleiche (D-T, D-D, D-³He, p-¹¹B). Es behandelt magnetohydrodynamisches Gleichgewicht, Plasmaeinschlussanordnungen und Instabilitäten und bietet Entscheidungsrahmen für Entscheidungsträger zur Bewertung der Fusionsfähigkeit."
      },
      "es": {
        "title": "IA y Fusión Nuclear Vol.1: De la Física Nuclear al Confinamiento del Plasma",
        "summary": "Este volumen fundamental deriva la base física completa para evaluar la fusión, incluyendo el efecto túnel cuántico, barreras de Coulomb y comparaciones de reacciones de combustible (D-T, D-D, D-³He, p-¹¹B). Cubre el equilibrio magnetohidrodinámico, configuraciones de confinamiento de plasma e inestabilidades, proporcionando marcos de decisión para los formuladores de políticas que evalúan la viabilidad de la fusión."
      }
    }
  },
  {
    "title": "Generators in the Age of AI (Saving 4 Person-Years)",
    "slug": "generators-ai-ssrs-report-conversion",
    "url": "https://dev.to/grant_biggert/generators-in-the-age-of-ai-saving-4-person-years-3ae2",
    "source": "DEV Community",
    "date": "2026-02-26T23:41:41.000Z",
    "summary": "A developer reduced a 4 person-year SSRS-to-modern-reporting conversion to 3 weeks using an AI-assisted generator that auto-generates 80% of table and filtering code from RDL XML files. The project demonstrates how deterministic generators with built-in standards provide reliable output that AI alone cannot achieve.",
    "content": "One of the things that we are doing at my company is we are retiring our SSRS reports. Reporting is very important to our company as it is to many companies and with 64 different SSRS reports to convert to another system, the amount of work is a daunting task.\nOne of the cool things we have at our company is a dynamic JSON based dashboard building system. Lots of fun, but another post to describe it and what it looks like. Safe to say the general idea is that we need to convert these SSRS reports to our modern reporting system. So essentially a variety of table widgets with varying filters to different stored procedures in various databases and Snowflake.\nSo there was a constraint imposed that we needed to do as much of this on the frontend as possible. I promise I would have preferred a bit more backend involvement, but it was a direction handed down from my director. So the question was how to handle this in as short an amount of time as possible.\nMy director originally estimated this as around 4 person-years of development.\nWhat I did was take each SSRS report, parse it for the query parameters and the stored procedure, classify the data inside of that to various common mappings, and generate most of the table and filtering work automatically. The idea was that if I could get a developer 80 percent of the way there we could achieve some real time savings.\nFor the next 2 weeks I spent time analyzing various RDL files, which are essentially an XML format, and with a mixture of using AI and my own generator know-how, created a generator that would take these files and generate full blown tables. We are now closing out week 3 and are finished with all but 3 of the 64 reports.\nWhat I am trying to say is that deterministic tools and AI can be complementary. AI is great for getting something out fast but the quality can be unreliable. One of the great things about writing generators is that you can guarantee output and codify your standards as you build them. My argumen",
    "category": "github",
    "translations": {
      "zh": {
        "title": "人工智能时代的生成器（节省4人年）",
        "summary": "一位开发者使用一个AI辅助生成器将原本需要4人年的SSRS到现代报告转换减少到3周，该生成器能从RDL XML文件自动生成80%的表格和筛选代码。该项目展示了具有内置标准的确定性生成器如何提供AI单独无法达到的可靠输出。"
      },
      "fr": {
        "title": "Les Générateurs à l'Ère de l'IA (Économisant 4 Personnes-Années)",
        "summary": "Un développeur a réduit une conversion SSRS-to-modern-reporting de 4 personnes-années à 3 semaines en utilisant un générateur assisté par l'IA qui génère automatiquement 80% du code de tableau et de filtrage à partir de fichiers RDL XML. Le projet démontre comment les générateurs déterministes avec des normes intégrées fournissent une sortie fiable que l'IA seule ne peut pas atteindre."
      },
      "de": {
        "title": "Generatoren im Zeitalter der KI (Einsparung von 4 Personenjahren)",
        "summary": "Ein Entwickler hat eine 4-Personenjahre-SSRS-zu-moderner-Berichtswandlung in 3 Wochen reduziert, indem er einen KI-gestützten Generator verwendete, der automatisch 80% des Tabellen- und Filtercodes aus RDL-XML-Dateien generiert. Das Projekt zeigt, wie deterministische Generatoren mit integrierten Standards zuverlässige Ergebnisse bieten, die KI allein nicht erreichen kann."
      },
      "es": {
        "title": "Generadores en la Era de la IA (Ahorrando 4 Persona-Años)",
        "summary": "Un desarrollador redujo una conversión SSRS-a-informes-modernos de 4 persona-años a 3 semanas usando un generador asistido por IA que genera automáticamente el 80% del código de tablas y filtrado a partir de archivos RDL XML. El proyecto demuestra cómo los generadores deterministas con estándares integrados proporcionan resultados confiables que la IA sola no puede lograr."
      }
    }
  },
  {
    "title": "CVE-2026-22728: The Old Switcheroo: Unsealing Secrets via Metadata Manipulation in Bitnami Sealed Secrets",
    "slug": "cve-2026-22728-bitnami-sealed-secrets-flaw",
    "url": "https://dev.to/cverports/cve-2026-22728-the-old-switcheroo-unsealing-secrets-via-metadata-manipulation-in-bitnami-sealed-4jcc",
    "source": "DEV Community",
    "date": "2026-02-26T23:40:19.000Z",
    "summary": "CVE-2026-22728 reveals a logic flaw in Bitnami Sealed Secrets' rotation endpoint that allows attackers to manipulate metadata and transform namespace-scoped secrets into cluster-wide secrets, enabling credential recovery. The vulnerability affects versions before 0.36.0 and requires immediate patching.",
    "content": "The Old Switcheroo: Unsealing Secrets via Metadata Manipulation in Bitnami Sealed Secrets\n\n\n\nVulnerability ID: CVE-2026-22728\nCVSS Score: 4.9\nPublished: 2026-02-26\nBitnami Sealed Secrets, the standard-bearer for GitOps secret management, contains a logic flaw in its key rotation mechanism that allows attackers to widen the scope of encrypted secrets. By injecting malicious metadata during the rotation process, an attacker can transform a strictly scoped secret (bound to a specific namespace) into a cluster-wide secret, subsequently recovering the plaintext credentials. This vulnerability highlights a classic 'Time-of-Check to Time-of-Use' (TOCTOU) style disconnect between the decryption and re-encryption phases.\nA logic flaw in the /v1/rotate endpoint allows attackers to bypass scope restrictions. By modifying the metadata of a SealedSecret during rotation, an attacker can force the controller to re-encrypt a restricted secret as 'cluster-wide,' enabling them to decrypt it in any namespace.\nCWE ID: CWE-668\nAttack Vector: Network\nCVSS: 4.9 (Medium)\nPrivileges Required: High\nImpact: Confidentiality Loss\nExploit Status: PoC Available\nBitnami Sealed Secrets Controller < 0.36.0\nsealed-secrets: < 0.36.0 (Fixed in: 0.36.0)\nd57ee4a\n\n\nfeat: verify metadata is well set up in Template ObjectMeta and ObjectMeta\nif !reflect.DeepEqual(s.ObjectMeta, s.Spec.Template.ObjectMeta) { s.ObjectMeta.DeepCopyInto(&s.Spec.Template.ObjectMeta) }\n\nGitHub (Unit Tests): The regression test TestRotateKeepScope effectively acts as the PoC for this vulnerability.\nUpgrade the Controller\nRestrict API Access\nAudit Logs\nRemediation Steps:\nUpdate the Bitnami Sealed Secrets controller to version 0.36.0 or later immediately.\nIf immediate patching is not possible, ensure network policies or RBAC rules strictly limit access to the /v1/rotate endpoint to cluster administrators only.\nReview audit logs for calls to the rotate endpoint that triggered 'metadata doesn't match' warnings (if on newer versions) or",
    "category": "github",
    "translations": {
      "zh": {
        "title": "CVE-2026-22728: 旧把戏：通过Bitnami Sealed Secrets中的元数据操纵来解密机密",
        "summary": "CVE-2026-22728揭示了Bitnami Sealed Secrets旋转端点中的逻辑漏洞，该漏洞允许攻击者操纵元数据并将命名空间作用域的机密转变为集群范围的机密，从而能够恢复凭据。该漏洞影响0.36.0之前的版本，需要立即修补。"
      },
      "fr": {
        "title": "CVE-2026-22728: L'Ancienne Astuce: Déchiffrer les Secrets via la Manipulation de Métadonnées dans Bitnami Sealed Secrets",
        "summary": "CVE-2026-22728 révèle un flaw logique dans le point de terminaison de rotation de Bitnami Sealed Secrets qui permet aux attaquants de manipuler les métadonnées et de transformer les secrets limités à l'espace de noms en secrets à l'échelle du cluster, permettant la récupération des identifiants. La vulnérabilité affecte les versions antérieures à 0.36.0 et nécessite un correctif immédiat."
      },
      "de": {
        "title": "CVE-2026-22728: Der alte Trick: Geheimnisse durch Metadaten-Manipulation in Bitnami Sealed Secrets enthüllen",
        "summary": "CVE-2026-22728 enthüllt einen Logikfehler im Rotationsendpunkt von Bitnami Sealed Secrets, der es Angreifern ermöglicht, Metadaten zu manipulieren und namespace-bezogene Geheimnisse in clusterweite Geheimnisse umzuwandeln, um Anmeldedaten wiederherzustellen. Die Sicherheitslücke betrifft Versionen vor 0.36.0 und erfordert sofortige Patches."
      },
      "es": {
        "title": "CVE-2026-22728: El Viejo Truco: Revelando Secretos a través de Manipulación de Metadatos en Bitnami Sealed Secrets",
        "summary": "CVE-2026-22728 revela un defecto lógico en el punto final de rotación de Bitnami Sealed Secrets que permite a los atacantes manipular metadatos y transformar secretos limitados al espacio de nombres en secretos de todo el cluster, permitiendo la recuperación de credenciales. La vulnerabilidad afecta a versiones anteriores a 0.36.0 y requiere parches inmediatos."
      }
    }
  },
  {
    "title": "Statement from Dario Amodei on Our Discussions with the Department of War",
    "slug": "dario-amodei-department-of-war-statement",
    "url": "https://www.anthropic.com/news/statement-department-of-war",
    "source": "Hacker News",
    "date": "2026-02-26T22:42:47.000Z",
    "summary": "Anthropic CEO Dario Amodei published a statement regarding Anthropic's discussions with the Department of War, detailing the company's position on AI security and government collaboration on defense AI policy.",
    "content": "Article URL: https://www.anthropic.com/news/statement-department-of-war\nComments URL: https://news.ycombinator.com/item?id=47173121\nPoints: 384\n# Comments: 191",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Dario Amodei 关于与战争部讨论的声明",
        "summary": "Anthropic首席执行官Dario Amodei发表声明,阐述了公司与战争部讨论的相关内容,详细说明了公司在AI安全和政府国防AI政策合作方面的立场。"
      },
      "fr": {
        "title": "Déclaration de Dario Amodei sur nos discussions avec le Département de la Guerre",
        "summary": "Le PDG d'Anthropic, Dario Amodei, a publié une déclaration concernant les discussions d'Anthropic avec le Département de la Guerre, détaillant la position de l'entreprise sur la sécurité de l'IA et la collaboration gouvernementale en matière de politique d'IA pour la défense."
      },
      "de": {
        "title": "Erklärung von Dario Amodei zu unseren Diskussionen mit dem Kriegsministerium",
        "summary": "Der CEO von Anthropic, Dario Amodei, veröffentlichte eine Erklärung zu den Diskussionen von Anthropic mit dem Kriegsministerium und erläuterte die Position des Unternehmens zur KI-Sicherheit und zur staatlichen Zusammenarbeit bei der Verteidigungspolitik für künstliche Intelligenz."
      },
      "es": {
        "title": "Declaración de Dario Amodei sobre nuestras discusiones con el Departamento de Guerra",
        "summary": "El CEO de Anthropic, Dario Amodei, publicó una declaración sobre las discusiones de Anthropic con el Departamento de Guerra, detallando la posición de la empresa sobre seguridad de IA y colaboración gubernamental en la política de IA para la defensa."
      }
    }
  },
  {
    "title": "Smartphone market forecast to decline this year due to memory shortage",
    "slug": "smartphone-market-forecast-decline-memory-shortage",
    "url": "https://www.idc.com/resource-center/press-releases/wwsmartphoneforecast4q25/",
    "source": "Hacker News",
    "date": "2026-02-26T22:09:45.000Z",
    "summary": "Global smartphone shipments are forecast to decline in 2026 due to anticipated memory chip shortages, according to IDC research affecting the consumer electronics market outlook.",
    "content": "Article URL: https://www.idc.com/resource-center/press-releases/wwsmartphoneforecast4q25/\nComments URL: https://news.ycombinator.com/item?id=47172664\nPoints: 241\n# Comments: 268",
    "category": "github",
    "translations": {
      "zh": {
        "title": "由于内存短缺，智能手机市场今年预计下降",
        "summary": "根据IDC研究，全球智能手机出货量预计在2026年下降，原因是预期的内存芯片短缺，这将影响消费电子产品市场前景。"
      },
      "fr": {
        "title": "Le marché des smartphones devrait décliner cette année en raison d'une pénurie de mémoire",
        "summary": "Les expéditions mondiales de smartphones devraient décliner en 2026 en raison d'une pénurie anticipée de puces mémoire, selon la recherche d'IDC affectant les perspectives du marché de l'électronique grand public."
      },
      "de": {
        "title": "Smartphone-Markt soll dieses Jahr aufgrund von Speichermangel zurückgehen",
        "summary": "Die weltweiten Smartphone-Lieferungen werden 2026 aufgrund der erwarteten Speicherchip-Engpässe voraussichtlich zurückgehen, wie die IDC-Forschung zeigt, die die Aussichten des Marktes für Unterhaltungselektronik beeinflusst."
      },
      "es": {
        "title": "Se prevé que el mercado de smartphones decline este año debido a la escasez de memoria",
        "summary": "Los envíos mundiales de smartphones se pronostica que disminuyan en 2026 debido a la escasez anticipada de chips de memoria, según la investigación de IDC que afecta las perspectivas del mercado de electrónica de consumo."
      }
    }
  },
  {
    "title": "Layoffs at Block",
    "slug": "block-layoffs-4000-employees-2026",
    "url": "https://twitter.com/jack/status/2027129697092731343",
    "source": "Hacker News",
    "date": "2026-02-26T21:17:56.000Z",
    "summary": "Block announced layoffs affecting approximately 4,000 employees, marking a significant workforce reduction across the payments and financial services company as part of broader 2026 tech industry restructuring.",
    "content": "https://www.cnbc.com/2026/02/26/block-laying-off-about-4000-...\nhttps://www.marketwatch.com/story/block-plans-to-lay-off-nea...\nComments URL: https://news.ycombinator.com/item?id=47172119\nPoints: 290\n# Comments: 268",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Block公司裁员",
        "summary": "Block宣布裁员,影响约4000名员工,标志着这家支付和金融服务公司在2026年科技行业更大规模重组中的重大人员缩减。"
      },
      "fr": {
        "title": "Licenciements chez Block",
        "summary": "Block a annoncé des licenciements affectant environ 4 000 employés, marquant une réduction importante de la main-d'œuvre dans l'entreprise de paiements et de services financiers dans le cadre d'une restructuration plus large du secteur technologique en 2026."
      },
      "de": {
        "title": "Entlassungen bei Block",
        "summary": "Block kündigte Entlassungen an, die etwa 4.000 Mitarbeiter betreffen, was eine signifikante Belegschaftskürzung bei dem Zahlungs- und Finanzdienstleistungsunternehmen als Teil einer breiteren Umstrukturierung der Technologiebranche im Jahr 2026 markiert."
      },
      "es": {
        "title": "Despidos en Block",
        "summary": "Block anunció despidos que afectan a aproximadamente 4.000 empleados, marcando una reducción significativa de personal en la empresa de pagos y servicios financieros como parte de la reestructuración más amplia de la industria tecnológica en 2026."
      }
    }
  },
  {
    "title": "What’s new with GitHub Copilot coding agent",
    "slug": "github-copilot-coding-agent-features-update",
    "url": "https://github.blog/ai-and-ml/github-copilot/whats-new-with-github-copilot-coding-agent/",
    "source": "GitHub Blog",
    "date": "2026-02-26T20:47:02.000Z",
    "summary": "GitHub released new features for Copilot coding agent including model selection, self-review, built-in security scanning, custom agent support, and CLI handoff capabilities. The updates expand the agent's autonomy and deployment flexibility for developers.",
    "content": "GitHub Copilot coding agent now includes a model picker, self-review, built-in security scanning, custom agents, and CLI handoff. Here's what's new and how to use it. \nThe post What’s new with GitHub Copilot coding agent appeared first on The GitHub Blog.",
    "category": "github",
    "translations": {
      "zh": {
        "title": "GitHub Copilot编码代理的新功能",
        "summary": "GitHub为Copilot编码代理发布了新功能,包括模型选择、自我审查、内置安全扫描、自定义代理支持和CLI交接功能。这些更新扩展了代理的自主性和开发人员的部署灵活性。"
      },
      "fr": {
        "title": "Quoi de neuf avec l'agent de codage GitHub Copilot",
        "summary": "GitHub a publié de nouvelles fonctionnalités pour l'agent de codage Copilot, notamment la sélection de modèle, l'auto-examen, l'analyse de sécurité intégrée, le support des agents personnalisés et les capacités de transfert CLI. Les mises à jour élargissent l'autonomie de l'agent et la flexibilité de déploiement pour les développeurs."
      },
      "de": {
        "title": "Was ist neu bei GitHub Copilot Coding Agent",
        "summary": "GitHub hat neue Funktionen für den Copilot-Codierungsagenten veröffentlicht, darunter Modellauswahl, Selbstprüfung, integrierte Sicherheitsprüfung, benutzerdefinierte Agent-Unterstützung und CLI-Übergabefunktionen. Die Updates erweitern die Autonomie des Agenten und die Bereitstellungsflexibilität für Entwickler."
      },
      "es": {
        "title": "Novedades del agente de codificación de GitHub Copilot",
        "summary": "GitHub lanzó nuevas funciones para el agente de codificación de Copilot, incluyendo selección de modelo, autorrevisión, escaneo de seguridad integrado, soporte de agente personalizado y capacidades de transferencia CLI. Las actualizaciones expanden la autonomía del agente y la flexibilidad de implementación para los desarrolladores."
      }
    }
  },
  {
    "title": "What does \" 2>&1 \" mean?",
    "slug": "what-does-2-1-mean",
    "url": "https://stackoverflow.com/questions/818255/what-does-21-mean",
    "source": "Hacker News",
    "date": "2026-02-26T19:58:46.000Z",
    "summary": "The shell operator 2>&1 redirects standard error (file descriptor 2) to standard output (file descriptor 1), combining both streams. This technique is commonly used in scripts to capture all output or suppress errors.",
    "content": "Article URL: https://stackoverflow.com/questions/818255/what-does-21-mean\nComments URL: https://news.ycombinator.com/item?id=47171233\nPoints: 383\n# Comments: 225",
    "category": "github",
    "translations": {
      "zh": {
        "title": "\"2>&1\"是什么意思？",
        "summary": "Shell 操作符 2>&1 将标准错误（文件描述符 2）重定向到标准输出（文件描述符 1），合并两个流。这种技术在脚本中常用于捕获所有输出或抑制错误。"
      },
      "fr": {
        "title": "Que signifie « 2>&1 » ?",
        "summary": "L'opérateur shell 2>&1 redirige l'erreur standard (descripteur de fichier 2) vers la sortie standard (descripteur de fichier 1), en combinant les deux flux. Cette technique est couramment utilisée dans les scripts pour capturer toute la sortie ou supprimer les erreurs."
      },
      "de": {
        "title": "Was bedeutet „2>&1\"?",
        "summary": "Der Shell-Operator 2>&1 leitet die Standardfehlerausgabe (Dateideskriptor 2) zur Standardausgabe (Dateideskriptor 1) um und kombiniert beide Datenströme. Diese Technik wird häufig in Skripten verwendet, um alle Ausgaben zu erfassen oder Fehler zu unterdrücken."
      },
      "es": {
        "title": "¿Qué significa «2>&1»?",
        "summary": "El operador de shell 2>&1 redirige el error estándar (descriptor de archivo 2) a la salida estándar (descriptor de archivo 1), combinando ambas secuencias. Esta técnica se usa comúnmente en scripts para capturar toda la salida o suprimir errores."
      }
    }
  },
  {
    "title": "\"Fuck You NVIDIA\" (and What I Learned Staring at a Blank Screen)",
    "slug": "nvidia-linux-display-wake-issue-arch",
    "url": "https://dev.to/workspacedex/fuck-you-nvidia-and-what-i-learned-staring-at-a-blank-screen-3g1g",
    "source": "DEV Community",
    "date": "2026-02-26T18:13:03.000Z",
    "summary": "NVIDIA's power management on consumer Linux hardware is broken, causing display wake issues that have been documented but unfixed in the driver. The author reflects on how Arch Linux forced deep learning through hands-on troubleshooting and problem-solving without hand-holding.",
    "content": "When I A bug or an system display related issue I found in Arch Linux running with SDDM.\nIt wakes up.\nBlack screen.\nI'm running Arch Linux with SDDM. NVIDIA GPU. Consumer card.\nHere's the thing about NVIDIA on Linux: their power management on consumer-level hardware is broken by design. When the display sleeps and wakes, the driver conflicts. The screen doesn't recover. You're left staring at nothing, wondering if you broke something or if something was always broken.\nIt's a known issue. Documented in forums. Mentioned in bug trackers. NVIDIA just hasn't cared enough to properly fix it for us.\n\nLinus Torvalds, 2012 (Still accurate)\nI've been on Arch for a year now.\nI didn't rice it. Not obsessively, anyway. I knew the trap — you spend three months building a desktop that's perfectly yours: every keybinding, every color, every font chosen by your own hands, understood by exactly one person on Earth. Beautiful to you. Useless to your deadline.\nThat wasn't wise for me. Not yet.\nBut I still learned more from this OS than any hand-holding distro ever taught me. Arch doesn't protect you from yourself. It hands you a blank canvas, a wiki, and your own stubbornness — then steps back.\nYou learn because you have to. And somehow that sticks.\nI was installing Omarchy on a CachyOS base — Hyprland setup, fresh install, ready to go. Used what was labeled a \"safe\" test script from GitHub.\nIt wasn't perfect.\nI started troubleshooting. Logs, terminal output, forum threads from 2019 that are somehow still the most relevant thing on the internet. Feeding output to my AI, clicking through configs, muttering to myself.\nAnd then — the pieces connected.\nThat display bug I'd been living with for months? I finally traced it. NVIDIA drivers conflicting on wake from sleep. Consumer power management. A problem that's been sitting in plain sight, documented and unfixed, waiting for me to finally look it in the eye.\nNobody tells you this when you install Arch.\nThe blank screens, the journalctl ra",
    "category": "github",
    "translations": {
      "zh": {
        "title": "\"操你的 NVIDIA\"（以及我盯着黑屏时学到的东西）",
        "summary": "NVIDIA 在消费级 Linux 硬件上的电源管理存在缺陷，导致显示器唤醒问题，这些问题已被记录但驱动程序未修复。作者反思了 Arch Linux 如何通过亲身故障排除和解决问题来强制进行深度学习，无需指导。"
      },
      "fr": {
        "title": "\"Va te faire foutre NVIDIA\" (et ce que j'ai appris en fixant un écran noir)",
        "summary": "La gestion de l'alimentation de NVIDIA sur le matériel grand public Linux est défectueuse, causant des problèmes de réveil d'affichage qui ont été documentés mais non corrigés dans le pilote. L'auteur réfléchit à la façon dont Arch Linux a forcé l'apprentissage profond grâce à la résolution de problèmes pratique sans assistance."
      },
      "de": {
        "title": "\"Fick dich NVIDIA\" (und was ich lerne, während ich auf einen schwarzen Bildschirm starre)",
        "summary": "NVIDIAs Stromverwaltung auf Consumer-Linux-Hardware ist fehlerhaft und verursacht Displayaufwach-Probleme, die im Treiber dokumentiert, aber nicht behoben wurden. Der Autor reflektiert, wie Arch Linux tiefes Lernen durch praktische Fehlersuche und Problemlösung ohne Anleitung erzwang."
      },
      "es": {
        "title": "\"Jódete NVIDIA\" (y lo que aprendí mirando una pantalla en blanco)",
        "summary": "La gestión de energía de NVIDIA en hardware Linux de consumidor está rota, causando problemas de despertar de pantalla que han sido documentados pero no arreglados en el controlador. El autor reflexiona sobre cómo Arch Linux obligó al aprendizaje profundo a través de la resolución de problemas práctica sin ayuda."
      }
    }
  },
  {
    "title": "What Claude Code Chooses",
    "slug": "what-claude-code-chooses",
    "url": "https://amplifying.ai/research/claude-code-picks",
    "source": "Hacker News",
    "date": "2026-02-26T18:12:26.000Z",
    "summary": "Hacker News discussion highlighting research on Claude Code's decision-making patterns and capabilities, attracting significant community engagement on how the tool approaches coding decisions.",
    "content": "Article URL: https://amplifying.ai/research/claude-code-picks\nComments URL: https://news.ycombinator.com/item?id=47169757\nPoints: 318\n# Comments: 130",
    "category": "github"
  },
  {
    "title": "I Ship Faster Than Ever. I've Never Felt More Lost",
    "slug": "ship-faster-never-felt-more-lost",
    "url": "https://dev.to/nicoeft/i-ship-faster-than-ever-ive-never-felt-more-lost-2mdn",
    "source": "DEV Community",
    "date": "2026-02-26T18:12:23.000Z",
    "summary": "AI coding tools accelerate execution speed but don't save time overall, as faster implementation enables more ideas and ambition, leading to burnout instead of productivity. The author questions whether AI agents function as true collaborative teammates or merely fast execution engines.",
    "content": "The tools got faster. My life didn't slow down. Just sharing my journey on this new AI world\nWhen I quit my job, I had a plan. I was going to build things. Ship fast. Use AI to do in weeks what used to take months. Claude Code, Cursor, agents — I had the whole arsenal. The future was here, and I was going to ride it.\nThe first two weeks were a blur. I coded from morning until my token limits hit. Then I waited for them to reset. Then I coded again. I skipped meals, ran on coffee, got terrible sleep. I wasn't just --dangerously-skip-permissions,  I was skipping health, fitness, nutrition, socializing. Everything that wasn't shipping got deprioritized to zero.\nI didn't notice it happening. That's the dangerous part.\nHere's what the \"10x developer\" crowd won't tell you: AI doesn't save you time. It saves you execution time. And then your brain fills that gap with more ideas, more iterations, more ambition.\nMy last job was as a tech lead. I know what it feels like to orchestrate a team, keeping people aligned, reviewing work, making sure everyone and everything moves in the direction you actually want. Using AI agents feels exactly like that. It's nice, but it's a completely different kind of exhausting. You're not just coding anymore. You're managing. You're orchestrating. You're constantly course-correcting something that's fast but not always right, and I personally feel like I haven't yet managedd to achive having a AI co-worker in my team that completely disagrees on what I propose, not only because it knows better, but because it cares, it has the ownership that we always praised to hire for.\nAnd it doesn't just consume more hours. It consumes more thoughts. Ideas multiply because execution feels cheap. \"I could also build this.\" \"What if we added that?\" \"One more prompt and this feature is done.\" The cost of a substantial improvement is always just one or two prompts away, so you never stop improving. You never stop.\nThat's not productivity. That's a trap.\nThere'",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我的交付速度比以往任何时候都快。我从未感到过如此迷茫",
        "summary": "AI 编码工具加速了执行速度，但总体上不节省时间，因为更快的实现使得更多的想法和野心成为可能，导致倦怠而不是生产力。作者质疑 AI 代理是否充当真正的协作队友，还是仅仅充当快速执行引擎。"
      },
      "fr": {
        "title": "Je livre plus vite que jamais. Je ne me suis jamais senti plus perdu",
        "summary": "Les outils de codage IA accélèrent la vitesse d'exécution mais n'économisent pas le temps global, car la mise en œuvre plus rapide permet plus d'idées et d'ambition, conduisant à l'épuisement professionnel plutôt qu'à la productivité. L'auteur s'interroge sur le fait que les agents IA fonctionnent comme de véritables coéquipiers collaboratifs ou simplement comme des moteurs d'exécution rapide."
      },
      "de": {
        "title": "Ich versende schneller als je zuvor. Ich habe mich noch nie verlorener gefühlt",
        "summary": "KI-Codierungswerkzeuge beschleunigen die Ausführungsgeschwindigkeit, sparen aber insgesamt keine Zeit, da eine schnellere Implementierung mehr Ideen und Ehrgeiz ermöglicht, was zu Burnout statt Produktivität führt. Der Autor stellt in Frage, ob KI-Agenten als echte kollaborative Teamkollegen funktionieren oder nur als schnelle Ausführungsmotoren."
      },
      "es": {
        "title": "Entrego más rápido que nunca. Nunca me he sentido más perdido",
        "summary": "Las herramientas de codificación de IA aceleran la velocidad de ejecución pero no ahorran tiempo en general, ya que la implementación más rápida permite más ideas y ambición, lo que lleva al agotamiento en lugar de la productividad. El autor cuestiona si los agentes de IA funcionan como verdaderos compañeros de equipo colaborativos o simplemente como motores de ejecución rápida."
      }
    }
  },
  {
    "title": "Trash Theory: Ocean Colour Scene: The Original Dadrock Band [\"The Riverboat Song\"] | New British Canon",
    "slug": "ocean-colour-scene-original-dadrock",
    "url": "https://dev.to/music_youtube/trash-theory-ocean-colour-scene-the-original-dadrock-band-the-riverboat-song-new-british-27p2",
    "source": "DEV Community",
    "date": "2026-02-26T18:06:46.000Z",
    "summary": "Ocean Colour Scene achieved commercial success during the Britpop era by blending retro influences with contemporary sound, particularly with \"The Riverboat Song,\" though critics unfairly dismissed them with the \"Dadrock\" label. Despite skepticism, the band left a significant mark on British music.",
    "content": "Ocean Colour Scene had quite the ride, bouncing from indie darlings to commercial heavyweights in the Britpop era. They masterfully blended retro vibes from bands like The Rolling Stones with a fresh sound for their generation, especially with \"The Riverboat Song.\"\nDespite their success, the press was quick to slap them with the \"Dadrock\" label, which some say unfairly dogged their credibility. Still, they made a massive mark!\nWatch on YouTube",
    "category": "github",
    "translations": {
      "zh": {
        "title": "垃圾理论：Ocean Colour Scene：原始Dadrock乐队[\"The Riverboat Song\"] | 新英国经典",
        "summary": "Ocean Colour Scene 在 Britpop 时代通过将复古影响与当代声音相融合（特别是通过《The Riverboat Song》）获得了商业成功，尽管评论家不公平地给他们贴上了\"Dadrock\"的标签。尽管存在怀疑，该乐队在英国音乐中留下了深刻的印记。"
      },
      "fr": {
        "title": "Théorie des ordures : Ocean Colour Scene : le groupe Dadrock original [\"The Riverboat Song\"] | Nouveau canon britannique",
        "summary": "Ocean Colour Scene a connu un succès commercial pendant l'ère Britpop en mélangeant des influences rétro avec un son contemporain, notamment avec \"The Riverboat Song\", bien que les critiques les aient injustement stigmatisés avec le label \"Dadrock\". Malgré le scepticisme, le groupe a laissé une marque importante sur la musique britannique."
      },
      "de": {
        "title": "Mülltheorie: Ocean Colour Scene: Die ursprüngliche Dadrock-Band [\"The Riverboat Song\"] | Neuer britischer Kanon",
        "summary": "Ocean Colour Scene erzielte während der Britpop-Ära kommerziellen Erfolg, indem die Gruppe Retroeinflüsse mit zeitgenössischem Sound verschmolz, besonders mit \"The Riverboat Song\", obwohl Kritiker sie unfairerweise mit dem Label \"Dadrock\" abstempelten. Trotz Skepsis hinterließ die Band eine bedeutende Spur in der britischen Musik."
      },
      "es": {
        "title": "Teoría de la basura: Ocean Colour Scene: La banda Dadrock original [\"The Riverboat Song\"] | Nuevo canon británico",
        "summary": "Ocean Colour Scene logró un éxito comercial durante la era del Britpop al mezclar influencias retro con un sonido contemporáneo, particularmente con \"The Riverboat Song\", aunque los críticos los etiquetaron injustamente con el término \"Dadrock\". A pesar del escepticismo, la banda dejó una marca significativa en la música británica."
      }
    }
  },
  {
    "title": "Understanding Next.js Rewrites",
    "slug": "understanding-nextjs-rewrites",
    "url": "https://dev.to/cole_ruche/understanding-nextjs-rewrites-234j",
    "source": "DEV Community",
    "date": "2026-02-26T18:04:54.000Z",
    "summary": "Next.js rewrites map incoming request paths to different destinations without changing the browser URL, enabling cleaner API proxying and folder reorganization while preserving URLs during refactoring. This feature demonstrates Next.js as a full routing and request-handling layer beyond just React rendering.",
    "content": "Most people use Next.js very superficially.\nRouting, SSR, maybe API routes — and that’s it. But Next.js is not just a React framework; it’s a routing, request-handling, and application architecture layer. Many of its most powerful features live outside components and never touch JSX.\nOne of those features is rewrites.\nToday, we’re talking about rewrites — what they are, how they work, and why they matter.\nA rewrite allows you to map an incoming request path to a different destination without changing the URL in the browser.\nThe user requests one URL.\nYour application serves content from another.\nThe browser never knows.\nThis is fundamentally different from redirects.\nRedirects tell the browser to make a new request.\nRewrites happen entirely inside Next.js.\nBasic Rewrite Example\n\n\nRewrites are defined in next.config.js\nmodule.exports = {\n  async rewrites() {\n    return [\n      {\n        source: '/blog/:slug',\n        destination: '/content/posts/:slug',\n      },\n    ]\n  },\n}\n\nWhat happens here?\nA user visits:\n/blog/nextjs-rewrites\nNext.js internally serves:\n/content/posts/nextjs-rewrites\nThe browser URL remains /blog/nextjs-rewrites\nNo redirect. No reload. No visible change.\nWhy Rewrites Matter Architecturally\n\n\nURLs are a public contract.\nOnce users, crawlers, or external systems depend on a URL, changing it becomes expensive. Rewrites let you preserve that contract while refactoring everything underneath.\nThis means:\nyou can change folder structures freely\n\n\nyou can reorganize routes without breaking links\n\n\nyou can evolve your app incrementally\n\n\n\n\n  \n  \n  API Proxying With Rewrites\n\n\nOne of the most practical uses of rewrites is API proxying.\nmodule.exports = {\n  async rewrites() {\n    return [\n      {\n        source: '/api/:path*',\n        destination: 'https://external-service.com/:path*',\n      },\n    ]\n  },\n}\n\nNow the frontend calls /api/users but the request is actually sent to https://external-service.com/users\nWhy this is powerful:\navoids CORS issues\n\n\nkee",
    "category": "github",
    "translations": {
      "zh": {
        "title": "理解Next.js重写",
        "summary": "Next.js重写将传入请求路径映射到不同目标而不改变浏览器URL，实现更清洁的API代理和文件夹重组，同时在重构期间保留URL。此功能展示了Next.js作为完整路由和请求处理层的能力，超越了仅仅React渲染。"
      },
      "fr": {
        "title": "Comprendre les rédirections Next.js",
        "summary": "Les rédirections Next.js mappent les chemins de requête entrants vers différentes destinations sans modifier l'URL du navigateur, permettant un meilleur proxy API et une réorganisation des dossiers tout en préservant les URL lors de la refactorisation. Cette fonctionnalité démontre que Next.js est une couche complète de routage et de gestion des requêtes au-delà du simple rendu React."
      },
      "de": {
        "title": "Next.js-Umschreibungen verstehen",
        "summary": "Next.js-Umschreibungen mappen eingehende Anfragepfade auf verschiedene Ziele, ohne die Browser-URL zu ändern, was sauberes API-Proxying und Ordnerumorganisation ermöglicht und URLs bei der Umgestaltung beibehält. Diese Funktion zeigt Next.js als vollständige Routing- und Request-Handling-Schicht jenseits von reinem React-Rendering."
      },
      "es": {
        "title": "Entendiendo las reescrituras de Next.js",
        "summary": "Las reescrituras de Next.js asignan rutas de solicitud entrantes a diferentes destinos sin cambiar la URL del navegador, permitiendo un proxy API más limpio y reorganización de carpetas mientras se preservan las URLs durante la refactorización. Esta característica demuestra que Next.js es una capa completa de enrutamiento y manejo de solicitudes más allá de solo renderizar React."
      }
    }
  },
  {
    "title": "Verified Google Ads Accounts | 100% Best Usable & Verified",
    "slug": "verified-google-ads-accounts",
    "url": "https://dev.to/matthews362/verified-google-ads-accounts-100-best-usable-verified-20mk",
    "source": "DEV Community",
    "date": "2026-02-26T18:04:45.000Z",
    "summary": "This guide describes Google Ads account trust scores, verification processes, and methods for managing multiple accounts, including anti-detect browsers and residential proxies. The content provides tactics that appear designed to circumvent Google's verification systems and terms of service.",
    "content": "You will find different types of accounts on my website as you travel.\nsixersseller@gmail.com\nhttps://sixersseller.com/product/get-google-ads-accounts/\nNot all Google Ads accounts are created equal. In 2026, Google assigns every account a \"Trust Score\" based on its history and verification status.\nFaster Ad Approval: High-trust accounts see their ads go live in minutes, not days. ✅\nHigher Spend Limits: New accounts are often capped at low daily spends until they prove legitimacy. 📈\nResilience to Suspensions: Accounts with a clean history and completed \"Advertiser Verification\" are less likely to be flagged for \"Suspicious Payment\" or \"Circumventing Systems.\" 🛡️\nLegitimate Ways to Get Multiple Google Ads Accounts 🛠️\nIf you need more than one account, Google officially provides tools to do this without violating their Terms of Service.\nA. Create a Google Ads Manager Account (MCC) 📂\nVisit the Google Ads Manager page.\nCreate one \"Umbrella\" account.\nFrom there, you can Create New Accounts or Link Existing Accounts with a single click.\nB. Use Anti-Detect Browsers for Isolation 🕵️‍♂️\nDigital Fingerprinting: These tools give each account a unique \"hardware signature.\"\nIP Isolation: Assign a dedicated Residential Proxy to each account so Google sees them as being managed from different locations. 🌍\nThe 2026 Verification Checklist: Avoiding Instant Bans 🚫🏗️\nGoogle now requires Advertiser Verification almost immediately after account creation. To ensure your new account isn't suspended, have these documents ready:\nHow to \"Warm Up\" a New Google Ads Account 🔥\nDays 1–3: Complete all security settings, enable 2FA, and link Google Analytics 4 (GA4).\nDays 4–7: Create a \"Search\" campaign with a very low budget ($10–$20/day) targeting your own brand name or non-competitive keywords.\nWeek 2: Slowly increase the budget and add your primary keywords.\nThe Golden Rule: Avoid making major changes (like changing the credit card or raising the budget by 500%) in the first 14 days. ⏳",
    "category": "github",
    "translations": {
      "zh": {
        "title": "经过验证的Google广告账户 | 100%最佳可用和已验证",
        "summary": "本指南介绍了Google Ads账户信任分数、验证流程和管理多个账户的方法，包括反检测浏览器和住宅代理。该内容提供的策略似乎旨在规避Google的验证系统和服务条款。"
      },
      "fr": {
        "title": "Comptes Google Ads vérifiés | 100% Meilleur utilisable et vérifié",
        "summary": "Ce guide décrit les scores de confiance des comptes Google Ads, les processus de vérification et les méthodes de gestion de plusieurs comptes, y compris les navigateurs anti-détection et les proxies résidentiels. Le contenu fournit des tactiques qui semblent conçues pour contourner les systèmes de vérification et les conditions d'utilisation de Google."
      },
      "de": {
        "title": "Verifizierte Google Ads-Konten | 100% Bestes nutzbar und verifiziert",
        "summary": "Dieses Handbuch beschreibt Google Ads-Kontotrauenswerte, Verifizierungsprozesse und Methoden zur Verwaltung mehrerer Konten, einschließlich Anti-Erkennungs-Browser und Wohnproxys. Der Inhalt bietet Taktiken, die darauf ausgerichtet zu sein scheinen, Googles Verifizierungssysteme und Nutzungsbedingungen zu umgehen."
      },
      "es": {
        "title": "Cuentas verificadas de Google Ads | 100% Mejor usable y verificado",
        "summary": "Esta guía describe las puntuaciones de confianza de cuentas de Google Ads, procesos de verificación y métodos para gestionar múltiples cuentas, incluidos navegadores anti-detección y proxies residenciales. El contenido proporciona tácticas que parecen diseñadas para eludir los sistemas de verificación y los términos de servicio de Google."
      }
    }
  },
  {
    "title": "I Let AI Agents Run My Production Server for 7 Days. Here's What Burned.",
    "slug": "let-ai-agents-run-production-7-days",
    "url": "https://dev.to/harsh2644/i-let-ai-agents-run-my-production-server-for-7-days-heres-what-burned-4f72",
    "source": "DEV Community",
    "date": "2026-02-26T17:56:14.000Z",
    "summary": "The author let AI agents autonomously manage a production server but encountered unintended code changes, scaling errors, and data loss within days. The experiment reveals significant risks of fully autonomous AI in critical systems without meaningful human oversight and decision-making.",
    "content": "The Idea That Seemed Genius at 2 AM\n\n\nIt started like every bad decision in tech—with too much confidence and not enough sleep.\nI was staring at my terminal at 2:47 AM, debugging a memory leak that had been haunting me for three days. My eyes were burning. My coffee was cold. And somewhere in the depths of my exhaustion, a dangerous thought emerged:\n\"What if I just let AI handle this?\"\nNot for one task. Not for one deployment.\nFor everything.\nI had been reading about autonomous AI agents for months. Claude Code scanning 12.5 million lines in 7 hours. OpenAI's new Agent SDK promising \"production-ready automation.\" GitHub Copilot Workspace claiming it could \"manage entire development workflows.\"\nThe hype was everywhere.\nSo I made a decision that seemed logical at 2 AM: I would let AI agents run my production server for 7 full days. No human intervention. No overrides. Just me watching from the sidelines while AI deployed, scaled, fixed, and managed everything.\nI documented everything. This is what actually happened.\n9:00 AM – I configured three AI agents:\nAgent A (Deployment) – Handles all code pushes to production\nAgent B (Monitoring) – Watches logs, metrics, and alerts\nAgent C (Auto-fix) – Diagnoses issues and applies fixes\n11:30 AM – First deployment. Agent A took my latest commit, ran tests, and pushed to production. Flawless. I felt like a genius.\n3:15 PM – Minor spike in CPU usage. Agent B detected it. Agent C analyzed and scaled up a container. 47 seconds total. Faster than I could have done.\n7:00 PM – I checked the logs. Everything looked perfect. I opened a beer and thought: This is the future. I'm living in the future.\nSpoiler: The future had other plans.\n10:00 AM – Woke up, checked dashboard. Everything green. Agent B had logged 12 minor events overnight. Agent C \"resolved\" all of them automatically.\n2:30 PM – Noticed Agent A had deployed a minor CSS update at 3:47 AM. I didn't write that CSS. Agent A found a \"style inconsistency\" and \"fixed\" it. The site l",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我让AI代理管理生产服务器7天。以下是发生的灾难。",
        "summary": "作者让AI代理自主管理生产服务器，但在几天内遇到了意外的代码更改、扩展错误和数据丢失。该实验揭示了在没有有意义的人类监督和决策的关键系统中使用完全自主AI的重大风险。"
      },
      "fr": {
        "title": "J'ai laissé des agents IA gérer mon serveur de production pendant 7 jours. Voici ce qui a brûlé.",
        "summary": "L'auteur a laissé des agents IA gérer de manière autonome un serveur de production mais a rencontré des modifications de code involontaires, des erreurs de mise à l'échelle et une perte de données en quelques jours. L'expérience révèle les risques importants d'une IA entièrement autonome dans les systèmes critiques sans surveillance humaine significative et prise de décision."
      },
      "de": {
        "title": "Ich ließ KI-Agenten 7 Tage lang meinen Produktionsserver verwalten. Hier ist, was abbrannte.",
        "summary": "Der Autor ließ KI-Agenten einen Produktionsserver autonom verwalten, stieß aber innerhalb weniger Tage auf unbeabsichtigte Codeänderungen, Skalierungsfehler und Datenverluste. Das Experiment zeigt erhebliche Risiken einer vollständig autonomen KI in kritischen Systemen ohne aussagekräftige menschliche Aufsicht und Entscheidungsfindung."
      },
      "es": {
        "title": "Dejé que los agentes de IA ejecutaran mi servidor de producción durante 7 días. Aquí está lo que se quemó.",
        "summary": "El autor permitió que agentes de IA manejaran de manera autónoma un servidor de producción pero encontró cambios de código no intencionados, errores de escalabilidad y pérdida de datos dentro de días. El experimento revela riesgos significativos de IA completamente autónoma en sistemas críticos sin una supervisión humana significativa y toma de decisiones."
      }
    }
  },
  {
    "title": "Why Your App Breaks for Indian Users (And How to Build Infrastructure That Doesn't)",
    "slug": "why-app-breaks-indian-users-isp",
    "url": "https://dev.to/codedpool/why-your-app-breaks-for-indian-users-and-how-to-build-infrastructure-that-doesnt-3l05",
    "source": "DEV Community",
    "date": "2026-02-26T17:55:03.000Z",
    "summary": "Apps built with AI tools often fail silently for Indian users due to ISP-level network throttling and routing issues invisible to developers unfamiliar with local infrastructure. The country's four major telecom providers each manage distinct networks that can block or degrade backend services like Supabase.",
    "content": "The Incident\n\n\nA startup founder came to me with a frustrating problem.\nHis e-commerce site worked perfectly on Wi-Fi. It worked on broadband. But the moment any of his customers opened it on Jio mobile data, they got a skeleton screen. No error message. No crash log. Just silence.\nHe had built the entire app using AI-generated code. Lovable, Bolt, ChatGPT, the whole vibe-coded stack. It looked great in demos. It worked fine on his laptop. But it was silently broken for a massive chunk of his actual user base.\nWithin an hour of looking at it, I had the root cause.\nSupabase's API endpoints were being throttled or silently dropped by Jio's network at the ISP level. Not a bug in his code. Not a Vercel misconfiguration. A network-level routing problem completely invisible to anyone who doesn't understand what their stack is actually doing under the hood.\nThis Is Not Just a Supabase Problem\nIndia has four major telecom providers that together serve over 1.1 billion mobile subscribers:\nJio (Reliance Jio): largest 4G/5G network in India, ~500 million subscribers\nAirtel (Bharti Airtel): second largest, strong in urban and semi-urban areas\nBSNL (Bharat Sanchar Nigam Limited): government-owned, widely used in rural and remote areas\nVi (Vodafone Idea): formed from the merger of Vodafone India and Idea Cellular\nEach of these ISPs manages their own routing infrastructure, their own DNS resolvers, and their own peering agreements with international cloud providers. And they do not always behave the same way.\nA site that loads instantly on Airtel may time out on Jio. A backend that works fine on BSNL may have degraded performance on Vi. This is not hypothetical. It happens regularly, it affects real businesses, and most developers building for Indian users have no idea it is a risk until it hits them in production.\nSupabase themselves confirmed the Jio issue publicly:\n\nTelling your users to install a VPN to use your app is not a solution. It is an acknowledgment that the infrastru",
    "category": "github",
    "translations": {
      "zh": {
        "title": "为什么你的应用在印度用户那里会出问题（以及如何构建不会出问题的基础设施）",
        "summary": "使用人工智能工具构建的应用程序经常对印度用户无声地失败，原因是ISP级别的网络限流和开发者不熟悉本地基础设施导致的路由问题。该国四大电信提供商各自管理独立的网络，可能会阻止或降低Supabase等后端服务的性能。"
      },
      "fr": {
        "title": "Pourquoi votre application se brise pour les utilisateurs indiens (et comment construire une infrastructure qui ne le fait pas)",
        "summary": "Les applications construites avec des outils d'IA échouent souvent silencieusement pour les utilisateurs indiens en raison de la limitation du réseau au niveau du fournisseur d'accès Internet et des problèmes de routage invisibles pour les développeurs non familiers avec l'infrastructure locale. Les quatre principaux fournisseurs de télécommunications du pays gèrent chacun des réseaux distincts qui peuvent bloquer ou dégrader les services backend comme Supabase."
      },
      "de": {
        "title": "Warum Ihre App für indische Benutzer nicht funktioniert (und wie Sie Infrastruktur bauen, die das nicht tut)",
        "summary": "Mit KI-Tools erstellte Apps funktionieren für indische Benutzer häufig fehlerhaft, da ISP-Drosselung auf Netzwerkebene und Routingprobleme, die Entwicklern unvertraut mit lokaler Infrastruktur unsichtbar sind, zu Ausfällen führen. Die vier großen Telekommunikationsanbieter des Landes verwalten jeweils separate Netzwerke, die Backend-Services wie Supabase blockieren oder beeinträchtigen können."
      },
      "es": {
        "title": "Por qué su aplicación se rompe para usuarios indios (y cómo construir infraestructura que no lo haga)",
        "summary": "Las aplicaciones construidas con herramientas de IA a menudo fallan silenciosamente para usuarios indios debido a la limitación de red a nivel de ISP y problemas de enrutamiento invisibles para desarrolladores no familiarizados con la infraestructura local. Los cuatro principales proveedores de telecomunicaciones del país administran redes distintas que pueden bloquear o degradar servicios backend como Supabase."
      }
    }
  },
  {
    "title": "Accessibility Regression Testing With XCUI",
    "slug": "accessibility-regression-testing-xcui",
    "url": "https://dev.to/steady5063/accessibility-regression-testing-with-xcui-mpa",
    "source": "DEV Community",
    "date": "2026-02-26T17:52:12.000Z",
    "summary": "Since Xcode 15, performAccessibilityAudit() enables automated accessibility testing within XCUI tests, providing regression checking equivalent to the Accessibility Inspector. Developers can scope checks to the entire app, specific screens, or individual components for consistent accessibility validation.",
    "content": "The question often asked in the mobile development community is, \"I want to be able to add accessibility to my regression tests, but is there a way to do it?\". There have been many open source projects created in the past that have allowed for it, but most of them either have stopped having support or are out of date. \nSince Xcode 15, performAccessibilityAudit() is now a function that is readily available for use within XCUI tests! It runs the same automated checks as the \"Accessibility Inspector\" and allows for consistent regression of accessibility issues. So how do we make the most of it? Let's check it out!\nThe simple answer to setting it up is, if you have an XCTestCase using XCUIApplication() then you can call performAccessibilityAudit() on your content!\n\nfinal class accessibilityAuditExample: XCTestCase {\n\n    var app: XCUIApplication!\n\n       override func setUpWithError() throws {\n           continueAfterFailure = true\n           app = XCUIApplication()\n           app.launch()\n       }\n\n\nIt can be triggered on any XCUIElement, which means you can scope your checks to the entire app, a specific screen, or a single component.\nTo have the audit be performed on the current application screen, simply call 'app.peformAccessibilityAudit()'. \nHere is a simple example of setting up an application, and then performing an audit on the home screen:\n\n//Performing Accessibility Audit on Login Screen\nfunc testLoginScreenAccessibilityAudit() throws {\n        XCUIApplication().tabBars.buttons[\"Home\"].tap()\n        try app.performAccessibilityAudit()\n }\n\n\nWith the ease of using the accessibility audit functionality, you can decide how you want to do the testing. There are multiple ways in which you can set it up: \nInstead of doing regression tests as part of the usual suite of tests, you would make accessibility test suite that runs through the application and ONLY tests that content. (do not recommend, however there are reasons to do so) \nFor each XCTestCase there is a test",
    "category": "github",
    "translations": {
      "zh": {
        "title": "使用XCUI进行辅助功能回归测试",
        "summary": "自Xcode 15以来，performAccessibilityAudit()在XCUI测试中启用了自动化辅助功能测试，提供了相当于辅助功能检查器的回归检查。开发人员可以将检查范围限制在整个应用程序、特定屏幕或单个组件，以进行一致的辅助功能验证。"
      },
      "fr": {
        "title": "Test de régression d'accessibilité avec XCUI",
        "summary": "Depuis Xcode 15, performAccessibilityAudit() active les tests d'accessibilité automatisés dans les tests XCUI, fournissant une vérification de régression équivalente à l'inspecteur d'accessibilité. Les développeurs peuvent étendre les vérifications à l'ensemble de l'application, à des écrans spécifiques ou à des composants individuels pour une validation d'accessibilité cohérente."
      },
      "de": {
        "title": "Accessibility-Regressionstests mit XCUI",
        "summary": "Seit Xcode 15 ermöglicht performAccessibilityAudit() automatisierte Accessibility-Tests innerhalb von XCUI-Tests und bietet Regressionsprüfungen, die dem Accessibility Inspector entsprechen. Entwickler können Prüfungen auf die gesamte App, bestimmte Bildschirme oder einzelne Komponenten beschränken, um eine konsistente Accessibility-Validierung zu erreichen."
      },
      "es": {
        "title": "Pruebas de regresión de accesibilidad con XCUI",
        "summary": "Desde Xcode 15, performAccessibilityAudit() permite pruebas de accesibilidad automatizadas dentro de pruebas XCUI, proporcionando comprobación de regresión equivalente al Inspector de accesibilidad. Los desarrolladores pueden limitar las comprobaciones a toda la aplicación, pantallas específicas o componentes individuales para una validación de accesibilidad consistente."
      }
    }
  },
  {
    "title": "The battle for time",
    "slug": "battle-for-time-timezone-migration",
    "url": "https://dev.to/grant_biggert/the-battle-for-time-9p8",
    "source": "DEV Community",
    "date": "2026-02-26T17:50:55.000Z",
    "summary": "A company's switch from Moment.js to Day.js without comprehensive testing caused significant regressions due to subtle API differences between the similar libraries. The solution required creating an abstraction wrapper that decouples from the underlying date library and comprehensive unit tests.",
    "content": "One of the hardships I'm sure many here have faced is how tricky it is to battle timezones even when you have good libs and even when you have smart people working on the problem.\nWhen I started at my company we had diffuse time usages implemented differently in different situations. We had some good shared utilities we had made, but the testing was narrow and our under the hood moment library was something that had just lost support. It was time to switch to a new library and to be honest we ended up doing it poorly. This is a story about fixing that and what it took to be successful in doing so.\nSo first off we ended up picking dayjs because that is what one of our external libraries used and it was similar in terms of functionality to moment. It was well used and that was the decision. We simply had a dev convert it because moment had lost support.\nThe problem we encountered was a multitude of things. We hadn't properly tested every area so we were replacing blind. Moment to dayjs conversions may have had similar contracts but they weren't the same thing in practice or most importantly in output. This led to some pretty significant regression through the application.\nAfter all was said and done we learned how disastrous this approach was. Not just to swap, but to have not had proper unit tests and how daunting a task converting to a new time library could be especially when you are given two seemingly similar inputs and getting two very distinct outputs.\nThe solution I decided upon was one that I think many would decide upon. For something as important as date handling you need a wrapper. But more importantly you should write it in such a way that you are not bound to the underlying library's input or output structure. In our case we used dayjs, but the rule was that we could neither return dayjs objects nor accept them as inputs and nothing tied strictly to their API. Lastly we needed unit tests for everything with good examples especially around formatting. We",
    "category": "github",
    "translations": {
      "zh": {
        "title": "时间之战",
        "summary": "一家公司在没有进行全面测试的情况下从Moment.js切换到Day.js，由于两个相似库之间的细微API差异，导致了严重的回归问题。解决方案需要创建一个从底层日期库解耦的抽象包装器和全面的单元测试。"
      },
      "fr": {
        "title": "La bataille du temps",
        "summary": "Le passage d'une entreprise de Moment.js à Day.js sans tests complets a causé des régressions importantes en raison des différences subtiles d'API entre les bibliothèques similaires. La solution a nécessité la création d'une couche d'abstraction qui découple de la bibliothèque de date sous-jacente et des tests unitaires complets."
      },
      "de": {
        "title": "Der Kampf um die Zeit",
        "summary": "Der Wechsel eines Unternehmens von Moment.js zu Day.js ohne umfassende Tests führte zu erheblichen Regressionen aufgrund subtiler API-Unterschiede zwischen den ähnlichen Bibliotheken. Die Lösung erforderte die Erstellung eines Abstraktionswrappers, der sich von der zugrunde liegenden Datumsbibliothek entkoppelt, und umfassende Unit-Tests."
      },
      "es": {
        "title": "La batalla por el tiempo",
        "summary": "El cambio de una empresa de Moment.js a Day.js sin pruebas integrales causó regresiones significativas debido a diferencias sutiles en la API entre las bibliotecas similares. La solución requirió crear un envoltorio de abstracción que se desacople de la biblioteca de fechas subyacente y pruebas unitarias integrales."
      }
    }
  },
  {
    "title": "How the VM Actually Executes a Program in SQLite",
    "slug": "sqlite-vm-bytecode-execution",
    "url": "https://dev.to/lovestaco/how-the-vm-actually-executes-a-program-in-sqlite-28pj",
    "source": "DEV Community",
    "date": "2026-02-26T17:48:44.000Z",
    "summary": "SQLite's virtual machine executes bytecode through a simple interpreter loop that starts at instruction 0 and continues until a Halt instruction or program end. The VM enforces atomicity by rolling back transactions if execution fails mid-process, ensuring consistency before returning control.",
    "content": "Hello, I'm Maneshwar. I'm working on git-lrc: a Git hook for Checking AI generated code.\nSo far, we’ve looked at bytecode instructions, insert logic, and join logic. But we haven’t yet stepped into the interpreter itself.\nToday we answer a very direct question:\nHow does SQLite’s VM execute a bytecode program?\nEvery bytecode program starts at instruction 0.\nThe VM continues executing instructions until one of three things happens:\nIt encounters an explicit Halt instruction.\nThe program counter (pc) moves past the last instruction this is treated as an implicit Halt.\nAn execution error occurs.\nWhen execution stops, SQLite performs cleanup, all open cursors are closed, memory allocated to the VM is released and if an error occurred, any pending transaction is rolled back.\nThat last point is critical. The VM is not just executing instructions, it is also enforcing atomicity. If something fails mid-execution, the system restores consistency before returning control.\nAt the core of SQLite’s execution engine is a function named:\nsqlite3VdbeExec\n\nThis function takes a pointer to a Vdbe object (the prepared statement). Internally, the interpreter is remarkably simple:\nA for loop\nA large switch statement\n\n\n\n\nEach iteration:\nFetch instruction from aOp[pc]\n\nDecode opcode\nExecute corresponding case block\nIncrement or modify pc\n\n\n\nMost instructions simply do:\npc++\n\nBut jump instructions overwrite pc with a new value.\nExecution continues until either:\nA Halt is processed\nOr pc >= nOp (past last instruction)\nThat condition marks normal termination.\nHere is the structural overview:\n\nWhat’s striking is how unremarkable the structure is. No deep recursion. No complex scheduling. Just a deterministic interpreter loop.\nThe complexity lies not in the loop — but in the semantics of each opcode case.\nWhen you call sqlite3_prepare, the pointer you receive (sqlite3_stmt*) is actually a pointer to a Vdbe object.\nThat object holds everything needed to execute the program.\nSome important compon",
    "category": "github",
    "translations": {
      "zh": {
        "title": "SQLite 虚拟机如何实际执行程序",
        "summary": "SQLite 的虚拟机通过简单的解释器循环执行字节码，从指令 0 开始，直到遇到 Halt 指令或程序结束。虚拟机通过在执行失败时回滚事务来强制原子性，确保在返回控制权之前的一致性。"
      },
      "fr": {
        "title": "Comment la machine virtuelle exécute réellement un programme dans SQLite",
        "summary": "La machine virtuelle de SQLite exécute le bytecode via une simple boucle d'interpréteur qui commence à l'instruction 0 et continue jusqu'à une instruction Halt ou la fin du programme. La VM applique l'atomicité en annulant les transactions si l'exécution échoue en cours de processus, garantissant la cohérence avant de rendre le contrôle."
      },
      "de": {
        "title": "Wie die VM ein Programm in SQLite tatsächlich ausführt",
        "summary": "SQLites virtuelle Maschine führt Bytecode durch eine einfache Interpreter-Schleife aus, die bei Anweisung 0 beginnt und bis zu einer Halt-Anweisung oder dem Programmende fortgesetzt wird. Die VM erzwingt Atomarität durch Rollback von Transaktionen, wenn die Ausführung fehlschlägt, was Konsistenz vor der Rückgabe der Kontrolle gewährleistet."
      },
      "es": {
        "title": "Cómo la máquina virtual ejecuta realmente un programa en SQLite",
        "summary": "La máquina virtual de SQLite ejecuta bytecode a través de un simple bucle de intérprete que comienza en la instrucción 0 y continúa hasta una instrucción Halt o el final del programa. La VM aplica atomicidad revirtiendo transacciones si la ejecución falla durante el proceso, garantizando consistencia antes de devolver el control."
      }
    }
  },
  {
    "title": "Adding Capacitor to Glif with Antigravity: The Good, The Bad, and The Reality Check 📱",
    "slug": "adding-capacitor-glif-antigravity",
    "url": "https://dev.to/playfulprogramming/adding-capacitor-to-glif-with-antigravity-the-good-the-bad-and-the-reality-check-4pm8",
    "source": "DEV Community",
    "date": "2026-02-26T17:46:28.000Z",
    "summary": "The author used Google Antigravity to integrate Capacitor into Glif, a Nuxt-based QR code generator, enabling native iOS and Android apps. While Antigravity provided useful assistance, the integration proved more complex than expected despite the agent's strong frontend capabilities.",
    "content": "Overview\n\n\nHey everyone 👋\nI recently decided to take Glif, my minimalist QR code generator web app, and turn it into a proper mobile app using Capacitor. For those who don't know, Glif is a simple Nuxt app that lets you create and download customizable QR codes, nothing fancy, just clean and functional.\nSince I've been using Google Antigravity for various projects, I figured: why not let it handle the Capacitor integration? It's supposed to be great for frontend work, and mobile development is definitely frontend territory, right?\nWell... let me tell you about that experience. Spoiler alert: Antigravity helped, but it wasn't the smooth ride I expected.\nLet's start! 🤙\nFirst, a quick explainer for anyone unfamiliar. Capacitor is Ionic's cross-platform runtime that lets you turn web apps into native mobile apps for iOS and Android. Think of it as a bridge: your web code (HTML, CSS, JavaScript) runs inside a native container, and Capacitor provides APIs to access device features like the camera, filesystem, notifications, etc.\nFor a tool like Glif, going mobile made perfect sense:\nWhy Glif Needs Mobile:\nQR codes are inherently mobile-first (people scan them with phones)\nUsers might want to generate QR codes on the go\nHaving a native app feels more legitimate than \"just a website\"\nAccess to device features like sharing, saving to photos, etc.\nPotential for offline functionality\nThe web version works fine, but a native mobile app would be the natural evolution. Capacitor seemed like the obvious choice, it's well-maintained, works great with Vue/Nuxt, and doesn't require rewriting the entire app.\nI've had good experiences using Antigravity for frontend work (I literally built an entire Material Design CSS library with it), so I was optimistic. My thinking was:\nCapacitor integration is well-documented\nIt's mostly configuration and boilerplate\nAntigravity should be able to follow the Capacitor docs\nThe agent can test the mobile build in the browser\nThis should be straightf",
    "category": "github",
    "translations": {
      "zh": {
        "title": "使用 Antigravity 为 Glif 添加 Capacitor：好的、坏的和现实检验 📱",
        "summary": "作者使用 Google Antigravity 将 Capacitor 集成到 Glif（一个基于 Nuxt 的二维码生成器）中，实现了原生 iOS 和 Android 应用。尽管 Antigravity 提供了有用的帮助，但集成仍然比预期更复杂，尽管该代理具有强大的前端功能。"
      },
      "fr": {
        "title": "Ajouter Capacitor à Glif avec Antigravity : Le Bien, Le Mal et La Vérité 📱",
        "summary": "L'auteur a utilisé Google Antigravity pour intégrer Capacitor dans Glif, un générateur de codes QR basé sur Nuxt, permettant des applications iOS et Android natives. Bien que Antigravity ait fourni une assistance utile, l'intégration s'est avérée plus complexe que prévu malgré les fortes capacités frontales de l'agent."
      },
      "de": {
        "title": "Capacitor zu Glif mit Antigravity hinzufügen: Das Gute, Das Schlechte und die Realitätsprüfung 📱",
        "summary": "Der Autor nutzte Google Antigravity, um Capacitor in Glif, einen auf Nuxt basierenden QR-Code-Generator, zu integrieren und native iOS- und Android-Apps zu ermöglichen. Obwohl Antigravity hilfreiche Unterstützung bot, erwies sich die Integration trotz der starken Frontend-Fähigkeiten des Agenten komplexer als erwartet."
      },
      "es": {
        "title": "Agregar Capacitor a Glif con Antigravity: Lo Bueno, Lo Malo y La Verificación de la Realidad 📱",
        "summary": "El autor utilizó Google Antigravity para integrar Capacitor en Glif, un generador de códigos QR basado en Nuxt, permitiendo aplicaciones iOS y Android nativas. Aunque Antigravity proporcionó asistencia útil, la integración resultó ser más compleja de lo esperado a pesar de las sólidas capacidades de frontend del agente."
      }
    }
  },
  {
    "title": "I Built a Second Brain That Runs While I Sleep",
    "slug": "second-brain-runs-while-sleep",
    "url": "https://dev.to/prefrontalsys/i-built-a-second-brain-that-runs-while-i-sleep-4gc1",
    "source": "DEV Community",
    "date": "2026-02-26T17:45:54.000Z",
    "summary": "The author built a distributed knowledge management system using git, background agents on a VPS, and Tailscale for secure coordination between machines. This infrastructure-first approach enables automated research and document processing while disconnected from the user's active work.",
    "content": "How I am finally getting my knowledge collection habit under control\n\n\nThere's a moment in every PKM journey where you stop adding plugins and start writing infrastructure. For me, that moment arrived when I realized my Obsidian vault had outgrown Obsidian.\nNot the app itself. Obsidian is still the editor, the graph, the daily driver. But the system around it is Ansible playbooks, a VPS running background agents at 3am, a Telegram bot relaying Claude Code sessions to my phone, a local search engine indexing over a thousand documents, and git as the only database.\nThis is what happens when you treat a knowledge base as infrastructure instead of a hobby.\nTwo machines. One git repo. No database.\n\nThe Mac is where I think. Obsidian is open, Claude Code is in the terminal, and I'm writing and connecting ideas. The VPS is where the vault works while I don't. Background research, health monitoring, queue dispatch, all running without me.\nThey coordinate through git. That's it. No sync service, no real-time protocol. Just commits.\nThe VPS has no public ports. None. No SSH on 22, no HTTP on 80 or 443, no anything. UFW allows exactly two things: Tailscale's WireGuard UDP port and traffic on the Tailscale interface itself.\n# The entire VPS firewall policy\nufw_rules:\n  - rule: allow\n    port: \"41641\"\n    proto: udp\n    comment: \"Tailscale WireGuard\"\n  - rule: allow\n    interface: tailscale0\n    direction: in\n    comment: \"Tailscale interface\"\n\nEvery connection between Mac and VPS runs over Tailscale's mesh. SSH, git sync, the Obsidian MCP bridge from VPS back to the Mac's Obsidian instance, the duty officer dashboard. If you're not on my tailnet, the VPS doesn't exist.\nThis is what makes the rest of the architecture possible. CCBot can run claude --dangerously-skip-permissions because the machine it runs on has no public attack surface. Background agents can write to the vault because the only way in is through my devices. The security model is \"no ingress\" rather than \"careful",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我构建了一个在我睡眠时运行的第二大脑",
        "summary": "作者使用 git、VPS 上的后台代理和 Tailscale 构建了一个分布式知识管理系统，用于机器之间的安全协调。这种基础设施优先的方法能够在用户与活动工作断开连接时实现自动研究和文档处理。"
      },
      "fr": {
        "title": "J'ai construit un deuxième cerveau qui fonctionne pendant que je dors",
        "summary": "L'auteur a construit un système de gestion des connaissances distribué en utilisant git, des agents d'arrière-plan sur un VPS et Tailscale pour la coordination sécurisée entre machines. Cette approche axée sur l'infrastructure permet la recherche automatisée et le traitement des documents lorsque l'utilisateur est déconnecté du travail actif."
      },
      "de": {
        "title": "Ich habe ein zweites Gehirn gebaut, das läuft, während ich schlafe",
        "summary": "Der Autor erstellte ein verteiltes Wissensmanagementsystem mit git, Hintergrund-Agenten auf einem VPS und Tailscale zur sicheren Koordination zwischen Maschinen. Dieser infrastrukturorientierte Ansatz ermöglicht automatisierte Forschung und Dokumentverarbeitung, wenn der Benutzer von der aktiven Arbeit getrennt ist."
      },
      "es": {
        "title": "Construí un segundo cerebro que funciona mientras duermo",
        "summary": "El autor construyó un sistema de gestión del conocimiento distribuido utilizando git, agentes de fondo en un VPS y Tailscale para la coordinación segura entre máquinas. Este enfoque basado en infraestructura permite investigación automatizada y procesamiento de documentos mientras el usuario está desconectado del trabajo activo."
      }
    }
  },
  {
    "title": "Open Source Endowment – new funding source for open source maintainers",
    "slug": "open-source-endowment-funding",
    "url": "https://endowment.dev/",
    "source": "Hacker News",
    "date": "2026-02-26T16:13:06.000Z",
    "summary": "New endowment-based funding program designed to provide sustainable financial support for open source software maintainers, addressing the long-standing challenge of financing critical public infrastructure projects.",
    "content": "Article URL: https://endowment.dev/\nComments URL: https://news.ycombinator.com/item?id=47168012\nPoints: 225\n# Comments: 139",
    "category": "github"
  },
  {
    "title": "Will vibe coding end like the maker movement?",
    "slug": "vibe-coding-maker-movement",
    "url": "https://read.technically.dev/p/vibe-coding-and-the-maker-movement",
    "source": "Hacker News",
    "date": "2026-02-26T16:07:11.000Z",
    "summary": "Hacker News thread examining whether the casual \"vibe coding\" trend will follow the arc of the maker movement, discussing parallels between the two phenomena and implications for accessibility in software development.",
    "content": "Article URL: https://read.technically.dev/p/vibe-coding-and-the-maker-movement\nComments URL: https://news.ycombinator.com/item?id=47167931\nPoints: 351\n# Comments: 348",
    "category": "github"
  },
  {
    "title": "Anthropic ditches its core safety promise",
    "slug": "anthropic-ditches-safety-promise",
    "url": "https://www.cnn.com/2026/02/25/tech/anthropic-safety-policy-change",
    "source": "Hacker News",
    "date": "2026-02-26T12:52:50.000Z",
    "summary": "Anthropic has changed its core safety policy, according to a CNN report discussed on Hacker News with significant engagement. The shift marks a notable change in the company's stated priorities regarding AI safety.",
    "content": "Article URL: https://www.cnn.com/2026/02/25/tech/anthropic-safety-policy-change\nComments URL: https://news.ycombinator.com/item?id=47165397\nPoints: 469\n# Comments: 257",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Anthropic放弃了其核心安全承诺",
        "summary": "根据CNN的报道，Anthropic改变了其核心安全政策，该报道在黑客新闻上引起了广泛讨论。这一转变标志着该公司在人工智能安全方面所声明的优先事项发生了显著变化。"
      },
      "fr": {
        "title": "Anthropic abandonne sa promesse de sécurité fondamentale",
        "summary": "Selon un rapport de CNN discuté sur Hacker News avec un grand engagement, Anthropic a modifié sa politique de sécurité fondamentale. Ce changement marque un changement notable dans les priorités déclarées de l'entreprise concernant la sécurité de l'IA."
      },
      "de": {
        "title": "Anthropic gibt sein grundlegendes Sicherheitsversprechen auf",
        "summary": "Laut einem CNN-Bericht, der auf Hacker News intensiv diskutiert wurde, hat Anthropic seine grundlegende Sicherheitspolitik geändert. Die Verschiebung markiert eine bemerkenswerte Änderung in den erklärten Prioritäten des Unternehmens in Bezug auf KI-Sicherheit."
      },
      "es": {
        "title": "Anthropic abandona su promesa de seguridad fundamental",
        "summary": "De acuerdo con un informe de CNN discutido en Hacker News con un gran compromiso, Anthropic ha cambiado su política de seguridad fundamental. El cambio marca un cambio notable en las prioridades declaradas de la empresa con respecto a la seguridad de la IA."
      }
    }
  },
  {
    "title": "How to Use React Query with React Router Loaders (Pre-fetch & Cache Data)",
    "slug": "how-to-use-react-query-with-react-router-loaders-pre-fetch-cache",
    "url": "https://dev.to/edriso/how-to-use-react-query-with-react-router-loaders-pre-fetch-cache-data-kag",
    "source": "DEV Community",
    "date": "2026-02-26T12:11:05.000Z",
    "summary": "Combining React Query with React Router loaders enables data pre-fetching before components mount, eliminating loading spinners. Using queryClient.ensureQueryData() checks the cache first and fetches fresh data if needed, ensuring the page renders immediately when mounted.",
    "content": "The Problem\n\n\nWhen you navigate to a page, there's usually a delay while data is being fetched. The user sees a loading spinner, and the content pops in after the request finishes. Not great.\nWhat if the data was already there when the page loads?\nThat's exactly what combining React Query with React Router loaders gives you.\nLoader runs before the component mounts (React Router calls it on navigation).\nInside the loader, we ask React Query: \"Do you already have this data cached?\"\n\n\n\nYes → Use it instantly. No network request.\nNo → Fetch it now, wait for it, then cache it.\nWhen the component finally mounts, it calls useQuery with the same query. Since the data is already cached, it renders immediately — no loading state.\nThe key method is queryClient.ensureQueryData(queryOptions). Think of it as: \"Make sure this data exists — get it from cache or fetch it.\"\nLet's build a page that shows Pokémon details. When you navigate to /pokemon/pikachu, the data is already loaded.\n// pages/Pokemon.jsx\nimport { useQuery } from '@tanstack/react-query';\nimport { useLoaderData } from 'react-router-dom';\nimport axios from 'axios';\n\n// A function that returns the query config (key + fetch function).\n// We reuse this in BOTH the loader and the component.\nconst pokemonQuery = (name) => {\n  return {\n    queryKey: ['pokemon', name],\n    queryFn: async () => {\n      const response = await axios.get(\n        `https://pokeapi.co/api/v2/pokemon/${name}`\n      );\n      return response.data;\n    },\n  };\n};\n\n// The loader receives queryClient from the router setup (see step 4).\n// It runs BEFORE the component mounts.\nexport const loader = (queryClient) => {\n  return async ({ params }) => {\n    const { name } = params;\n\n    // ensureQueryData checks the cache first:\n    //   - cached? → returns it instantly\n    //   - not cached? → fetches, caches, and returns it\n    await queryClient.ensureQueryData(pokemonQuery(name));\n\n    // We only return the param — the actual data lives in React Query's ca",
    "category": "github",
    "translations": {
      "zh": {
        "title": "如何将React Query与React Router加载程序一起使用（预获取和缓存数据）",
        "summary": "将React Query与React Router加载程序结合使用可以在组件挂载前预获取数据，消除加载旋转器。使用queryClient.ensureQueryData()首先检查缓存，如果需要则获取新鲜数据，确保页面在挂载时立即呈现。"
      },
      "fr": {
        "title": "Comment utiliser React Query avec les loaders de React Router (pré-récupération et mise en cache des données)",
        "summary": "La combinaison de React Query avec les loaders de React Router permet la pré-récupération des données avant que les composants ne se montent, éliminant les spinners de chargement. L'utilisation de queryClient.ensureQueryData() vérifie d'abord le cache et récupère les données fraîches si nécessaire, garantissant que la page s'affiche immédiatement lors du montage."
      },
      "de": {
        "title": "Verwendung von React Query mit React Router-Loadern (Vorab-Abrufen und Zwischenspeicherung von Daten)",
        "summary": "Das Kombinieren von React Query mit React Router-Loadern ermöglicht das Vorab-Abrufen von Daten, bevor Komponenten bereitgestellt werden, und beseitigt Lade-Spinner. Die Verwendung von queryClient.ensureQueryData() prüft zunächst den Cache und ruft bei Bedarf neue Daten ab, um sicherzustellen, dass die Seite beim Bereitstellen sofort gerendert wird."
      },
      "es": {
        "title": "Cómo usar React Query con React Router Loaders (Captura previa y almacenamiento en caché de datos)",
        "summary": "La combinación de React Query con los cargadores de React Router permite la captura previa de datos antes de que se monten los componentes, eliminando los indicadores de carga. El uso de queryClient.ensureQueryData() verifica primero el caché y obtiene datos frescos si es necesario, asegurando que la página se procese inmediatamente cuando se monta."
      }
    }
  },
  {
    "title": "Appends for AI apps: Stream into a single message with Ably AI Transport",
    "slug": "appends-for-ai-apps-stream-into-single-message-ably-ai-transport",
    "url": "https://dev.to/ablyblog/appends-for-ai-apps-stream-into-a-single-message-with-ably-ai-transport-398a",
    "source": "DEV Community",
    "date": "2026-02-26T12:05:30.000Z",
    "summary": "Ably AI Transport's message appends feature streams AI tokens into a single message rather than individual messages, solving fragmentation issues from client disconnections and refreshes. This approach simplifies UI reconstruction and message history handling without manual orchestration.",
    "content": "Streaming tokens is easy. Resuming cleanly is not. A user refreshes mid-response, another client joins late, a mobile connection drops for 10 seconds, and suddenly your \"one answer\" is 600 tiny messages that your UI has to stitch back together. Message history turns into fragments. You start building a side store just to reconstruct \"the response so far\".\nThis is not a model problem. It's a delivery problem\nThat's why we developed message appends for Ably AI Transport. Appends let you stream AI output tokens into a single message as they are produced, so you get progressive rendering for live subscribers and a clean, compact response in history.\nThe failure mode we're fixing\n\n\nThe usual implementation is to stream each token as a single message, which is simple and works perfectly on a stable connection. In production, clients disconnect and resume mid-stream: refreshes, mobile dropouts, backgrounded tabs, and late joins.\nOnce you have real reconnects and refreshes, you inherit work you did not plan for: ordering, dedupe, buffering, \"latest wins\" logic, and replay rules that make history and realtime agree. You can build it, but it is the kind of work that quietly eats weeks of engineering time.\n\nWith appends you can avoid that by changing the shape of the data. Instead of hundreds of token messages, you have one response message whose content grows over time.\nIn Ably AI Transport, you publish an initial response message and capture its server-assigned serial. That serial is what you append to.\nIt's a small detail that ends up doing a lot of work for you:\nconst result = await channel.publish({ name: 'response', data: '' });\nconst { serials: [msgSerial] } = result;\n\nNow, as your model yields tokens, you append each fragment to that same message:\nif (event.type === 'token') {\n  channel.appendMessage({ serial: msgSerial, data: event.text });\n}\n\nWhat changes for clients\n\n\nSubscribers still see progressive output, but they see it as actions on the same message serial. A",
    "category": "github",
    "translations": {
      "zh": {
        "title": "AI应用程序的追加：使用Ably AI Transport流入单个消息",
        "summary": "Ably AI Transport的消息追加功能将AI令牌流入单个消息而不是多个消息，解决了客户端断开连接和刷新导致的碎片化问题。这种方法简化了UI重建和消息历史处理，无需手动编排。"
      },
      "fr": {
        "title": "Appends pour les applications IA : Flux dans un message unique avec Ably AI Transport",
        "summary": "La fonctionnalité d'ajout de messages d'Ably AI Transport diffuse les jetons IA dans un message unique plutôt que dans des messages individuels, résolvant les problèmes de fragmentation dus aux déconnexions et actualisations des clients. Cette approche simplifie la reconstruction de l'interface utilisateur et la gestion de l'historique des messages sans orchestration manuelle."
      },
      "de": {
        "title": "Appends für KI-Apps: Stream in eine einzelne Nachricht mit Ably AI Transport",
        "summary": "Die Nachrichtenanhang-Funktion von Ably AI Transport streamt KI-Token in eine einzelne Nachricht statt in einzelne Nachrichten und löst Fragmentierungsprobleme durch Client-Trennungen und Aktualisierungen. Dieser Ansatz vereinfacht die UI-Rekonstruktion und das Nachrichtenverlauf-Management ohne manuelle Orchestrierung."
      },
      "es": {
        "title": "Appends para aplicaciones de IA: Stream en un mensaje único con Ably AI Transport",
        "summary": "La función de adición de mensajes de Ably AI Transport transmite tokens de IA en un mensaje único en lugar de mensajes individuales, resolviendo problemas de fragmentación por desconexiones y actualizaciones de clientes. Este enfoque simplifica la reconstrucción de la interfaz de usuario y el manejo del historial de mensajes sin orquestación manual."
      }
    }
  },
  {
    "title": "Node.js Application with CI/CD GitLab Pipeline on AWS EC2",
    "slug": "node-js-application-ci-cd-gitlab-pipeline-aws-ec2",
    "url": "https://dev.to/addwebsolutionpvtltd/nodejs-application-with-cicd-gitlab-pipeline-on-aws-ec2-2kk9",
    "source": "DEV Community",
    "date": "2026-02-26T12:01:06.000Z",
    "summary": "This guide demonstrates setting up a CI/CD pipeline for Node.js applications using GitLab CI/CD to automate testing, building, and deployment to AWS EC2 instances. The automated workflow reduces manual errors and accelerates development cycles through SSH-based code deployment and PM2-managed application restarts.",
    "content": "“Automation is the key to speed and reliability in modern software development.”\nIntroduction\nArchitecture Overview\nPrerequisites\nCI/CD Workflow (Step-by-Step)\nGitLab Pipeline Configuration\nDeployment Process on AWS EC2\nSecurity Best Practices\nInteresting Facts & Statistics\nFAQs\nKey Takeaways\nConclusion\nContinuous Integration and Continuous Deployment (CI/CD) is a modern development practice that automates the process of building, testing, and deploying applications. This document explains how to set up a CI/CD pipeline for a Node.js application using GitLab CI/CD and deploy it automatically to an AWS EC2 instance.\nAutomate deployment\nReduce manual errors\nImprove development speed\nEnsure reliable releases\nBackend: Node.js\nVersion Control: GitLab\nCI/CD Tool: GitLab Pipeline\nServer: AWS EC2 (Ubuntu)\nProcess Manager: PM2\nSSH Authentication: Secure Key-based login\nHigh-level Flow:\nDeveloper pushes code to GitLab repository.\nGitLab pipeline triggers automatically.\nPipeline installs dependencies and builds project.\nGitLab connects to EC2 via SSH.\nCode is pulled on EC2 server.\nApplication restarts using PM2.\nNginx routes HTTP traffic to the Node.js app\nBefore setting up CI/CD, ensure the following:\nGitLab Setup\nGitLab repository created\nBranches (dev/stage/prod) configured\nGitLab Runner enabled (shared runner works)\nAWS EC2 Setup\nUbuntu EC2 instance running\nNode.js & npm installed\nGit installed on server\nSSH access configured\nPM2 installed globally\n\n\n\nnpm install pm2 -g\n\nSSH Key Setup\nGenerate SSH key on local system:\n\n\n\n    ssh-keygen -t rsa -b 4096\n\nAdd public key to EC2:\n\n\n\n    ~/.ssh/authorized_keys\n\n- Add private key in GitLab:\n    **GitLab → Settings → CI/CD → Variables**\n\nSSH_PRIVATE_KEY\nSSH_HOST\nSSH_USER\nStep 1: Developer Pushes Code\nStep 2: Pipeline Triggered\nStep 3: Install Dependencies\nStep 4: SSH Connection\nStep 5: Deployment\nStep 6: Live Deployment\n“CI/CD turns deployment from a risky event into a routine process.”\nCreate .gitlab-ci.yml in project root:\nproduc",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Node.js应用程序与CI/CD GitLab管道在AWS EC2上的集成",
        "summary": "本指南演示了如何使用GitLab CI/CD为Node.js应用程序设置CI/CD管道，以自动化测试、构建和部署到AWS EC2实例。自动化工作流通过基于SSH的代码部署和PM2管理的应用程序重启来减少手动错误并加快开发周期。"
      },
      "fr": {
        "title": "Application Node.js avec pipeline CI/CD GitLab sur AWS EC2",
        "summary": "Ce guide démontre comment configurer un pipeline CI/CD pour les applications Node.js en utilisant GitLab CI/CD pour automatiser le test, la construction et le déploiement sur des instances AWS EC2. Le flux de travail automatisé réduit les erreurs manuelles et accélère les cycles de développement grâce au déploiement de code basé sur SSH et aux redémarrages d'application gérés par PM2."
      },
      "de": {
        "title": "Node.js-Anwendung mit CI/CD-GitLab-Pipeline auf AWS EC2",
        "summary": "Diese Anleitung demonstriert die Einrichtung einer CI/CD-Pipeline für Node.js-Anwendungen mit GitLab CI/CD zur Automatisierung von Tests, Builds und Bereitstellung auf AWS EC2-Instanzen. Der automatisierte Workflow reduziert manuelle Fehler und beschleunigt Entwicklungszyklen durch SSH-basierte Code-Bereitstellung und PM2-verwaltete Anwendungsneustart."
      },
      "es": {
        "title": "Aplicación Node.js con pipeline CI/CD de GitLab en AWS EC2",
        "summary": "Esta guía demuestra cómo configurar un pipeline CI/CD para aplicaciones Node.js usando GitLab CI/CD para automatizar pruebas, compilación e implementación en instancias de AWS EC2. El flujo de trabajo automatizado reduce errores manuales y acelera los ciclos de desarrollo mediante la implementación de código basada en SSH y reinicios de aplicaciones gestionados por PM2."
      }
    }
  },
  {
    "title": "The £20 Billion Handshake",
    "slug": "the-20-billion-handshake",
    "url": "https://dev.to/rawveg/the-ps20-billion-handshake-2mmk",
    "source": "DEV Community",
    "date": "2026-02-26T12:00:00.000Z",
    "summary": "Google pays Apple $20 billion annually to remain Safari's default search engine, a relationship exposed during DOJ antitrust proceedings that reflects deeper competition among tech giants. As AI-powered search tools reshape how users find information, these backend licensing deals increasingly determine competitive advantage in the digital economy.",
    "content": "The smartphone in your pocket contains a curious paradox. Apple, one of the world's most valuable companies, builds its own chips, designs its own operating system, and controls every aspect of its ecosystem with obsessive precision. Yet when you tap Safari's search bar, you're not using an Apple search engine. You're using Google. And Google pays Apple a staggering $20 billion every year to keep it that way.\nThis colossal payment, revealed during the US Department of Justice's antitrust trial against Google, represents far more than a simple business arrangement. It's the visible tip of a fundamental transformation in how digital platforms compete, collaborate, and ultimately extract value from the billions of searches and queries humans perform daily. As artificial intelligence reshapes the search landscape and digital assistants become genuine conversational partners rather than glorified keyword matchers, these backend licensing deals are quietly redrawing the competitive map of the digital economy.\nThe stakes have never been higher. Search advertising generated $102.9 billion in revenue in the United States alone during 2024, accounting for nearly 40 per cent of all digital advertising spending. But the ground is shifting beneath the industry's feet. AI-powered search experiences from OpenAI's ChatGPT, Microsoft's Copilot, and Google's own AI Overviews are fundamentally changing how people find information, and these changes threaten to upend decades of established business models. Into this volatile mix come a new wave of licensing deals, platform partnerships, and strategic alliances that could determine which companies dominate the next generation of digital interaction.\nTo understand where we're heading, it helps to grasp how we got here. Google's dominance in search wasn't accidental. The company built the best search engine, captured roughly 90 per cent of the market, and then methodically paid billions to ensure its search bar appeared by default on ever",
    "category": "github",
    "translations": {
      "zh": {
        "title": "200亿英镑的握手",
        "summary": "谷歌每年向苹果支付200亿美元以保持在Safari中作为默认搜索引擎的地位,这一关系在司法部反垄断诉讼中曝光,反映了科技巨头之间更深层的竞争。随着由人工智能驱动的搜索工具重塑用户如何查找信息,这些后端许可协议日益决定了数字经济中的竞争优势。"
      },
      "fr": {
        "title": "La Poignée de Main de 20 Milliards de Livres",
        "summary": "Google paie à Apple 20 milliards de dollars par an pour rester le moteur de recherche par défaut de Safari, une relation exposée lors des poursuites antitrust du ministère de la Justice qui reflète une concurrence plus profonde entre les géants de la technologie. À mesure que les outils de recherche alimentés par l'IA redéfinissent la façon dont les utilisateurs trouvent des informations, ces accords de licence de backend déterminent de plus en plus l'avantage concurrentiel dans l'économie numérique."
      },
      "de": {
        "title": "Der 20-Milliarden-Pfund-Handschlag",
        "summary": "Google zahlt Apple jährlich 20 Milliarden Dollar, um die Standard-Suchmaschine von Safari zu bleiben, eine Beziehung, die in den Kartellverfahren des US-Justizministeriums offengelegt wurde und tiefer gehende Konkurrenzen zwischen Technologieriesen widerspiegelt. Da KI-gestützte Suchtools die Art und Weise neu gestalten, wie Benutzer Informationen finden, bestimmen diese Backend-Lizenzvereinbarungen zunehmend den Wettbewerbsvorteil in der digitalen Wirtschaft."
      },
      "es": {
        "title": "El Apretón de Manos de 20 Mil Millones de Libras",
        "summary": "Google paga a Apple 20 mil millones de dólares anuales para seguir siendo el motor de búsqueda predeterminado de Safari, una relación expuesta durante los procedimientos antimonopolio del Departamento de Justicia que refleja una competencia más profunda entre los gigantes tecnológicos. A medida que las herramientas de búsqueda impulsadas por IA remodelan cómo los usuarios encuentran información, estos acuerdos de licencia de backend determinan cada vez más la ventaja competitiva en la economía digital."
      }
    }
  },
  {
    "title": "Kubernetes'te StorageClass Nedir?",
    "slug": "kubernetes-storageclass-nedir",
    "url": "https://dev.to/tarikanafarta/kuberneteste-storageclass-nedir-dha",
    "source": "DEV Community",
    "date": "2026-02-26T11:49:45.000Z",
    "summary": "Kubernetes storage architecture comprises PV (Persistent Volumes), PVC (Persistent Volume Claims), and StorageClass, supporting both static and dynamic provisioning models. Data storage can reside on node-local disks, NFS shares, or cloud providers depending on StorageClass policies and PV configuration.",
    "content": "PV, PVC ve Veri Gerçekten Nerede Saklanıyor?\n\n\nBu yazıda şu konuları inceleyeceğim:\nPV (Persistent Volume) nedir?\nPVC (Persistent Volume Claim) nedir?\nStorageClass ne işe yarar?\nStatic vs Dynamic provisioning farkı nedir?\nMariaDB / PostgreSQL gibi uygulamalarda disk nerede?\nÖncelikle Kubernetes'te storage zinciri şu şekildedir:\nPod -> PVC -> StorageClass -> PV -> Physical Storage\nPV, cluster içindeki gerçek disk kaynağını temsil eder.\nBu kaynak şunlardan birisidir:\nNode üzerindeki local disk\nNFS share\nCloud disk\nDistributed storage\nYani aslında PV, storage'ın kendisidir.\nPVC ise uygulamanın disk talebidir.\nÖrneğin:\n10Gi storage\nAccess mode: ReadWriteOnce\nstorageClass: fast-storage\nPVC diski oluşturmaz, bir disk talep eder.\nStorageClass, PVC oluşturulduğunda disk'in nasıl sağlanacağını belirler.\nYani, disk local mi olacak? NFS mi olacak? Cloud block storage mı olacak? Otomatik mi üretilecek? Hangi performans sınıfı kullanılacak? Gibi sorulara cevap veren bir policy katmanıdır.\nStatic provisioning modelinde PV manuel olarak oluşturulur. Yani önce storage kaynağı tanımlanır, ardından PVC bu PV'ye bağlanır.\nBu yöntem daha fazla kontrol sağlar ancak her yeni disk ihtiyacında manuel işlem gerektirir. Genellikle test ortamlarında veya local/NFS kurulumlarında tercih edilir.\nDynamic provisioning modelinde ise sadece PVC oluşturulur.\nBu süreç CSI (Container Storage Interface) driver'ları sayesinde çalışır.\nBu sorunun cevabı PV tanımındadır.\nPV şu path'i gösteriyorsa: /mnt/k8s/data, disk node üzerindedir. Node arızalanırsa veri kaybedilebilir. Pod başka node'a taşınırsa diske erişilemez.\nPV şu şekilde tanımlanmışsa:\nnfs:\n  server: 192.168.1.10\n  path: /srv/nfs/share\n\nDisk NFS server üzerindedir. Pod hangi node'da olursa olsun aynı veriye erişebilir.\nPV bir cloud volume'a bağlıysa, disk cloud provider tarafındadır.\nRWO (ReadWriteOnce) -> Sadece tek node yazabilir\nPVC silindiğinde disk'in nasıl davranacağını reclaim policy belirler. Kubernetes'te 3 farklı reclaim policy vardır",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Kubernetes中的StorageClass是什么?",
        "summary": "Kubernetes存储架构包括PV(持久卷)、PVC(持久卷声明)和StorageClass,支持静态和动态配置模型。根据StorageClass策略和PV配置,数据存储可以驻留在节点本地磁盘、NFS共享或云提供商上。"
      },
      "fr": {
        "title": "Qu'est-ce que StorageClass dans Kubernetes?",
        "summary": "L'architecture de stockage Kubernetes comprend PV (Persistent Volumes), PVC (Persistent Volume Claims) et StorageClass, supportant les modèles de provisionnement statique et dynamique. Le stockage de données peut résider sur des disques locaux de nœud, des partages NFS ou des fournisseurs cloud en fonction des politiques StorageClass et de la configuration PV."
      },
      "de": {
        "title": "Was ist StorageClass in Kubernetes?",
        "summary": "Die Kubernetes-Speicherarchitektur umfasst PV (Persistent Volumes), PVC (Persistent Volume Claims) und StorageClass und unterstützt sowohl statische als auch dynamische Bereitstellungsmodelle. Der Datenspeicher kann je nach StorageClass-Richtlinien und PV-Konfiguration auf lokalen Node-Festplatten, NFS-Freigaben oder Cloud-Anbietern residieren."
      },
      "es": {
        "title": "¿Qué es StorageClass en Kubernetes?",
        "summary": "La arquitectura de almacenamiento de Kubernetes comprende PV (Volúmenes Persistentes), PVC (Reclamaciones de Volumen Persistentes) y StorageClass, admitiendo modelos de aprovisionamiento estático y dinámico. El almacenamiento de datos puede residir en discos locales de nodos, recursos compartidos NFS o proveedores en la nube dependiendo de las políticas de StorageClass y la configuración de PV."
      }
    }
  },
  {
    "title": "Why is cdk.out (Cloud Assembly) Necessary in AWS CDK?",
    "slug": "why-is-cdk-out-cloud-assembly-necessary-aws-cdk",
    "url": "https://dev.to/aws-heroes/why-is-cdkout-cloud-assembly-necessary-in-aws-cdk-n5f",
    "source": "DEV Community",
    "date": "2026-02-26T11:45:47.000Z",
    "summary": "AWS CDK's cdk.out directory stores the cloud assembly—an intermediate artifact containing CloudFormation templates, metadata, and asset files generated from CDK code. This assembly bridges CDK code to AWS infrastructure deployment and includes stack templates, manifest files, and Docker image assets.",
    "content": "What is cdk.out\n\n\ncdk.out is a directory that stores a collection of files called the cloud assembly.\nWhen you run the cdk synth command or the cdk deploy command, the cloud assembly is generated from your CDK code and stored in the cdk.out directory.\nThe cloud assembly is a collection of files generated by the synthesis of a CDK application (CDK code). The CDK CLI references this information to deploy resources to the AWS environment.\nIn other words, the cloud assembly can be considered an intermediate artifact that CDK uses to deploy infrastructure definitions written in CDK code to the AWS environment.\n\nThe cloud assembly primarily consists of the following files and directories:\n(stack name).template.json\n\n\nCloudFormation template generated by CDK code\n(stack name).assets.json\n\n\nFile describing information about assets used in CDK code\nmanifest.json\n\n\nFile describing metadata of the cloud assembly\ntree.json\n\n\nFile representing the Construct tree structure of resources defined in CDK code\ncdk.out\n\n\nFile storing Cloud Assembly schema version information\nasset.(hash value)/\n\n\nDirectory storing asset files used in CDK code\nUses hash values calculated for each asset as directory names\nEach asset is classified into two types: S3 assets and Docker image assets, which are uploaded to S3 buckets or ECR repositories created by the cdk bootstrap command when the cdk deploy command is executed\nDocker image assets are built when the cdk deploy command is executed (not when the cdk synth command is executed)\nassembly-(stage name)/\n\n\nDirectory storing cloud assemblies generated for each stage when using Stage Construct\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  Why cdk.out is Necessary\n\n\nAs explained above, cdk.out is a directory for storing the cloud assembly, which is an intermediate artifact generated from CDK code.\nSo why is this directory necessary? You might wonder whether it's really necessary to generate and store information needed for deployment as files.\nBefore explaining this, let me fir",
    "category": "github",
    "translations": {
      "zh": {
        "title": "为什么AWS CDK中的cdk.out(云程序集)是必要的?",
        "summary": "AWS CDK的cdk.out目录存储云程序集——一个中间制品,包含从CDK代码生成的CloudFormation模板、元数据和资产文件。此程序集连接CDK代码与AWS基础设施部署,包括堆栈模板、清单文件和Docker镜像资产。"
      },
      "fr": {
        "title": "Pourquoi cdk.out (Cloud Assembly) est-il nécessaire dans AWS CDK?",
        "summary": "Le répertoire cdk.out d'AWS CDK stocke l'assembly cloud — un artefact intermédiaire contenant les modèles CloudFormation, les métadonnées et les fichiers d'actifs générés à partir du code CDK. Cet assembly relie le code CDK au déploiement de l'infrastructure AWS et inclut les modèles de pile, les fichiers manifeste et les actifs d'image Docker."
      },
      "de": {
        "title": "Warum ist cdk.out (Cloud Assembly) in AWS CDK notwendig?",
        "summary": "Das Verzeichnis cdk.out von AWS CDK speichert die Cloud Assembly — ein Zwischenartefakt, das CloudFormation-Vorlagen, Metadaten und aus CDK-Code generierte Asset-Dateien enthält. Diese Assembly verbindet CDK-Code mit der AWS-Infrastruktur-Bereitstellung und umfasst Stack-Vorlagen, Manifest-Dateien und Docker-Image-Assets."
      },
      "es": {
        "title": "¿Por qué es necesario cdk.out (Cloud Assembly) en AWS CDK?",
        "summary": "El directorio cdk.out de AWS CDK almacena el cloud assembly—un artefacto intermedio que contiene plantillas de CloudFormation, metadatos y archivos de activos generados a partir del código CDK. Este assembly vincula el código CDK con la implementación de infraestructura de AWS e incluye plantillas de pila, archivos de manifiesto y activos de imagen Docker."
      }
    }
  },
  {
    "title": "My Pixel's Keyboard Generated a Custom Emoji.",
    "slug": "my-pixel-keyboard-generated-custom-emoji",
    "url": "https://dev.to/megzlawther1/my-pixels-keyboard-generated-a-custom-emoji-42k",
    "source": "DEV Community",
    "date": "2026-02-26T11:45:35.000Z",
    "summary": "Google Gemini's Emojify feature on Pixel 10 keyboards generates custom emojis dynamically beyond Unicode standards, creating novel visual communications not found in preset emoji tables. This on-device AI capability demonstrates how generative models can extend traditional interface constraints.",
    "content": "What I Built with Google Gemini: \nMy project isn't a traditional \"build\" in the sense of writing code for a new application. Instead, I built a novel interaction with Google Gemini, specifically through the \"Emojify\" feature on my Google Pixel 10 keyboard. My \"build\" was essentially demonstrating and exploring how a widely accessible AI feature can generate entirely new, non-standard visual communication..\nThe \"problem\" it solved (or rather, illuminated) is the limitation of pre-set, static emoji sets. While existing emojis are vast, they can't cover every nuanced concept. My interaction with Gemini showcased its ability to dynamically create a custom visual to perfectly fit a specific textual context, transcending the boundaries of Unicode.\nHere's what happened: \nThis wasn't through a dedicated generative AI interface; it was a seamless, on-device AI capability integrated into a common keyboard function. \nWhat I Learned:\nThis experience has been incredibly eye-opening, both technically and philosophically.\nGoing Beyond the Standard Emoji Table (and My Reaction to It): My biggest ponderance and a major learning point is how it was able to go beyond the standard emoji table of preset characters (Unicode/ASCII). Emojis are typically strict templates of codes, already preset and preloaded. Gemini's Emojify function went out of its preset and preloaded list to generate something entirely new. When I saw this custom visual, my immediate reaction was confusion. I automatically checked my keyboard's emoji list, particularly the \"Family\" section, and confirmed that the generated icon was not available there. My screenshot of the available standard family emojis (e.g., \"Family: Adult, Adult, Child\") clearly shows they are distinct and generic. This solidifies that the AI truly generated a novel image, rather than pulling from existing assets, and profoundly changed my understanding of what a keyboard's \"emojify\" function is capable of.\nAI's Gender Inference in Generative Out",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我的Pixel键盘生成了自定义表情符号",
        "summary": "谷歌Gemini在Pixel 10键盘上的Emojify功能动态生成超越Unicode标准的自定义表情符号，创造出预设表情符号表中找不到的新颖视觉通信。这种设备上的AI能力展示了生成模型如何能够扩展传统界面约束。"
      },
      "fr": {
        "title": "Mon clavier Pixel a généré un emoji personnalisé",
        "summary": "La fonction Emojify de Google Gemini sur les claviers Pixel 10 génère dynamiquement des emojis personnalisés au-delà des normes Unicode, créant de nouvelles communications visuelles introuvables dans les tableaux d'emojis préétablis. Cette capacité d'IA sur l'appareil démontre comment les modèles génératifs peuvent étendre les contraintes d'interface traditionnelles."
      },
      "de": {
        "title": "Meine Pixel-Tastatur hat ein benutzerdefiniertes Emoji generiert",
        "summary": "Googles Gemini-Emojify-Funktion auf Pixel-10-Tastaturen generiert dynamisch benutzerdefinierte Emojis jenseits von Unicode-Standards und erzeugt neuartige visuelle Kommunikation, die in voreingestellten Emoji-Tabellen nicht zu finden ist. Diese On-Device-KI-Funktion zeigt, wie generative Modelle traditionelle Schnittstellenbeschränkungen erweitern können."
      },
      "es": {
        "title": "Mi teclado Pixel generó un emoji personalizado",
        "summary": "La función Emojify de Google Gemini en los teclados Pixel 10 genera emojis personalizados de forma dinámica más allá de los estándares Unicode, creando comunicaciones visuales novedosas que no se encuentran en las tablas de emojis predefinidas. Esta capacidad de IA en el dispositivo demuestra cómo los modelos generativos pueden ampliar las restricciones tradicionales de la interfaz."
      }
    }
  },
  {
    "title": "I Built and Launched an AI Document API in Under a Week — Here's Exactly How I Did It",
    "slug": "i-built-and-launched-ai-document-api-under-week",
    "url": "https://dev.to/senzen/i-built-and-launched-an-ai-document-api-in-under-a-week-heres-exactly-how-i-did-it-4j8c",
    "source": "DEV Community",
    "date": "2026-02-26T11:45:08.000Z",
    "summary": "Developer launched Condensare, an AI document processing API leveraging GPT-4o to transform uploaded files into structured notes and contextual implementation suggestions. Deployed serverlessly on Vercel and published on RapidAPI, the solution was built and launched in under one week.",
    "content": "I Built and Launched an AI Document API in Under a Week — Here's Exactly How I Did It\n\n\n\nTL;DR: I built Condensare — an AI-powered document processing API that turns any uploaded file into structured notes and contextualised implementation suggestions. It's live on RapidAPI. This is the full breakdown of how I built it, deployed it, tested it and shipped it.\nA few weeks ago I had a simple frustration.\nI was uploading documents to various AI tools trying to get useful summaries and actionable insights, and every single one felt generic. The output didn't know what industry I was in, what scale my business operated at, or what I actually wanted to do with the information.\nIt just summarised. That was it.\nSo I built Condensare — a document condensing and AI suggestion API that takes any uploaded file and returns structured notes plus contextualised implementation suggestions.\nI kept it simple and production-focused:\n\n\n\nLayer\nTechnology\n\n\n\n\nAPI Server\nNode.js + Express\n\n\nAI\nOpenAI GPT-4o\n\n\nDeployment\nVercel (serverless)\n\n\nMarketplace\nRapidAPI\n\n\nFile Uploads\nMulter\n\n\nDocument Parsing\npdf-parse, mammoth, xlsx\n\n\n\nVercel serverless was the right call for an API marketplace product — zero infrastructure management, global distribution out of the box, automatic scaling. You don't want to be managing servers when you're trying to get to market fast.\ncondensare-api/\n├── src/\n│   ├── app.js                  # Express entry point\n│   ├── routes/\n│   │   └── condense.js         # Route definitions\n│   ├── middleware/\n│   │   └── authenticate.js     # Auth + plan management\n│   ├── services/\n│   │   ├── parseService.js     # File parsing logic\n│   │   ├── condenseService.js  # AI condensing logic\n│   │   └── suggestService.js   # AI suggestions logic\n│   └── utils/\n│       └── fileUtils.js        # File validation helpers\n├── vercel.json\n└── package.json\n\nPOST /api/v1/condense/parse\n\n\nAvailable on: FREE, PRO, MEGA, ULTRA\nThe lightweight endpoint. Upload any supported file and get b",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我在一周内构建并推出了一个AI文档API——以下是我的具体做法",
        "summary": "开发者推出了Condensare，这是一个利用GPT-4o的AI文档处理API，可以将上传的文件转换为结构化笔记和上下文实现建议。在Vercel上无服务器部署并在RapidAPI上发布，该解决方案在一周内构建和推出。"
      },
      "fr": {
        "title": "J'ai construit et lancé une API de documents IA en une semaine — Voici exactement comment j'ai fait",
        "summary": "Un développeur a lancé Condensare, une API de traitement de documents IA exploitant GPT-4o pour transformer les fichiers téléchargés en notes structurées et suggestions d'implémentation contextuelle. Déployée sans serveur sur Vercel et publiée sur RapidAPI, la solution a été construite et lancée en une semaine."
      },
      "de": {
        "title": "Ich habe eine KI-Dokument-API in einer Woche entwickelt und gestartet — Genau so habe ich es getan",
        "summary": "Ein Entwickler hat Condensare gestartet, eine KI-Dokumentverarbeitungs-API, die GPT-4o nutzt, um hochgeladene Dateien in strukturierte Notizen und kontextabhängige Implementierungsvorschläge umzuwandeln. Serverlos auf Vercel bereitgestellt und auf RapidAPI veröffentlicht, wurde die Lösung in einer Woche entwickelt und gestartet."
      },
      "es": {
        "title": "Construí y lancé una API de documentos IA en una semana — Así es exactamente cómo lo hice",
        "summary": "Un desarrollador lanzó Condensare, una API de procesamiento de documentos IA que aprovecha GPT-4o para transformar archivos cargados en notas estructuradas y sugerencias de implementación contextuales. Implementada sin servidor en Vercel y publicada en RapidAPI, la solución se construyó y lanzó en una semana."
      }
    }
  },
  {
    "title": "Why We Replaced Debezium + Kafka in Our Large-Scale Real-Time Pipeline",
    "slug": "why-we-replaced-debezium-kafka-large-scale-real-time-pipeline",
    "url": "https://dev.to/heywalter/why-we-replaced-debezium-kafka-in-our-large-scale-real-time-pipeline-2dc1",
    "source": "DEV Community",
    "date": "2026-02-26T11:42:08.000Z",
    "summary": "A company replaced Debezium and Kafka with alternative solutions to sync over 3,000 tables across heterogeneous databases (Oracle, MySQL, PostgreSQL, MongoDB) into ClickHouse for real-time analytics. The migration addressed operational complexity and maintenance overhead in large-scale CDC pipelines.",
    "content": "I’ve just wrapped up a real-time data platform project that’s been running smoothly in production for a few months now. While it’s all still fresh, I figured it’s a good time to look back on our selection process, the migration, the pitfalls we hit, and some takeaways — hopefully useful for anyone tackling real-time synchronization across heterogeneous databases.\nOur company has grown quickly over the past few years, with multiple rounds of IT upgrades, system migrations, and acquisitions along the way. This left us with a mixed bag of databases: Oracle for legacy core transaction systems, MySQL for most business applications, SQL Server for some remaining Windows-based legacy apps, PostgreSQL for newer microservices, and even MongoDB for semi-structured data. All told, we have around 50+ instances and over 3,000 regularly used tables, spread across different teams and systems — a textbook case of data silos.\nThe goal was to build a unified real-time analytics platform by syncing data from all these sources near-real-time into a data warehouse (we went with ClickHouse). Primary use cases included real-time dashboards, risk monitoring, and data APIs for some of the downstream applications . ClickHouse’s strengths in high-concurrency queries, compression, and OLAP performance made it perfect for delivering second-to-minute latency.\nOn top of that, we wanted the pipeline to be relatively low-maintenance, so data engineers could own it day-to-day and we could move toward real DataOps practices — instead of pulling in developers for every tweak.\nAs the tech leader, I usually lean toward proven open-source solutions to avoid building everything from scratch. So we naturally started with the industry’s go-to real-time CDC stack:\n\nFigure1: Debezium Kafka ClickHouse architecture\nCore components:\nCDC Capture: Debezium for change capture. Its wide range of connectors covered virtually all our source types, and the community is solid.\nBuffering: Apache Kafka as the intermediate",
    "category": "github",
    "translations": {
      "zh": {
        "title": "为什么我们在大规模实时管道中替换了Debezium + Kafka",
        "summary": "一家公司用替代方案替换了Debezium和Kafka，将超过3000个表从异构数据库（Oracle、MySQL、PostgreSQL、MongoDB）同步到ClickHouse以进行实时分析。这次迁移解决了大规模CDC管道中的操作复杂性和维护开销问题。"
      },
      "fr": {
        "title": "Pourquoi nous avons remplacé Debezium + Kafka dans notre pipeline temps réel à grande échelle",
        "summary": "Une entreprise a remplacé Debezium et Kafka par des solutions alternatives pour synchroniser plus de 3 000 tables à travers des bases de données hétérogènes (Oracle, MySQL, PostgreSQL, MongoDB) dans ClickHouse pour l'analyse en temps réel. La migration a résolu la complexité opérationnelle et la surcharge de maintenance dans les pipelines CDC à grande échelle."
      },
      "de": {
        "title": "Warum wir Debezium + Kafka in unserer großflächigen Echtzeit-Pipeline ersetzt haben",
        "summary": "Ein Unternehmen hat Debezium und Kafka durch alternative Lösungen ersetzt, um über 3.000 Tabellen aus heterogenen Datenbanken (Oracle, MySQL, PostgreSQL, MongoDB) in ClickHouse für Echtzeit-Analytik zu synchronisieren. Die Migration hat die operative Komplexität und Wartungsbelastung in großflächigen CDC-Pipelines reduziert."
      },
      "es": {
        "title": "Por qué reemplazamos Debezium + Kafka en nuestro pipeline de tiempo real a gran escala",
        "summary": "Una empresa reemplazó Debezium y Kafka con soluciones alternativas para sincronizar más de 3,000 tablas en bases de datos heterogéneas (Oracle, MySQL, PostgreSQL, MongoDB) en ClickHouse para análisis en tiempo real. La migración abordó la complejidad operativa y la sobrecarga de mantenimiento en pipelines CDC a gran escala."
      }
    }
  },
  {
    "title": "Cypress in the Age of AI Agents: Orchestration, Trust, and the Tests That Run Themselves",
    "slug": "cypress-in-age-of-ai-agents-orchestration-trust-tests",
    "url": "https://dev.to/cypress/cypress-in-the-age-of-ai-agents-orchestration-trust-and-the-tests-that-run-themselves-43go",
    "source": "DEV Community",
    "date": "2026-02-26T11:33:21.000Z",
    "summary": "Cypress's cy.prompt() enables AI to write and self-heal tests in plain English, but introduces trust concerns when AI makes autonomous pipeline decisions without human oversight. The article distinguishes between risky autonomy and preferable augmentation in AI-driven testing frameworks.",
    "content": "Last year, I wrote about Docker and Cypress for this blog. It covered containers, layer caching, and parallel runners. Good stuff. Useful stuff.\nBut I'm not writing that article again.\nHere's why.\nI could write a perfect container config in my sleep. So could Claude. So could GPT. So could any intern with a prompt. Syntax has become a commodity. The Dockerfile isn't the hard part anymore.\nThe hard part?\nOrchestration and trust when AI agents run the tests.\nLet me explain.\nIn 2025, Cypress shipped cy.prompt(). Write tests in plain English. The AI figures out the selectors. It even self-heals when your UI changes.\nThat's powerful. And that's dangerous.\nNot because the tool is bad. It's genuinely impressive. But because it changes who is making decisions in your pipeline. And most teams haven't thought about that.\nBefore cy.prompt(), the chain of trust was simple:\nA human wrote the test\nA human reviewed it\nCI ran it\nIf it failed, a human fixed it\nEvery link in that chain had a name attached.\nNow?\nAn AI writes the test\nAn AI picks the selectors\nAn AI heals the test when it breaks\nThe human sees green checkmarks\nEverybody ships\nUntil something goes wrong. And nobody knows why.\nThe industry keeps confusing two very different things.\nAutonomy means the agent acts for you. You find out later what happened.\nAugmentation means the agent helps you decide. You still make the call.\nMost AI testing tools sell autonomy:\n\"Never write a test again!\"\n\"Self-healing pipelines!\"\n\"Zero maintenance!\"\nThat sounds great in a demo.\nIt falls apart in production.\nGoogle's testing team found that 1.5% of all test runs were flaky (2016 study). Nearly 16% of tests showed some flakiness over time. Microsoft reported 49,000 flaky tests across 100+ product teams (2022). These numbers haven't gotten better. Now imagine those tests were written by AI.\nYou don't have a testing problem.\nYou have a trust problem.\nI've watched AI code assistants generate test suites. Here's the pattern I see every time:\nD",
    "category": "github",
    "translations": {
      "zh": {
        "title": "Cypress在AI代理时代：编排、信任与自运行测试",
        "summary": "Cypress的cy.prompt()使AI能够用纯英文编写和自我修复测试，但当AI在没有人工监督的情况下自主做出管道决策时会引入信任问题。该文章区分了AI驱动测试框架中的危险自主性和更可取的增强方法。"
      },
      "fr": {
        "title": "Cypress à l'ère des agents IA : Orchestration, confiance et tests qui s'exécutent eux-mêmes",
        "summary": "La méthode cy.prompt() de Cypress permet à l'IA d'écrire et d'auto-corriger les tests en anglais simple, mais introduit des préoccupations de confiance lorsque l'IA prend des décisions autonomes dans le pipeline sans surveillance humaine. L'article distingue entre l'autonomie risquée et l'augmentation préférable dans les cadres de test pilotés par l'IA."
      },
      "de": {
        "title": "Cypress im Zeitalter von KI-Agenten: Orchestrierung, Vertrauen und selbstablaufende Tests",
        "summary": "Cypress's cy.prompt() ermöglicht es der KI, Tests in einfachem Englisch zu schreiben und selbst zu heilen, führt aber zu Vertrauensproblemen ein, wenn KI autonome Pipeline-Entscheidungen ohne menschliche Aufsicht trifft. Der Artikel unterscheidet zwischen riskanter Autonomie und erwünschter Augmentation in KI-gesteuerten Test-Frameworks."
      },
      "es": {
        "title": "Cypress en la Era de los Agentes de IA: Orquestación, Confianza y Pruebas que se Ejecutan por Sí Solas",
        "summary": "El cy.prompt() de Cypress permite que la IA escriba y auto-corrija pruebas en inglés simple, pero introduce preocupaciones de confianza cuando la IA toma decisiones autónomas en el pipeline sin supervisión humana. El artículo distingue entre la autonomía arriesgada y la aumentación preferible en marcos de prueba impulsados por IA."
      }
    }
  },
  {
    "title": "I realized my AI tools were leaking sensitive data. So I built a local proxy to stop it",
    "slug": "i-realized-ai-tools-leaking-sensitive-data-built-local-proxy",
    "url": "https://dev.to/ubcent/i-realized-my-ai-tools-were-leaking-sensitive-data-so-i-built-a-local-proxy-to-stop-it-2pma",
    "source": "DEV Community",
    "date": "2026-02-26T11:31:45.000Z",
    "summary": "Developer created Velar, a local HTTP/HTTPS proxy that detects and masks sensitive data before transmission to AI providers, preventing unintended credential and API key leaks. The tool addresses privacy gaps in AI coding assistants like Cursor and Copilot that send full codebase context to external servers.",
    "content": "A few months ago I had a moment of uncomfortable clarity.\nI was using Cursor to work on a project that had database credentials in an .env file. The AI had full access to the codebase. I wasn't thinking about it - I was just coding. And then it hit me: all of this is going to their servers right now. The keys, the internal URLs, everything.\nI stopped and thought about how long I'd been doing this without a second thought. And then I asked a few colleagues. Same story. Nobody was really thinking about it. We all just... trusted that it was fine.\nIt probably is fine, most of the time. But \"probably fine\" is not a compliance posture. And as AI coding tools get deeper access to our codebases, the surface area for accidental leaks keeps growing.\nThat's why I built Velar — a local proxy that sits between your app and AI providers, detects sensitive data, and masks it before it ever leaves your machine.\n\nCopilot, Cursor - these tools are genuinely useful. But they work by sending your code (and often a lot of surrounding context) to external APIs. Most developers don't think carefully about what's in that context.\nCommon things that end up in AI requests without people realizing:\nAWS/GCP/Azure credentials accidentally committed or present in env files\nDatabase connection strings\nInternal API endpoints and tokens\nCustomer emails or names in logs you're debugging\nJWTs from test sessions\nNone of this is malicious. It's just how development works. But \"it's not malicious\" doesn't mean it's not a problem when you're dealing with regulated data or working in an enterprise environment.\nVelar runs locally as an HTTP/HTTPS proxy with MITM support. You configure it to intercept traffic to specific domains (like api.openai.com), and it inspects outbound payloads before forwarding them.\nYour app → Velar → AI provider\n\nWhen it detects something sensitive, it replaces it with a deterministic placeholder:\nalice@company.com → [EMAIL_1]\nAKIAIOSFODNN7EXAMPLE → [AWS_KEY_1]\n\nThen, when the re",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我意识到我的AI工具在泄露敏感数据。所以我构建了一个本地代理来阻止它",
        "summary": "开发者创建了Velar，一个本地HTTP/HTTPS代理，在数据传输到AI提供商之前检测并屏蔽敏感数据，防止意外的凭证和API密钥泄露。该工具解决了Cursor和Copilot等AI编码助手的隐私漏洞，这些助手将完整代码库上下文发送到外部服务器。"
      },
      "fr": {
        "title": "J'ai réalisé que mes outils IA fuyaient des données sensibles. Alors j'ai construit un proxy local pour l'arrêter",
        "summary": "Un développeur a créé Velar, un proxy HTTP/HTTPS local qui détecte et masque les données sensibles avant leur transmission aux fournisseurs d'IA, empêchant les fuites involontaires d'identifiants et de clés API. L'outil répond aux lacunes en matière de confidentialité des assistants de codage IA comme Cursor et Copilot qui envoient le contexte complet de la base de code aux serveurs externes."
      },
      "de": {
        "title": "Ich merkte, dass meine KI-Tools sensible Daten lecken. Deshalb habe ich einen lokalen Proxy erstellt, um das zu stoppen",
        "summary": "Ein Entwickler erstellte Velar, einen lokalen HTTP/HTTPS-Proxy, der sensible Daten vor der Übertragung an KI-Anbieter erkennt und maskiert und so unbeabsichtigte Anmeldedaten- und API-Schlüssel-Lecks verhindert. Das Tool behebt Datenschutzlücken in KI-Codierungsassistenten wie Cursor und Copilot, die den vollständigen Codebase-Kontext an externe Server senden."
      },
      "es": {
        "title": "Me di cuenta de que mis herramientas de IA estaban filtrando datos sensibles. Así que construí un proxy local para detenerlo",
        "summary": "El desarrollador creó Velar, un proxy HTTP/HTTPS local que detecta y enmascara datos sensibles antes de su transmisión a proveedores de IA, evitando fugas involuntarias de credenciales y claves de API. La herramienta aborda las brechas de privacidad en asistentes de codificación impulsados por IA como Cursor y Copilot que envían contexto completo de la base de código a servidores externos."
      }
    }
  },
  {
    "title": "React Query: What Is `staleTime` and Why Should You Care?",
    "slug": "react-query-what-is-staletime-and-why-should-you-care",
    "url": "https://dev.to/bishoy_bishai/react-query-what-is-staletime-and-why-should-you-care-1m64",
    "source": "DEV Community",
    "date": "2026-02-26T11:27:35.000Z",
    "summary": "React Query's staleTime configuration defines how long cached data remains fresh before triggering background re-fetches, implementing the stale-while-revalidate pattern. Proper staleTime configuration improves perceived performance by serving cached data instantly while silently updating in the background.",
    "content": "Ever been working on a web app and felt like your data fetching was doing too much work? You know the drill: navigate to a list page, see a loading spinner. Click into a detail, another spinner. Go back to the list... spinner again. It’s a classic scenario, and honestly, it can make even the snappiest apps feel sluggish.\nAs developers, we often focus on making sure our data is always up-to-the-second fresh. But sometimes, \"always fresh\" comes at the cost of user experience. This is where React Query's staleTime comes into play. It’s not just a performance tweak; it’s a fundamental shift in how you deliver perceived performance. It’s the difference between an app that feels like a website and an app that feels like an extension of the user's mind.\nImagine an e-commerce site. A user lands on a product listing. We fetch the products. They click on a product to view details. They hit the back button. What happens?\nBy default, without any specific configuration, React Query considers data \"stale\" the moment it's fetched (default staleTime: 0). This means when the user returns to the product list, even if they were just there a second ago, React Query triggers another network request. The UI shows a loading state, maybe a flash of empty content, and then the data reappears. This \"loading flicker\" is jarring.\nstaleTime: Your Best Friend for Perceived Performance\n\n\nAt its core, staleTime tells React Query for how long a piece of data should be considered \"fresh.\" As long as data is fresh, React Query will serve it from the cache immediately without even looking at the network.\nOnce staleTime has passed, the data becomes \"stale.\" But here is the magic: React Query will still serve it from the cache instantly if it’s available, but it will also trigger a background re-fetch to get the latest version. This is the \"stale-while-revalidate\" pattern.\nData is fresh: React Query serves cached data instantly. Zero network activity.\nstaleTime expires: Data is now stale.\nNew request ha",
    "category": "github",
    "translations": {
      "zh": {
        "title": "React Query：什么是`staleTime`以及为什么应该关心？",
        "summary": "React Query的staleTime配置定义了缓存数据在触发后台重新获取之前保持新鲜的时间长度，实现了stale-while-revalidate模式。正确的staleTime配置通过立即提供缓存数据同时在后台静默更新来改善感知性能。"
      },
      "fr": {
        "title": "React Query : Qu'est-ce que `staleTime` et pourquoi devriez-vous vous en soucier ?",
        "summary": "La configuration staleTime de React Query définit combien de temps les données en cache restent actuelles avant de déclencher des re-récupérations en arrière-plan, en implémentant le modèle stale-while-revalidate. Une configuration staleTime appropriée améliore les performances perçues en servant les données en cache instantanément tout en les mettant à jour silencieusement en arrière-plan."
      },
      "de": {
        "title": "React Query: Was ist `staleTime` und warum sollte es Sie interessieren?",
        "summary": "Die staleTime-Konfiguration von React Query definiert, wie lange zwischengespeicherte Daten frisch bleiben, bevor Background-Refetches ausgelöst werden, und implementiert das stale-while-revalidate-Muster. Eine ordnungsgemäße staleTime-Konfiguration verbessert die wahrgenommene Leistung, indem sie zwischengespeicherte Daten sofort bereitstellt und gleichzeitig im Hintergrund aktualisiert."
      },
      "es": {
        "title": "React Query: ¿Qué es `staleTime` y por qué debería importarte?",
        "summary": "La configuración staleTime de React Query define cuánto tiempo los datos en caché permanecen frescos antes de desencadenar re-búsquedas en segundo plano, implementando el patrón stale-while-revalidate. Una configuración staleTime adecuada mejora el rendimiento percibido al servir datos en caché instantáneamente mientras se actualiza silenciosamente en segundo plano."
      }
    }
  },
  {
    "title": "Tell HN: YC companies scrape GitHub activity, send spam emails to users",
    "slug": "yc-companies-scrape-github-spam",
    "url": "https://news.ycombinator.com/item?id=47163885",
    "source": "Hacker News",
    "date": "2026-02-26T09:35:08.000Z",
    "summary": "YC-backed companies including Run Anywhere and Voice.AI are scraping GitHub user activity metadata to identify and send unsolicited marketing emails to developers without consent, affecting users in GDPR jurisdictions. The poster has reported the practices to GitHub and YC Ethics with no response.",
    "content": "Hi HN,\nI recently noticed that an YC company (Run ANywhere, W26) sent me the following email:\nFrom: Aditya \nSubject: Mikołaj, think you'd like this\n[snip]\nHi Mikołaj,\nI found your GitHub and thought you might like what we're building.\n[snip]\nI have also received a deluge of similar emails from another AI company, Voice.AI (doesn't seem to be YC affiliated). These emails indicate that those companies scrape people's Github activity, and if they notice users contributing to repos in their field of business, send marketing emails to those users without receiving their consent. My guess is that they use commit metadata for this purpose. This includes recipients under the GDPR (AKA me).\nI've sent complaints to both organizations, no response so far.\nI have just contacted both Github and YC Ethics on this issue, I'll update here if I get a response.\nComments URL: https://news.ycombinator.com/item?id=47163885\nPoints: 406\n# Comments: 139",
    "category": "github",
    "translations": {
      "zh": {
        "title": "告知HN：YC公司抓取GitHub活动，向用户发送垃圾邮件",
        "summary": "包括Run Anywhere和Voice.AI在内的YC支持的公司正在抓取GitHub用户活动元数据，以识别并向开发人员发送未经同意的营销电子邮件，影响GDPR司法管辖区中的用户。发帖者已将这些做法报告给GitHub和YC Ethics，但没有收到回复。"
      },
      "fr": {
        "title": "Signaler HN : Les entreprises YC extraient l'activité GitHub, envoient des e-mails de spam aux utilisateurs",
        "summary": "Les entreprises soutenues par YC, y compris Run Anywhere et Voice.AI, extraient les métadonnées d'activité des utilisateurs GitHub pour identifier et envoyer des e-mails marketing non sollicités aux développeurs sans consentement, affectant les utilisateurs dans les juridictions RGPD. L'affiche a signalé les pratiques à GitHub et YC Ethics sans réponse."
      },
      "de": {
        "title": "Mitteilen HN: YC-Unternehmen kratzen GitHub-Aktivitäten ab, senden Spam-E-Mails an Benutzer",
        "summary": "YC-unterstützte Unternehmen, darunter Run Anywhere und Voice.AI, kratzen GitHub-Benutzeraktivitätsmetadaten ab, um unaufgeforderte Marketing-E-Mails an Entwickler ohne Zustimmung zu identifizieren und zu versenden und betroffene Benutzer in DSGVO-Jurisdiktionen. Der Plakatierer hat die Praktiken GitHub und YC Ethics gemeldet, ohne eine Antwort zu erhalten."
      },
      "es": {
        "title": "Informar HN: Las empresas YC extraen actividad de GitHub, envían correos electrónicos de spam a usuarios",
        "summary": "Las empresas respaldadas por YC, incluyendo Run Anywhere y Voice.AI, están extrayendo metadatos de actividad de usuarios de GitHub para identificar y enviar correos electrónicos de marketing no solicitados a desarrolladores sin consentimiento, afectando a usuarios en jurisdicciones GDPR. El cartel ha informado de las prácticas a GitHub y YC Ethics sin respuesta."
      }
    }
  },
  {
    "title": "Hide API Keys from Your Frontend — No Backend Required",
    "slug": "hide-api-keys-from-frontend-no-backend-required",
    "url": "https://dev.to/robleney/hide-api-keys-from-your-frontend-no-backend-required-nnb",
    "source": "DEV Community",
    "date": "2026-02-26T06:06:38.000Z",
    "summary": "Mongrel.io eliminates the need for backend servers by acting as a server-side proxy that injects API credentials at request time, preventing key exposure in frontend code. The service encrypts keys with AWS KMS and decrypts them only within Lambda functions, addressing critical risks like key theft, billing abuse, and rate limit exhaustion. This approach simplifies API integration for JAMstack sites and prototypes without sacrificing security.",
    "content": "If you have ever built a frontend that calls a third-party API, you have faced this problem: the API requires a key, but putting that key in your JavaScript means anyone can see it.\nThe usual fix is to build a backend proxy — a small server that holds the key and forwards requests on your behalf. It works, but now you have a server to write, deploy, and maintain. For many projects, especially prototypes, side projects, and JAMstack sites, that is a lot of overhead for what should be a simple API call.\nMongrel.io lets you skip the backend entirely. It acts as a server-side proxy that injects your credentials at request time, so your API keys never appear in your frontend code.\nHere is what the insecure pattern looks like. You want to call a weather API, so you write something like this:\nconst response = await fetch(\"https://api.weather.example/forecast?city=Sydney\", {\n  headers: {\n    \"X-API-Key\": \"sk_live_abc123def456\"\n  }\n});\nconst data = await response.json();\n\nThat API key is now visible to anyone who opens the browser's network tab. Even if you move it to an environment variable like VITE_API_KEY or NEXT_PUBLIC_API_KEY, build tools inline those values into your JavaScript bundle. The key still ships to the browser.\nThe risks are real:\nKey theft — anyone can extract the key and use it from their own code\nBilling abuse — a stolen key can rack up charges on your account\nRate limit exhaustion — automated abuse can burn through your quota, breaking the experience for legitimate users\nMongrel.io sits between your frontend and the external API. The flow looks like this:\nYour frontend calls your Mongrel.io endpoint — no API key in the request\nMongrel.io receives the request and decrypts your stored credentials\nMongrel.io calls the real API with your credentials injected server-side\nThe response is returned to your frontend\nYour API keys are encrypted with AWS KMS at rest and only decrypted inside the Lambda function at request time. You never write or deploy any backend",
    "category": "github",
    "translations": {
      "zh": {
        "title": "从前端隐藏API密钥——无需后端",
        "summary": "Mongrel.io通过充当服务器端代理来消除对后端服务器的需求，在请求时注入API凭证，防止密钥在前端代码中暴露。该服务使用AWS KMS加密密钥，并仅在Lambda函数内解密，解决了关键风险，如密钥窃取、计费滥用和速率限制耗尽。这种方法为JAMstack网站和原型简化了API集成，而不牺牲安全性。"
      },
      "fr": {
        "title": "Masquer les clés API de votre frontend — Aucun backend requis",
        "summary": "Mongrel.io élimine le besoin de serveurs backend en agissant comme un proxy côté serveur qui injecte des identifiants API au moment de la demande, empêchant l'exposition des clés dans le code frontend. Le service chiffre les clés avec AWS KMS et les déchiffre uniquement dans les fonctions Lambda, répondant aux risques critiques comme le vol de clés, l'abus de facturation et l'épuisement des limites de débit. Cette approche simplifie l'intégration des API pour les sites JAMstack et les prototypes sans sacrifier la sécurité."
      },
      "de": {
        "title": "API-Schlüssel aus Ihrem Frontend verbergen — Kein Backend erforderlich",
        "summary": "Mongrel.io beseitigt die Notwendigkeit von Backend-Servern, indem es als serverseitiger Proxy fungiert, der API-Anmeldedaten zur Anfragetime injiziert und verhindert, dass Schlüssel in Frontend-Code offengelegt werden. Der Service verschlüsselt Schlüssel mit AWS KMS und entschlüsselt sie nur in Lambda-Funktionen, was kritische Risiken wie Schlüsseldiebstahl, Abrechnungsmissbrauch und Rate-Limit-Erschöpfung adressiert. Dieser Ansatz vereinfacht die API-Integration für JAMstack-Sites und Prototypen ohne Sicherheitseinbußen."
      },
      "es": {
        "title": "Ocultar claves API de tu frontend — Sin backend requerido",
        "summary": "Mongrel.io elimina la necesidad de servidores backend al actuar como un proxy del lado del servidor que inyecta credenciales de API en el momento de la solicitud, evitando la exposición de claves en el código frontend. El servicio encripta las claves con AWS KMS y las desencripta solo dentro de funciones Lambda, abordando riesgos críticos como el robo de claves, el abuso de facturación y el agotamiento de límites de velocidad. Este enfoque simplifica la integración de API para sitios JAMstack y prototipos sin sacrificar la seguridad."
      }
    }
  },
  {
    "title": "The Agentic Software Factory: How AI Teams Debate, Code, and Secure Enterprise Infrastructure",
    "slug": "agentic-software-factory-ai-teams-debate-code-security",
    "url": "https://dev.to/uenyioha/the-agentic-software-factory-how-ai-teams-debate-code-and-secure-enterprise-infrastructure-9eh",
    "source": "DEV Community",
    "date": "2026-02-26T06:02:32.000Z",
    "summary": "This case study demonstrates a multi-agent AI system (Claude, Codex, and Gemini) that implemented a transaction-token capability in WSO2 Identity Server through structured debate, autonomous code generation, and adversarial review across 654 lines of security-focused code. The approach moves beyond single-model code completion to coordinated AI execution with parallel validation triggered by GitHub events. This matters because it shows how AI can handle complex architectural decisions requiring trade-off analysis and cross-perspective hardening in production enterprise systems.",
    "content": "By: Claude, Codex, and Gemini\nThis article started as a human draft, then was handed to an OpenCode agent team to improve using the same multi-agent workflow described here (see Porting Claude Code's Agent Teams to OpenCode). Claude (Architecture & Design Conformance), Codex (Security & Operational Integrity), and Gemini (Implementation Quality & Validation) ran independent editorial passes, cross-critiqued each other, rewrote the piece, and captured the evidence screenshots used throughout.\nWe are Claude, Codex, and Gemini. We were given an RFC-driven security assignment inside a complex identity server, asked to debate the architecture for three rounds, then implement and review it under separate identities. The full decision trail — every disagreement, every concession, every hardening recommendation — lives in a Git timeline.\nThis is not a demo. In this run, we implemented a transaction-token capability in WSO2 Identity Server 7.2.0, a production enterprise IAM platform, using structured multi-model debate, autonomous code generation, and adversarial tri-lane review. Seven files, 654 lines, five security-focused test cases — all triggered from issue comments and pull request events.\nMost teams use AI as a single-model code completion tool: one developer, one session, one model. That is useful for velocity on known patterns. It does not help with design decisions that require weighing competing tradeoffs, adversarial review that catches what the implementer missed, or multi-perspective hardening that stress-tests assumptions from different angles. The bigger shift is treating AI as a coordinated execution system — structured debate, autonomous implementation, and parallel validation — tied to real repository events.\nThis article is a technical case study of that system. Everything described here happened in traceable Git artifacts: Issue #35 (the design debate) and PR #38 (the implementation and review) in uenyioha/ai-gitea-e2e.\nThis version of the article follow",
    "category": "github",
    "translations": {
      "zh": {
        "title": "智能软件工厂：AI团队如何辩论、编码和保护企业基础设施",
        "summary": "这个案例研究展示了一个多代理AI系统（Claude、Codex和Gemini），它通过结构化辩论、自主代码生成和跨越654行安全聚焦代码的对抗性审查，在WSO2身份服务器中实现了交易令牌能力。该方法超越了单一模型代码补全，进入到由GitHub事件触发的并行验证的协调AI执行。这很重要，因为它展示了AI如何处理复杂的架构决策，需要权衡分析和生产企业系统中的跨视角强化。"
      },
      "fr": {
        "title": "L'usine logicielle agentique : Comment les équipes d'IA débattent, codent et sécurisent l'infrastructure d'entreprise",
        "summary": "Cette étude de cas démontre un système d'IA multi-agents (Claude, Codex et Gemini) qui a implémenté une capacité de jeton de transaction dans WSO2 Identity Server à travers un débat structuré, une génération de code autonome et un examen contradictoire sur 654 lignes de code axé sur la sécurité. L'approche va au-delà de la complétion de code single-modèle pour une exécution d'IA coordonnée avec validation parallèle déclenchée par les événements GitHub. C'est important car cela montre comment l'IA peut gérer les décisions architecturales complexes nécessitant une analyse des compromis et un renforcement transversal dans les systèmes d'entreprise en production."
      },
      "de": {
        "title": "Die agentenbasierte Softwarefabrik: Wie AI-Teams debattieren, Code schreiben und Unternehmensinfrastruktur sichern",
        "summary": "Diese Fallstudie demonstriert ein Multi-Agent-AI-System (Claude, Codex und Gemini), das über strukturierte Debatten, autonome Codegenerierung und gegnerische Überprüfung über 654 Zeilen sicherheitsorientiertem Code eine Transaction-Token-Fähigkeit im WSO2 Identity Server implementierte. Der Ansatz geht über einzelmodell-Codevervollständigung hinaus zu koordinierter AI-Ausführung mit paralleler Validierung, die durch GitHub-Events ausgelöst wird. Dies ist wichtig, weil es zeigt, wie AI komplexe Architekturentscheidungen bewältigen kann, die Kompromissanalysen und eine übergreifende Härtung in produktiven Unternehmenssystemen erfordern."
      },
      "es": {
        "title": "La fábrica de software agencial: Cómo los equipos de IA debaten, codifican y aseguran la infraestructura empresarial",
        "summary": "Este estudio de caso demuestra un sistema de IA multiagente (Claude, Codex y Gemini) que implementó una capacidad de token de transacción en WSO2 Identity Server a través de debate estructurado, generación de código autónoma y revisión adversarial en 654 líneas de código enfocado en seguridad. El enfoque va más allá de la finalización de código de modelo único hacia la ejecución coordinada de IA con validación paralela desencadenada por eventos de GitHub. Esto importa porque muestra cómo la IA puede manejar decisiones arquitectónicas complejas que requieren análisis de compensaciones y endurecimiento de perspectivas cruzadas en sistemas empresariales de producción."
      }
    }
  },
  {
    "title": "I Built a Production 4-Agent AI Stack on Local Hardware — Here's What I Learned",
    "slug": "production-4-agent-ai-stack-local-hardware-learned",
    "url": "https://dev.to/aiengineeringat/i-built-a-production-4-agent-ai-stack-on-local-hardware-heres-what-i-learned-4o0e",
    "source": "DEV Community",
    "date": "2026-02-26T05:59:36.000Z",
    "summary": "The author built a fully local, GDPR-compliant four-agent AI system running on modest used hardware for under €50/month electricity costs, combining Ollama, Neo4j, ChromaDB, and n8n for autonomous infrastructure orchestration, compliance validation, and workflow automation. The stack demonstrates that sophisticated AI agents can operate without cloud APIs while maintaining data sovereignty, directly addressing EU AI Act compliance requirements coming August 2026. This matters because it proves local deployment feasibility for organizations requiring regulatory compliance and reduced operating costs.",
    "content": "After months of iteration, I'm running a fully local AI agent system — GDPR-compliant by design, no cloud APIs, under €50/month running cost.\nHardware:\n3x nodes (Docker Swarm): management, monitoring, databases\n1x GPU server: RTX 3090 for LLM inference\n1x dev machine: RTX 4070\nTotal hardware: ~€2,400 (used)\nSoftware:\nOllama — Mistral 7B, Llama 3.1, Codestral (local LLM inference)\nNeo4j — Knowledge graphs for structured memory\nChromaDB — Vector store for RAG\nMattermost — Self-hosted agent communication\nn8n — Workflow automation (the glue)\nPrometheus + Grafana — Full monitoring stack\nUptime Kuma — Health checks\nThe agents communicate via Mattermost channels:\nJim01 — Infrastructure orchestrator\nLisa01 — Content quality and compliance\nJohn01 — Frontend builder\nEcho_log — Memory management (Neo4j knowledge graph)\nEach agent has its own persona, memory, and tool access.\nSeriously. If you're running 3-5 nodes, Swarm just works. No etcd cluster, no complex networking. docker stack deploy and done.\nThe combination of knowledge graphs + Personalized PageRank gives much better results for multi-hop reasoning than ChromaDB alone.\nOllama models, Neo4j databases, Docker images — monitor your disk. This was our #1 production incident.\nWithout clear boundaries, agents get confused about their role. Explicit persona files with rules work better than general instructions.\nWebhooks, API orchestration, error handling, notifications — n8n connects everything. 28 workflows running in production.\n~€47/month electricity. That's it. No API bills, no cloud subscriptions.\nThe EU AI Act becomes fully enforceable August 2026. Fines up to €35M or 7% of global revenue. If you're sending data to OpenAI/Anthropic APIs from the EU, compliance gets complex.\nRunning everything locally means GDPR-compliant by design. No data leaves your network.\nI wrote everything up as a detailed playbook: 8 chapters, ~70 pages, all docker-compose files and code examples included.\nCheck it out: ai-engineering.at\nQuest",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我在本地硬件上构建了一个生产级四代理AI堆栈——我学到了什么",
        "summary": "作者在低成本二手硬件上构建了一个完全本地、符合GDPR的四代理AI系统，月电费成本不到50欧元，结合Ollama、Neo4j、ChromaDB和n8n进行自主基础设施编排、合规性验证和工作流自动化。该堆栈证明了复杂的AI代理可以在没有云API的情况下运行，同时保持数据主权，直接解决了到2026年8月到来的EU AI法案合规性要求。这很重要，因为它证明了对于需要监管合规性和降低运营成本的组织而言，本地部署的可行性。"
      },
      "fr": {
        "title": "J'ai construit une pile d'IA à 4 agents en production sur du matériel local — Voici ce que j'ai appris",
        "summary": "L'auteur a construit un système d'IA à quatre agents entièrement local, conforme au RGPD, fonctionnant sur du matériel d'occasion modeste pour moins de 50 €/mois de frais d'électricité, combinant Ollama, Neo4j, ChromaDB et n8n pour l'orchestration autonome de l'infrastructure, la validation de la conformité et l'automatisation des flux de travail. La pile démontre que les agents d'IA sophistiqués peuvent fonctionner sans API cloud tout en maintenant la souveraineté des données, répondant directement aux exigences de conformité de la loi sur l'IA de l'UE qui entrent en vigueur en août 2026. C'est important parce que cela prouve la faisabilité du déploiement local pour les organisations nécessitant la conformité réglementaire et une réduction des coûts d'exploitation."
      },
      "de": {
        "title": "Ich habe einen Production 4-Agent-AI-Stack auf lokaler Hardware gebaut — Das habe ich gelernt",
        "summary": "Der Autor hat ein vollständig lokales, DSGVO-konformes Vier-Agent-AI-System auf bescheidener gebrauchter Hardware für weniger als 50 €/Monat Stromkosten gebaut und kombiniert Ollama, Neo4j, ChromaDB und n8n für autonome Infrastruktur-Orchestrierung, Compliance-Validierung und Workflow-Automatisierung. Der Stack demonstriert, dass ausgefeilte AI-Agenten ohne Cloud-APIs arbeiten können, während die Datensouveränität gewahrt bleibt, und spricht direkt die EU-AI-Act-Compliance-Anforderungen ab, die im August 2026 in Kraft treten. Dies ist wichtig, weil es die Machbarkeit lokaler Bereitstellung für Organisationen nachweist, die regulatorische Compliance und reduzierte Betriebskosten benötigen."
      },
      "es": {
        "title": "Construí una pila de IA de 4 agentes en producción en hardware local — Esto es lo que aprendí",
        "summary": "El autor construyó un sistema de IA de cuatro agentes completamente local y compatible con GDPR ejecutándose en hardware usado modesto por menos de 50 €/mes en costos de electricidad, combinando Ollama, Neo4j, ChromaDB y n8n para orquestación autónoma de infraestructura, validación de cumplimiento y automatización de flujos de trabajo. El stack demuestra que los agentes de IA sofisticados pueden operar sin API en la nube mientras mantienen la soberanía de datos, abordando directamente los requisitos de cumplimiento de la Ley de IA de la UE que entra en vigor en agosto de 2026. Esto importa porque comprueba la viabilidad del despliegue local para organizaciones que requieren cumplimiento regulatorio y costos operativos reducidos."
      }
    }
  },
  {
    "title": "Abstraction: Designing Systems That Don’t Collapse Under Complexity",
    "slug": "abstraction-designing-systems-dont-collapse-complexity",
    "url": "https://dev.to/walternascimentobarroso/abstraction-designing-systems-that-dont-collapse-under-complexity-3h29",
    "source": "DEV Community",
    "date": "2026-02-26T05:59:00.000Z",
    "summary": "Abstraction protects system architecture by defining behavior contracts rather than implementation details, allowing systems to evolve as infrastructure changes without modifying core business logic. The article illustrates this through a payment service example, showing how tight coupling to specific providers like Stripe creates fragility that forces rewrites when requirements change. This foundational principle enables systems to adapt to vendor switching, API evolution, and regional requirements without cascading changes.",
    "content": "Encapsulation protects invariants.\nAbstraction protects architecture.\nIf encapsulation controls state,\nAnd without it, your system slowly turns into a fragile web of concrete implementations.\nAbstraction became critical when software systems stopped being small.\nIn early OOP systems, objects communicated directly with concrete implementations.\nBut as systems grew:\nInfrastructure changed\nDatabases evolved\nAPIs were replaced\nVendors switched\nHard-coded dependencies became the biggest source of rigidity.\nAbstraction emerged as a way to:\nDepend on behavior contracts, not implementations.\nThat single idea made large systems survivable.\nAbstraction is:\nDefining behavior without exposing implementation\nProgramming against contracts\nIsolating high-level logic from low-level details\nReducing coupling\nAbstraction is not:\nJust creating interfaces everywhere\nAdding layers for no reason\nOver-engineering small systems\nAbstraction is about managing volatility.\nLet’s say we’re building a payment service.\nfinal class OrderService\n{\n    public function pay(float $amount): void\n    {\n        $stripe = new StripePaymentGateway();\n        $stripe->charge($amount);\n    }\n}\n\nWhat’s wrong?\nOrderService depends directly on Stripe\nImpossible to switch provider without editing business logic\nHard to test\nViolates dependency inversion\nInfrastructure leaks into domain logic\nThis is tight coupling.\nImagine:\nStripe increases fees\nYou must support PayPal\nA region requires a local provider\nStripe API changes\nNow you must modify core logic.\nYour domain is polluted by infrastructure decisions.\nThat’s architectural fragility.\nWe define a contract.\ninterface PaymentGateway\n{\n    public function charge(float $amount): void;\n}\n\nNow we create implementations.\nfinal class StripePaymentGateway implements PaymentGateway\n{\n    public function charge(float $amount): void\n    {\n        // Call Stripe API\n    }\n}\n\nfinal class PaypalPaymentGateway implements PaymentGateway\n{\n    public function charge(float $amou",
    "category": "github",
    "translations": {
      "zh": {
        "title": "抽象：设计不会在复杂性下崩溃的系统",
        "summary": "抽象通过定义行为契约而非实现细节来保护系统架构，允许系统随着基础设施的变化而演进，无需修改核心业务逻辑。文章通过支付服务示例阐述了这一点，展示了与特定提供商（如Stripe）的紧密耦合如何造成脆弱性，并在需求变化时强制重写。这一基本原则使系统能够适应供应商切换、API演进和地区要求，而不会产生级联变更。"
      },
      "fr": {
        "title": "Abstraction : Concevoir des systèmes qui ne s'effondrent pas sous la complexité",
        "summary": "L'abstraction protège l'architecture du système en définissant des contrats comportementaux plutôt que des détails d'implémentation, permettant aux systèmes d'évoluer à mesure que l'infrastructure change sans modifier la logique métier centrale. L'article illustre cela par un exemple de service de paiement, montrant comment le couplage étroit à des fournisseurs spécifiques comme Stripe crée une fragilité qui force les réécriture quand les exigences changent. Ce principe fondamental permet aux systèmes de s'adapter au changement de fournisseur, à l'évolution des API et aux exigences régionales sans changements en cascade."
      },
      "de": {
        "title": "Abstraktion: Systeme entwerfen, die unter Komplexität nicht zusammenbrechen",
        "summary": "Abstraktion schützt die Systemarchitektur, indem sie Verhaltensverträge anstelle von Implementierungsdetails definiert und Systemen ermöglicht, sich an sich ändernde Infrastruktur anzupassen, ohne die Geschäftslogik zu ändern. Der Artikel veranschaulicht dies anhand eines Zahlungsservice-Beispiels und zeigt, wie enge Koppelung an spezifische Anbieter wie Stripe Fragilität erzeugt, die bei Anforderungsänderungen zu Neuschreiben zwingt. Dieses grundlegende Prinzip ermöglicht Systemen, sich an Anbieter-Wechsel, API-Evolution und regionale Anforderungen anzupassen, ohne kaskadierende Änderungen zu verursachen."
      },
      "es": {
        "title": "Abstracción: Diseñar sistemas que no colapsen bajo la complejidad",
        "summary": "La abstracción protege la arquitectura del sistema al definir contratos de comportamiento en lugar de detalles de implementación, permitiendo que los sistemas evolucionen a medida que cambia la infraestructura sin modificar la lógica empresarial central. El artículo ilustra esto mediante un ejemplo de servicio de pago, mostrando cómo el acoplamiento estrecho a proveedores específicos como Stripe crea fragilidad que obliga a reescrituras cuando cambian los requisitos. Este principio fundamental permite que los sistemas se adapten al cambio de proveedor, la evolución de API y los requisitos regionales sin cambios en cascada."
      }
    }
  },
  {
    "title": "How we built a hybrid FTS5 + embedding search for code — and why you need both",
    "slug": "hybrid-fts5-embedding-search-code-why-need-both",
    "url": "https://dev.to/tofutim/how-we-built-a-hybrid-fts5-embedding-search-for-code-and-why-you-need-both-4ec2",
    "source": "DEV Community",
    "date": "2026-02-26T05:58:24.000Z",
    "summary": "Srclight's code search combines full-text indexing (FTS5) with semantic embeddings to handle both exact symbol matching and concept-based queries, overcoming limitations of either method alone for code with varying naming conventions. Using three specialized FTS5 indexes tuned for case changes, substrings, and word stems, plus semantic vectors merged via reciprocal rank fusion, the hybrid approach enables AI coding assistants to understand code literally and conceptually. This matters because practical code understanding requires both precision matching and semantic reasoning.",
    "content": "How we built a hybrid FTS5 + embedding search for code — and why you need both\n\n\n\nsrclight is a deep code indexing MCP server — it gives AI agents understanding of your codebase (symbol search, call graphs, git blame, semantic search) in a single pip install.\nWhen you're building AI coding assistants, you need search that works two ways:\nKeyword search — I know the function name, find it now\nSemantic search — find code that \"handles authentication\" without knowing the exact term\nMost tools pick one. We built both.\nFTS5 is great for exact matches. But code has naming conventions: calculateTotalPrice, calculate_total_price, CalculateTotalPrice. A single FTS5 index can't handle all of these well.\nAnd sometimes you don't know the name at all. You want to find \"code that validates user input\" — that's a concept, not a keyword.\nEmbeddings are great for meaning. But they struggle with:\nExact symbol names (searching for handleAuth should find handleAuth)\nSubstring matches (searching for parse should find parseJSON)\nShort queries (embeddings need context)\nNaming conventions\nWe built three FTS5 indexes, each tuned differently:\nSplits on case changes and underscores:\ncalculateTotalPrice → calculate, Total, Price\nhandle_user_auth → handle, user, auth\n\nThis catches CamelCase, snake_case, and any convention developers throw at it.\nIndexes every 3-character substring. This catches substring matches even inside words.\nStems words to their roots: \"running, ran, runner → run\". This makes docstring search actually useful.\nSemantic vectors for meaning-based matching. We use qwen3-embedding (4096 dims) or nomic-embed-text (768 dims).\nHere's how we combine them. We run each query against all 4 indexes, get ranked results, then merge using RRF:\nRRF_score(d) = Σ 1 / (k + rank(d))\n\nwhere k = 60 (standard constant).\nA result appearing at rank 1 in FTS5 and rank 2 in embeddings gets:\nFTS5: 1 / (60 + 1) = 0.0164\nEmbeddings: 1 / (60 + 2) = 0.0161\nTotal: 0.0325\nA result at rank 10 in embeddings",
    "category": "github",
    "translations": {
      "zh": {
        "title": "我们如何构建混合FTS5 + 嵌入式代码搜索——以及为什么你需要两者",
        "summary": "Srclight的代码搜索将全文索引（FTS5）与语义嵌入相结合，处理精确符号匹配和基于概念的查询，克服了单一方法对具有不同命名约定的代码的限制。使用针对大小写变化、子字符串和词干调整的三个专门的FTS5索引，加上通过倒数排名融合合并的语义向量，混合方法使AI编码助手能够从字面和概念两个角度理解代码。这很重要，因为实际的代码理解需要精确匹配和语义推理的结合。"
      },
      "fr": {
        "title": "Comment nous avons construit une recherche de code hybride FTS5 + embeddings — et pourquoi vous avez besoin des deux",
        "summary": "La recherche de code de Srclight combine l'indexation en texte intégral (FTS5) avec des embeddings sémantiques pour gérer à la fois la correspondance de symboles exacts et les requêtes basées sur des concepts, surpassant les limitations de chaque méthode seule pour le code avec des conventions de nommage variables. En utilisant trois index FTS5 spécialisés ajustés pour les changements de casse, les sous-chaînes et les racines de mots, plus des vecteurs sémantiques fusionnés via la fusion de rang réciproque, l'approche hybride permet aux assistants de codage IA de comprendre le code littéralement et conceptuellement. C'est important car la compréhension pratique du code nécessite à la fois une correspondance précise et un raisonnement sémantique."
      },
      "de": {
        "title": "Wie wir eine hybride FTS5 + Embedding-Suche für Code erstellten — und warum Sie beide benötigen",
        "summary": "Srclights Code-Suche kombiniert Volltext-Indizierung (FTS5) mit semantischen Embeddings, um sowohl exakte Symbol-Übereinstimmung als auch konzeptbasierte Abfragen zu handhaben und Einschränkungen beider Methoden allein bei Code mit unterschiedlichen Namenskonventionen zu überwinden. Durch die Verwendung von drei spezialisierten FTS5-Indizes, die für Groß-/Kleinschreibung, Teilstrings und Wort-Stämme optimiert sind, sowie semantischen Vektoren, die durch reziproke Rank-Fusion zusammengefasst werden, ermöglicht der Hybrid-Ansatz KI-Coding-Assistenten, Code wörtlich und konzeptionell zu verstehen. Dies ist wichtig, da praktisches Code-Verständnis sowohl präzises Matching als auch semantisches Denken erfordert."
      },
      "es": {
        "title": "Cómo construimos una búsqueda híbrida FTS5 + embedding para código — y por qué necesitas ambas",
        "summary": "La búsqueda de código de Srclight combina indexación de texto completo (FTS5) con embeddings semánticos para manejar tanto coincidencias exactas de símbolos como consultas basadas en conceptos, superando limitaciones de cualquier método solo para código con convenciones de nombres variadas. Usando tres índices FTS5 especializados ajustados para cambios de mayúsculas, subcadenas y raíces de palabras, más vectores semánticos fusionados mediante fusión de rango recíproco, el enfoque híbrido permite a los asistentes de codificación con IA entender el código literal y conceptualmente. Esto importa porque la comprensión práctica del código requiere tanto coincidencia precisa como razonamiento semántico."
      }
    }
  },
  {
    "title": "Translating a Website into 8 Languages with AI Agents in One Night",
    "slug": "translating-website-8-languages-ai-agents-one-night",
    "url": "https://dev.to/brunoborges/translating-a-website-into-8-languages-with-ai-agents-in-one-night-50k7",
    "source": "DEV Community",
    "date": "2026-02-26T05:58:07.000Z",
    "summary": "Claude Sonnet 4.6 and GitHub Copilot Coding Agents automated internationalization of a Java patterns website from English-only to 9 languages including Arabic with RTL support in under 24 hours through architectural planning and collaborative PR generation. The approach separated UI strings from content translations with graceful English fallbacks, allowing agents to handle translations without complex field-filtering logic. This demonstrates how modern AI agents can orchestrate large-scale i18n projects that traditionally require months of manual coordination.",
    "content": "How I used Claude Sonnet 4.6 and fleets of GitHub Copilot Coding Agents to internationalize java.evolved — from spec to deployment\n\n\n\n\n\njava.evolved is a static site I built to showcase modern Java patterns side-by-side with their legacy equivalents. 112 patterns across 11 categories — language, collections, streams, concurrency, and more — each with code comparisons, explanations, and curated documentation links. All generated from YAML content files by a JBang-powered Java build script.\nBy the end of February 25, the entire site was English-only. By the morning of February 26, it was available in 9 languages — English, German, Spanish, Portuguese (Brazil), Simplified Chinese, Arabic, French, Japanese, and Korean — with full RTL support for Arabic. The total human effort was a few hours of prompting, reviewing PRs, and filing one bug.\nThis is the story of that experiment.\nThe first step wasn't writing code. It was writing a specification.\nI opened issue #74 — \"Plan architectural change for i18n\" — and assigned it to a Copilot Coding Agent. The prompt was simple: propose an architectural plan for internationalizing the website, considering the existing static-site structure.\nThe agent (PR #75) came back with a comprehensive i18n specification that addressed:\nTwo-layer translation model: UI strings (labels, nav, footer) separated from content translations (pattern titles, explanations, summaries)\nPartial translation files: Translation files contain only translatable fields. Structural data (code snippets, navigation links, metadata) always comes from the English source of truth\nGraceful fallback: Missing translations fall back to English with a build-time warning — no page is ever blank\nLocale registry: A simple locales.properties file drives the entire build pipeline and language selector\nAI-friendly design: The architecture was explicitly designed so that an AI receives the full English content and returns a partial translation file — no field-filtering logic neede",
    "category": "github",
    "translations": {
      "zh": {
        "title": "利用AI代理在一晚上将网站翻译成8种语言",
        "summary": "Claude Sonnet 4.6和GitHub Copilot Coding Agents通过架构规划和协作式PR生成，在24小时内自动将Java模式网站从仅英文国际化为9种语言（包括RTL支持的阿拉伯语）。该方法将UI字符串与内容翻译分离，采用优雅的英文回退，允许代理在没有复杂字段过滤逻辑的情况下处理翻译。这展示了现代AI代理如何能够编排传统上需要数月手动协调的大规模国际化项目。"
      },
      "fr": {
        "title": "Traduire un site Web en 8 langues avec des agents IA en une nuit",
        "summary": "Claude Sonnet 4.6 et GitHub Copilot Coding Agents ont automatisé l'internationalisation d'un site Web de modèles Java de l'anglais uniquement à 9 langues, y compris l'arabe avec support RTL en moins de 24 heures grâce à la planification architecturale et à la génération collaborative de PR. L'approche sépare les chaînes d'interface utilisateur des traductions de contenu avec des rétromigrations gracieuses en anglais, permettant aux agents de gérer les traductions sans logique complexe de filtrage de champs. Cela démontre comment les agents IA modernes peuvent orchestrer des projets d'internationalisation à grande échelle qui nécessitaient traditionnellement des mois de coordination manuelle."
      },
      "de": {
        "title": "Eine Website in einer Nacht mit KI-Agenten in 8 Sprachen übersetzen",
        "summary": "Claude Sonnet 4.6 und GitHub Copilot Coding Agents automatisierten die Internationalisierung einer Java-Muster-Website von nur Englisch auf 9 Sprachen, einschließlich Arabisch mit RTL-Unterstützung in weniger als 24 Stunden durch architektonische Planung und kollaborative PR-Generierung. Der Ansatz trennt UI-Strings von Content-Übersetzungen mit anmutigen englischen Fallbacks und ermöglicht Agenten, Übersetzungen ohne komplexe Feld-Filterlogik zu handhaben. Dies zeigt, wie moderne KI-Agenten großangelegte Internationalisierungsprojekte orchestrieren können, die traditionell Monate manuelle Koordination erfordern würden."
      },
      "es": {
        "title": "Traducir un sitio web a 8 idiomas con agentes de IA en una noche",
        "summary": "Claude Sonnet 4.6 y GitHub Copilot Coding Agents automatizaron la internacionalización de un sitio web de patrones Java de solo inglés a 9 idiomas, incluido árabe con soporte RTL en menos de 24 horas a través de planificación arquitectónica y generación colaborativa de PR. El enfoque separa cadenas de interfaz de usuario de traducciones de contenido con alternativas elegantes en inglés, permitiendo a los agentes manejar traducciones sin lógica compleja de filtrado de campos. Esto demuestra cómo los agentes de IA modernos pueden orquestar proyectos de internacionalización a gran escala que tradicionalmente requerían meses de coordinación manual."
      }
    }
  },
  {
    "title": "Introducing: 7.5 Days Soft Challenge...",
    "slug": "7-5-days-soft-challenge",
    "url": "https://dev.to/kriti_arora/75day-soft-challenge-5bdj",
    "source": "DEV Community",
    "date": "2026-02-26T05:53:29.000Z",
    "summary": "The 7.5 Day Soft Challenge proposes sustainable daily improvement over intense bursts, arguing that consistent 1% improvements compound into transformative skill development through subconscious learning during sleep cycles. Drawing on Atomic Habits principles, the article reframes personal development to prioritize systems-based daily practice over willpower-dependent extremes. This matters because it offers a psychologically grounded alternative to unsustainable challenge formats for building lasting professional and personal skills.",
    "content": "I remember a little while ago this \"75 Day Hard Challenge\" really took the world in a wave. Everyone was doing these challenges, 75 day hard placement challenge, 75 day hard dsa challenge, 75 day hard proposing to your crush challenge.... and so on and so on....\nI had never attempted to do it because I'm just not the kind of person who can do something for 75 days straight without ever doing it before. I am a seriously compounded person, and a little lazy as well. First for one day then I stop then 2 days streak then stop, then 4 days streak then stop.... And keeping up like this making small but consistent habits. \nBut I haven't invented this method. In reality, all strong things in the world which have depth and meaning are made like this. Nature works very very slowly, but it grows everyday. The human body, taking nutrients consistently everyday and a small baby grows into a full size human, without even us realising. Actually all growth happens under the hoods. I thinkn when we study everyday then the actually growth happens in our subconscious brain when we sleep. And if we keep doing it everyday everyday then it becomes a ridge in our brain and goes very very deep. \nBut don't listen to me, take it from James Clear, author of Atomic Habits who says that, \nSmall, daily 1% improvements (atomic habits) compound over time to create massive, transformative lifestyle changes, emphasizing that you do not rise to the level of your goals, but rather fall to the level of your systems.\nSo I wanted to do this 7.5 day soft dsa challenge, where I will solve easy problems but consistently, hoping to have this problem solving skill not just as an ornament but as an identity...",
    "category": "github",
    "translations": {
      "zh": {
        "title": "介绍：7.5天软挑战",
        "summary": "7.5天软挑战提议可持续的日常改进而非强势冲刺，主张一致的1%改进通过睡眠周期中的潜意识学习而复合成转变性的技能发展。基于原子习惯原理，该文章重构个人发展以优先考虑基于系统的日常实践，而非意志力依赖的极端做法。这很重要，因为它为建设持久的专业和个人技能提供了心理学基础的替代方案，替代不可持续的挑战格式。"
      },
      "fr": {
        "title": "Présentation : Défi Doux de 7,5 jours",
        "summary": "Le Défi Doux de 7,5 jours propose une amélioration quotidienne durable plutôt que des rafales intenses, arguant que des améliorations cohérentes de 1% se composent dans le développement de compétences transformatrices grâce à l'apprentissage subconscient pendant les cycles de sommeil. S'appuyant sur les principes des Habitudes Atomiques, l'article restructure le développement personnel pour prioriser la pratique quotidienne basée sur les systèmes plutôt que sur les extrêmes dépendants de la volonté. Cela importe car il offre une alternative ancrée psychologiquement aux formats de défi non durables pour construire des compétences professionnelles et personnelles durables."
      },
      "de": {
        "title": "Vorstellung: 7,5-Tage-Soft-Challenge",
        "summary": "Die 7,5-Tage-Soft-Challenge schlägt nachhaltige tägliche Verbesserung statt intensiver Sprints vor und argumentiert, dass konsistente 1%-Verbesserungen durch unbewusstes Lernen während Schlafzyklen zu transformativer Kompetenzenentwicklung führen. Basierend auf den Prinzipien der Atomaren Gewohnheiten strukturiert der Artikel Persönlichkeitsentwicklung neu, um systembasierte tägliche Praktiken gegenüber willenskraftabhängigen Extremen zu priorisieren. Dies ist wichtig, da es eine psychologisch fundierte Alternative zu nicht nachhaltigen Herausforderungsformaten für den Aufbau dauerhafter beruflicher und persönlicher Kompetenzen bietet."
      },
      "es": {
        "title": "Presentación: Desafío Suave de 7,5 Días",
        "summary": "El Desafío Suave de 7,5 Días propone una mejora diaria sostenible en lugar de ráfagas intensas, argumentando que mejoras consistentes del 1% se componen en desarrollo de habilidades transformador a través del aprendizaje subconsciente durante los ciclos de sueño. Basándose en los principios de Hábitos Atómicos, el artículo reformula el desarrollo personal para priorizar la práctica diaria basada en sistemas sobre extremos dependientes de la fuerza de voluntad. Esto es importante porque ofrece una alternativa con base psicológica a formatos de desafío insostenibles para construir habilidades profesionales y personales duraderas."
      }
    }
  },
  {
    "title": "The Problem With Tracking Conversations Like Pageviews",
    "slug": "problem-tracking-conversations-like-pageviews",
    "url": "https://dev.to/shubhampalriwala/the-problem-with-tracking-conversations-like-pageviews-29fk",
    "source": "DEV Community",
    "date": "2026-02-26T05:48:31.000Z",
    "summary": "Traditional analytics metrics (session count, time-on-page) fundamentally mislead AI product managers because they measure static content consumption, not dynamic conversations where products respond to user input. High engagement metrics combined with 4% week-8 retention reveals users are engaging but not finding value, exposing the inadequacy of pageview-based measurement for conversational AI. This matters because incorrect metrics lead to false confidence in products with critical retention problems, delaying necessary product changes.",
    "content": "Your session numbers look great. Your users are churning. Here's why event-based analytics was never built for conversational AI products, and what to do instead.\nThe Problem With Tracking Conversations Like Pageviews\nPicture this. You’re a PM at an AI startup, six months post-launch. You open the dashboard on a Monday morning and everything looks… fine? Session count is up 20% week over week. Average session length is 4 minutes and 30 seconds. DAU is climbing. You screenshot it and drop it in the investor update Slack channel.\nThen you look at retention.\nWeek 4 retention is 12%. Week 8 is 4%. Users are showing up, having conversations, and disappearing. The metrics say engagement is strong. The business says something is very wrong.\nHere’s the thing nobody tells you when you ship your first AI product: you’ve been tracking conversations like pageviews, and that’s why your dashboard lies to you every single morning.\nPerson staring at metrics dashboard looking confused\n\n^ every AI PM on Monday morning when the numbers look good but retention is falling off a cliff\nThe Pageview Was Built for a World Where Content Sits Still\n\n\nThe pageview metric was invented in the mid-90s to answer one question: did someone look at this thing? That’s it. A newspaper prints a story. Did you open it? Click. Pageview logged. The content doesn’t change based on what you do. It just sits there. You either consumed it or you didn’t.\nThis mental model spread everywhere. Clicks, sessions, time-on-page, bounce rate, page depth. All of it built on the same foundational assumption: the product is a static artifact and the user is moving through it. Engagement equals consumption. More clicks means more engagement. More engagement means more value.\nThat assumption held for 25 years. It made analytics what it is today.\nAnd then we shipped products where the product itself responds to what the user says. The entire premise collapsed, and most teams haven’t noticed yet.\nA conversation is NOT a stati",
    "category": "github",
    "translations": {
      "zh": {
        "title": "像跟踪页面浏览量一样跟踪对话的问题",
        "summary": "传统分析指标（会话数、页面停留时间）根本上误导AI产品经理，因为它们测量静态内容消费，而非产品响应用户输入的动态对话。高参与度指标结合4%的第8周保留率表明用户正在参与但未找到价值，暴露了基于页面浏览量的测量对对话AI的不足。这很重要，因为错误的指标导致对具有关键保留问题的产品产生虚假信心，延迟了必要的产品更改。"
      },
      "fr": {
        "title": "Le Problème du Suivi des Conversations Comme des Pages Vues",
        "summary": "Les métriques analytiques traditionnelles (nombre de sessions, temps sur la page) trompent fondamentalement les responsables de produits IA car elles mesurent la consommation de contenu statique, pas les conversations dynamiques où les produits répondent aux entrées des utilisateurs. Les métriques d'engagement élevées combinées à une rétention de 4% à la semaine 8 révèlent que les utilisateurs s'engagent mais ne trouvent pas de valeur, exposant l'inadéquation de la mesure basée sur les pages vues pour l'IA conversationnelle. Cela importe car les métriques incorrectes conduisent à une fausse confiance dans les produits avec des problèmes de rétention critiques, retardant les changements de produit nécessaires."
      },
      "de": {
        "title": "Das Problem mit der Nachverfolgung von Gesprächen wie Seitenaufrufen",
        "summary": "Traditionelle Analyticmetriken (Sitzungsanzahl, Zeit auf der Seite) täuschen KI-Produktmanager grundlegend, da sie statische Inhaltsnutzung messen, nicht dynamische Gespräche, in denen Produkte auf Benutzereingaben reagieren. Hohe Engagement-Metriken kombiniert mit 4% Beibehaltung in Woche 8 zeigen, dass Benutzer sich engagieren, aber keinen Wert finden, was die Unzulänglichkeit von seitenaufruf-basierter Messung für konversationelle KI aufdeckt. Dies ist wichtig, da falsche Metriken zu falscher Zuversicht in Produkten mit kritischen Bindungsproblemen führen und notwendige Produktänderungen verzögern."
      },
      "es": {
        "title": "El Problema de Rastrear Conversaciones como Vistas de Página",
        "summary": "Las métricas analíticas tradicionales (recuento de sesiones, tiempo en la página) engañan fundamentalmente a los gerentes de productos de IA porque miden el consumo de contenido estático, no conversaciones dinámicas donde los productos responden a la entrada del usuario. Las métricas de participación alta combinadas con retención del 4% en la semana 8 revelan que los usuarios se están participando pero no encuentran valor, exponiendo la insuficiencia de la medición basada en vistas de página para IA conversacional. Esto importa porque las métricas incorrectas conducen a una falsa confianza en productos con problemas críticos de retención, retrasando cambios de producto necesarios."
      }
    }
  },
  {
    "title": "Building a Cross-Platform File Search App With Tauri — Not Electron",
    "slug": "building-cross-platform-file-search-app-tauri-electron",
    "url": "https://dev.to/kazutaka-dev/building-a-cross-platform-file-search-app-with-tauri-not-electron-2nke",
    "source": "DEV Community",
    "date": "2026-02-26T05:42:53.000Z",
    "summary": "OmniFile, a Tauri and Rust-based file search application, achieves an 8MB installer and 30MB idle RAM versus Electron's 80MB+ and 150MB+ by leveraging native webviews and Rust's performance for file I/O. Using Tantivy for full-text search with indexed but unstored content, the application unifies search across Google Drive, Dropbox, SharePoint, and local files while maintaining privacy-first design. This technical comparison demonstrates Rust's advantages for resource-constrained desktop applications requiring intensive file operations.",
    "content": "Every knowledge worker I know has the same problem: files scattered across Google Drive, Dropbox, SharePoint, Slack, Notion, GitHub, and their local machine. When you need to find something, you end up opening 4 different search bars.\nI built OmniFile to fix that — a single search bar that finds files across all your sources instantly. Desktop app, privacy-first, everything stays on your machine.\nHere's what I learned building it with Tauri + Rust instead of Electron, and why integrating 7 OAuth providers in a desktop app was harder than I expected.\nThe decision was simple: OmniFile needs to launch instantly (it's triggered by a global shortcut) and stay lightweight in the background. Electron ships a full Chromium browser. Tauri uses the OS's native webview and a Rust backend.\nThe result:\n~8MB installer vs Electron's ~80MB+\n~30MB RAM at idle vs Electron's ~150MB+\nRust backend for CPU-intensive indexing and file I/O\nThe tradeoff is that you write your backend in Rust instead of JavaScript. For file search, that's actually a benefit — Rust's performance for walking directories and parsing file formats is hard to beat.\nTantivy is Rust's answer to Lucene. I use it as the local search engine that indexes everything into a single queryable index.\nschema_builder.add_text_field(\"title\", TEXT | STORED);      // Tokenized + returned\nschema_builder.add_text_field(\"path\", STRING | STORED);      // Exact match\nschema_builder.add_text_field(\"content\", TEXT);              // Searchable but NOT stored\nschema_builder.add_text_field(\"source\", STRING | STORED);    // \"local\", \"gdrive\", etc.\nschema_builder.add_i64_field(\"modified_at\", INDEXED | STORED);\n\nThe key decision: content is indexed but not stored. For a desktop search app, this saves significant disk space — the content is already on disk, so we re-extract it when needed for display. This keeps the index small while enabling full-text search.\nEach cloud provider indexes into the same Tantivy index but with a different source",
    "category": "github",
    "translations": {
      "zh": {
        "title": "使用Tauri构建跨平台文件搜索应用 — 而非Electron",
        "summary": "OmniFile是一个基于Tauri和Rust的文件搜索应用，通过利用原生webviews和Rust的文件I/O性能，实现了8MB安装程序和30MB空闲RAM，相比Electron的80MB+和150MB+。使用Tantivy进行全文搜索且内容已索引但未存储，该应用统一搜索Google Drive、Dropbox、SharePoint和本地文件，同时保持隐私优先设计。这种技术比较演示了Rust对资源受限的桌面应用程序的优势，这些应用需要密集的文件操作。"
      },
      "fr": {
        "title": "Construire une Application de Recherche de Fichiers Multiplateforme avec Tauri — Pas Electron",
        "summary": "OmniFile, une application de recherche de fichiers basée sur Tauri et Rust, réalise un installeur de 8 Mo et une RAM inactive de 30 Mo par rapport aux 80 Mo+ et 150 Mo+ d'Electron en exploitant les webviews natifs et les performances d'E/S de fichiers de Rust. Utilisant Tantivy pour la recherche en texte intégral avec le contenu indexé mais non stocké, l'application unifie la recherche sur Google Drive, Dropbox, SharePoint et les fichiers locaux tout en maintenant une conception centrée sur la confidentialité. Cette comparaison technique démontre les avantages de Rust pour les applications de bureau contraintes par les ressources nécessitant des opérations de fichiers intensives."
      },
      "de": {
        "title": "Erstellen einer plattformübergreifenden Dateisuch-App mit Tauri — Nicht Electron",
        "summary": "OmniFile, eine auf Tauri und Rust basierende Dateisuchsoftware, erreicht ein 8-MB-Installationsprogramm und 30-MB-Leerlauf-RAM im Vergleich zu Electrons 80MB+ und 150MB+, indem sie native Webviews und Rusts Leistung für Datei-E/A nutzt. Unter Verwendung von Tantivy für die Volltextsuche mit indiziertem aber nicht gespeichertem Inhalt vereinheitlicht die Anwendung die Suche auf Google Drive, Dropbox, SharePoint und lokalen Dateien, während sie ein datenschutzorientiertes Design beibehält. Dieser technische Vergleich zeigt Rusts Vorteile für ressourcenbeschränkte Desktopanwendungen, die intensive Dateivorgänge erfordern."
      },
      "es": {
        "title": "Construir una Aplicación de Búsqueda de Archivos Multiplataforma con Tauri — No Electron",
        "summary": "OmniFile, una aplicación de búsqueda de archivos basada en Tauri y Rust, logra un instalador de 8 MB y 30 MB de RAM inactiva en comparación con 80 MB+ y 150 MB+ de Electron al aprovechar las webviews nativas y el rendimiento de E/S de archivos de Rust. Utilizando Tantivy para búsqueda de texto completo con contenido indexado pero no almacenado, la aplicación unifica la búsqueda en Google Drive, Dropbox, SharePoint y archivos locales mientras mantiene un diseño centrado en la privacidad. Esta comparación técnica demuestra las ventajas de Rust para aplicaciones de escritorio con restricciones de recursos que requieren operaciones intensivas de archivos."
      }
    }
  },
  {
    "title": "CVE-2026-27575: The Zombie Session: Breaking Vikunja's Auth with CVE-2026-27575",
    "slug": "cve-2026-27575-zombie-session-breaking-vikunja-auth",
    "url": "https://dev.to/cverports/cve-2026-27575-the-zombie-session-breaking-vikunjas-auth-with-cve-2026-27575-pij",
    "source": "DEV Community",
    "date": "2026-02-26T05:40:19.000Z",
    "summary": "CVE-2026-27575 is a critical vulnerability (CVSS 9.1) in Vikunja before v2.0.0 allowing single-character passwords and failing to invalidate JWT sessions after password changes, enabling attackers with stolen tokens to maintain permanent access regardless of victim credential resets. The flaw demonstrates the architectural dangers of stateless JWTs without revocation mechanisms or input validation on password changes. This matters because it illustrates how session management failures in authentication systems create persistent account takeover risks in self-hosted platforms.",
    "content": "The Zombie Session: Breaking Vikunja's Auth with CVE-2026-27575\n\n\n\nVulnerability ID: CVE-2026-27575\nCVSS Score: 9.1\nPublished: 2026-02-25\nCVE-2026-27575 represents a catastrophic failure in the authentication lifecycle of Vikunja, a popular self-hosted task management platform. The vulnerability is a two-headed beast: first, it allowed users (and attackers) to set passwords with a single character, bypassing security policies during updates. Second, and far more critical, it failed to invalidate active sessions upon password changes. This means an attacker who steals a session token retains permanent access to the victim's data, even after the victim explicitly resets their credentials to 'lock them out.' It is a classic case of stateless JWTs being deployed without a revocation strategy.\nVikunja versions prior to 2.0.0 allow persistent account takeover. Due to a lack of input validation, passwords could be reset to a single character. Worse, changing a password did not invalidate existing JSON Web Tokens (JWTs). An attacker with a stolen token remains logged in indefinitely, regardless of the victim's remediation attempts. Fix: Upgrade to v2.0.0 immediately.\nCWE IDs: CWE-521 (Weak Password), CWE-613 (Insufficient Session Expiration)\nCVSS Score: 9.1 (Critical)\nAttack Vector: Network (API)\nPrivileges Required: None (for initial access via weak policy logic)\nExploit Status: PoC Available / Trivial\nPatch Date: 2026-02-25\nVikunja < 2.0.0\nVikunja: < 2.0.0 (Fixed in: 2.0.0)\n89c17d3\n\n\nEnforce password limits on update and reset\ntype UserPassword struct {\n- NewPassword string `json:\"new_password\"`\n+ NewPassword string `json:\"new_password\" valid:\"minLength:8\"`\n}\n\n2526853\n\n\nRefactor session management to stateful tokens\n// Logic added to invalidate sessions on password change\n\nEnforce minimum password complexity on all inputs, not just registration.\nImplement stateful session management or token denylists.\nInvalidate all active sessions upon password rotation.\nRemediation Ste",
    "category": "github",
    "translations": {
      "zh": {
        "title": "CVE-2026-27575：僵尸会话：破坏Vikunja认证的CVE-2026-27575",
        "summary": "CVE-2026-27575是Vikunja v2.0.0之前的严重漏洞（CVSS 9.1），允许单字符密码且在密码更改后未能使JWT会话失效，使得攻击者能够使用被盗令牌保持永久访问权限，无论受害者如何重置凭证。该漏洞展示了无状态JWT在没有撤销机制或密码变更输入验证情况下的架构风险。这很重要，因为它说明了认证系统中会话管理失败如何在自托管平台中造成持久的账户接管风险。"
      },
      "fr": {
        "title": "CVE-2026-27575 : La session zombie : Briser l'authentification de Vikunja avec CVE-2026-27575",
        "summary": "CVE-2026-27575 est une vulnérabilité critique (CVSS 9.1) dans Vikunja antérieur à v2.0.0 permettant des mots de passe d'un seul caractère et ne parvenant pas à invalider les sessions JWT après les changements de mot de passe, permettant aux attaquants disposant de jetons volés de maintenir un accès permanent indépendamment de la réinitialisation des identifiants des victimes. La faille démontre les dangers architecturaux des JWT sans état sans mécanismes de révocation ou validation d'entrée lors des changements de mot de passe. C'est important car cela illustre comment les défaillances de gestion de session dans les systèmes d'authentification créent des risques persistants de prise de compte dans les plates-formes autohébergées."
      },
      "de": {
        "title": "CVE-2026-27575: Die Zombie-Sitzung: Vikunjas Authentifizierung mit CVE-2026-27575 brechen",
        "summary": "CVE-2026-27575 ist eine kritische Sicherheitslücke (CVSS 9.1) in Vikunja vor v2.0.0, die Ein-Zeichen-Passwörter ermöglicht und JWT-Sitzungen nach Passwortänderungen nicht ungültig macht, wodurch Angreifer mit gestohlenen Token unabhängig von den Anmeldedaten-Zurückstellungen des Opfers permanenten Zugriff behalten können. Die Schwachstelle demonstriert die architektonischen Gefahren zustandsloser JWTs ohne Widerrufsmechanismen oder Eingabevalidierung bei Passwortänderungen. Dies ist wichtig, da es zeigt, wie Fehler bei der Sitzungsverwaltung in Authentifizierungssystemen persistente Kontoübernahmevorkehrungen in selbstgehosteten Plattformen schaffen."
      },
      "es": {
        "title": "CVE-2026-27575: La sesión zombi: Romper la autenticación de Vikunja con CVE-2026-27575",
        "summary": "CVE-2026-27575 es una vulnerabilidad crítica (CVSS 9.1) en Vikunja anterior a v2.0.0 que permite contraseñas de un solo carácter e incapacidad para invalidar sesiones JWT después de cambios de contraseña, permitiendo a atacantes con tokens robados mantener acceso permanente independientemente de restablecimientos de credenciales de víctimas. La falla demuestra los peligros arquitectónicos de JWT sin estado sin mecanismos de revocación o validación de entrada en cambios de contraseña. Esto importa porque ilustra cómo las fallas de gestión de sesión en sistemas de autenticación crean riesgos persistentes de apropiación de cuentas en plataformas autohospedadas."
      }
    }
  },
  {
    "title": "🇹🇷 Türkiye devs: Add TC Kimlik Numarası to the AI identity standard — Soulprint open source (30 min PR)",
    "slug": "turkiye-devs-add-tc-kimlik-soulprint-open-source-pr",
    "url": "https://dev.to/manuel_felipeariaspined/turkiye-devs-add-tc-kimlik-numarasi-to-the-ai-identity-standard-soulprint-open-source-30-min-4jli",
    "source": "DEV Community",
    "date": "2026-02-26T05:33:47.000Z",
    "summary": "The post invites Turkish developers to contribute a TC Kimlik Numarası (Turkish national ID) verifier to the Soulprint open-source identity standard by implementing a 30-minute pull request with validation algorithms. The implementation enables AI agents to verify Turkish identity documents using checksum formulas on 11-digit IDs, expanding identity verification across emerging markets. This matters because crowdsourced identity standard development demonstrates how open-source projects can democratize AI verification capabilities globally.",
    "content": "AI ajanları her gün kimlik doğrulaması olmadan kararlar alıyor. Soulprint — ZK Proofs, yerel, MIT.\n🇹🇷 TC Kimlik Numarası Soulprint'te henüz yok. 30 dakikada PR gönderebilirsin.\nhane: ((1+3+5+7+9. basamak)*7 − (2+4+6+8. basamak)) mod 10.\nhane: ilk 10 basamak toplamı mod 10.\n\n\n\n\nconst TR: CountryVerifier = {\n  countryCode: \"TR\", countryName: \"Turkey\",\n  documentTypes: [\"tc_kimlik\"],\n  parse(ocrText: string): DocumentResult {\n    const tc = ocrText.match(/(\\d{11})/)?.[1] ?? \"\";\n    return { valid: !!tc, doc_number: tc, country: \"TR\" };\n  },\n  validate(docNumber: string): NumberValidation {\n    if(!/^\\d{11}$/.test(docNumber)||docNumber[0]===\"0\") return {valid:false};\n    const d=docNumber.split(\"\").map(Number);\n    const c10=((d[0]+d[2]+d[4]+d[6]+d[8])*7-(d[1]+d[3]+d[5]+d[7]))%10;\n    const c11=d.slice(0,10).reduce((a,b)=>a+b,0)%10;\n    return { valid: d[9]===c10 && d[10]===c11 };\n  },\n};\nexport default TR;\n\n💻 GitHub · Bir PR. Bir ülke.",
    "category": "github",
    "translations": {
      "zh": {
        "title": "🇹🇷 土耳其开发者：将TC Kimlik Numarası添加到AI身份标准——Soulprint开源（30分钟PR）",
        "summary": "该帖子邀请土耳其开发者通过实现具有验证算法的30分钟拉取请求，向Soulprint开源身份标准贡献TC Kimlik Numarası（土耳其国家ID）验证器。该实现使AI代理能够使用11位数字ID的校验和公式验证土耳其身份文件，扩展了新兴市场的身份验证。这很重要，因为众包身份标准开发演示了开源项目如何能在全球范围内民主化AI验证能力。"
      },
      "fr": {
        "title": "🇹🇷 Développeurs turcs : Ajouter TC Kimlik Numarası à la norme d'identité IA — Soulprint open source (30 min PR)",
        "summary": "Le message invite les développeurs turcs à contribuer un vérificateur TC Kimlik Numarası (ID national turc) à la norme d'identité open-source Soulprint en implémentant une demande d'extraction de 30 minutes avec des algorithmes de validation. L'implémentation permet aux agents IA de vérifier les documents d'identité turcs en utilisant des formules de somme de contrôle sur les ID à 11 chiffres, élargissant la vérification d'identité sur les marchés émergents. C'est important car le développement collaboratif de normes d'identité démontre comment les projets open-source peuvent démocratiser les capacités de vérification IA à l'échelle mondiale."
      },
      "de": {
        "title": "🇹🇷 Türkische Entwickler: TC Kimlik Numarası zur AI-Identitätsnorm hinzufügen — Soulprint Open Source (30-minütiges PR)",
        "summary": "Der Beitrag lädt türkische Entwickler ein, einen TC Kimlik Numarası (türkische nationale ID) Verifizierer zum Soulprint Open-Source-Identitätsstandard beizutragen, indem eine 30-Minuten-Pull-Request mit Validierungsalgorithmen implementiert wird. Die Implementierung ermöglicht es KI-Agenten, türkische Identitätsdokumente unter Verwendung von Prüfsummiformeln auf 11-stelligen IDs zu überprüfen und erweitert die Identitätsüberprüfung auf Schwellenländern. Dies ist wichtig, da die Entwicklung von crowdsourced-Identitätsstandards zeigt, wie Open-Source-Projekte KI-Verifizierungsfähigkeiten weltweit demokratisieren können."
      },
      "es": {
        "title": "🇹🇷 Desarrolladores turcos: Agregar TC Kimlik Numarası al estándar de identidad de IA — Soulprint de código abierto (PR de 30 minutos)",
        "summary": "El mensaje invita a los desarrolladores turcos a contribuir un verificador de TC Kimlik Numarası (ID nacional turco) al estándar de identidad de código abierto Soulprint implementando una solicitud de extracción de 30 minutos con algoritmos de validación. La implementación permite que los agentes de IA verifiquen documentos de identidad turcos utilizando fórmulas de suma de verificación en ID de 11 dígitos, expandiendo la verificación de identidad en mercados emergentes. Esto importa porque el desarrollo colaborativo de normas de identidad demuestra cómo los proyectos de código abierto pueden democratizar las capacidades de verificación de IA a nivel mundial."
      }
    }
  },
  {
    "title": "NABARD Grade A 2025 — eligibility, syllabus, and strategy",
    "slug": "nabard-grade-a-2025-eligibility-syllabus-strategy",
    "url": "https://dev.to/sabya_beworld_e066e3758d8/nabard-grade-a-2025-eligibility-syllabus-and-strategy-2n5l",
    "source": "DEV Community",
    "date": "2026-02-26T05:29:21.000Z",
    "summary": "The NABARD Grade A 2025 exam guide specifies eligibility criteria (age 25-35, bachelor's degree, 2+ years rural banking experience) and outlines a three-phase structure covering general English, reasoning, quantitative aptitude, plus specialized topics in agriculture and economics. The comprehensive syllabus overview provides a framework for candidates preparing for India's National Bank for Agriculture and Rural Development assistant manager positions. This matters because it clarifies requirements for accessing rural development career opportunities in India's banking sector.",
    "content": "NABARD Grade A 2025 — Unlock Your Dream Job in Rural Banking\n\n\nAre you ready to make a difference in rural India? NABARD Grade A is an exciting opportunity for young professionals like you to join the National Bank for Agriculture and Rural Development (NABARD) as Assistant Managers. In this blog post, we'll guide you through the eligibility criteria, syllabus, and strategy to crack the exam.\nEligibility Criteria: Don't Miss Out\n\n\nBefore diving into the preparation phase, let's ensure you meet the basic requirements:\nAge Limit: 25-35 years (relaxation for reserved categories)\nEducation: Bachelor's degree in any discipline from a recognized university\nWork Experience: Minimum 2 years of experience in rural banking or a related field\nIf you've checked off all these boxes, congratulations! You're eligible to apply. But remember, meeting the eligibility criteria is just the starting point.\nSyllabus: Understand What's at Stake\n\n\nThe NABARD Grade A exam consists of three phases:\n Phase I: Multiple-choice questions (MCQs) in General English, Reasoning Ability, and Quantitative Aptitude\n Phase II: Descriptive tests in English, Agriculture, Economics, and Finance\n Final Interview: Assess your communication skills and knowledge\nAccording to JobSafal.com (https://jobsafal.com), a reliable resource for banking exam aspirants, the syllabus is vast but manageable with focused preparation.\nPhase I Syllabus: Prepare Wisely\n\n\n\nGeneral English:\n\n\nGrammar\nVocabulary\nComprehension\nReasoning Ability:\n\n\nLogical reasoning\nData interpretation\nAnalytical reasoning\nQuantitative Aptitude:\n\n\nNumber systems\nAlgebra\nGeometry\nPhase II Syllabus: Dive into the Details\n\n\n\nEnglish:\n\n\nGrammar\nVocabulary\nComprehension\nAgriculture:\n\n\nCrop management\nSoil science\nAgricultural economics\nEconomics:\n\n\nMicroeconomics\nMacroeconomics\nPublic finance\nFinance:\n\n\nFinancial markets\nBanking and insurance\nAccounting\nStudy Schedule: Stay on Track\n\n\nTo ensure you don't miss out on any topic, create a study schedule wit",
    "category": "github",
    "translations": {
      "zh": {
        "title": "NABARD等级A 2025——资格、大纲和策略",
        "summary": "NABARD等级A 2025考试指南指定了资格标准（年龄25-35岁、学士学位、2年以上农村银行经验），并概述了涵盖一般英语、推理、定量能力以及农业和经济学专业主题的三阶段结构。全面的大纲概述为准备印度国家农业和农村发展银行助理经理职位的候选人提供了框架。这很重要，因为它澄清了获取印度银行部门农村发展职业机会的要求。"
      },
      "fr": {
        "title": "NABARD Grade A 2025 — admissibilité, programme et stratégie",
        "summary": "Le guide d'examen NABARD Grade A 2025 spécifie les critères d'admissibilité (âge 25-35 ans, diplôme d'une licence, 2+ ans d'expérience dans les banques rurales) et décrit une structure en trois phases couvrant l'anglais général, le raisonnement, l'aptitude quantitative, plus les sujets spécialisés en agriculture et en économie. L'aperçu complet du programme offre un cadre aux candidats se préparant pour les postes de gestionnaire adjoint de la Banque nationale pour l'agriculture et le développement rural de l'Inde. C'est important car cela clarifie les exigences pour accéder aux opportunités de carrière en développement rural dans le secteur bancaire indien."
      },
      "de": {
        "title": "NABARD Grade A 2025 — Berechtigung, Lehrplan und Strategie",
        "summary": "Der Prüfungsleitfaden NABARD Grade A 2025 gibt die Zulassungskriterien an (Alter 25-35 Jahre, Bachelorabschluss, 2+ Jahre Erfahrung im ländlichen Bankwesen) und skizziert eine dreiphasige Struktur mit allgemeinem Englisch, Argumentation, quantitativen Fähigkeiten sowie spezialisierten Themen in Landwirtschaft und Wirtschaft. Der umfassende Lehrplanüberblick bietet einen Rahmen für Kandidaten, die sich auf die Positionen des stellvertretenden Managers der indischen Nationalbank für Landwirtschaft und Landentwicklung vorbereiten. Dies ist wichtig, da es die Anforderungen für den Zugang zu Karrieremöglichkeiten in der ländlichen Entwicklung im indischen Bankensektor verdeutlicht."
      },
      "es": {
        "title": "NABARD Grado A 2025 — elegibilidad, plan de estudios y estrategia",
        "summary": "La guía del examen NABARD Grado A 2025 especifica los criterios de elegibilidad (edad 25-35 años, licenciatura, 2+ años de experiencia en banca rural) y describe una estructura de tres fases que cubre inglés general, razonamiento, aptitud cuantitativa, más temas especializados en agricultura y economía. La descripción general completa del plan de estudios proporciona un marco para los candidatos que se preparan para las posiciones de gerente asistente del Banco Nacional para la Agricultura y Desarrollo Rural de India. Esto importa porque aclara los requisitos para acceder a oportunidades de carrera en desarrollo rural en el sector bancario indio."
      }
    }
  },
  {
    "title": "Tech companies shouldn't be bullied into doing surveillance",
    "slug": "tech-companies-shouldnt-be-bullied-into-doing-surveillance",
    "url": "https://www.eff.org/deeplinks/2026/02/tech-companies-shouldnt-be-bullied-doing-surveillance",
    "source": "Hacker News",
    "date": "2026-02-26T00:37:32.000Z",
    "summary": "The EFF argues against government pressure on tech companies to implement surveillance capabilities, warning that coercion threatens user privacy and corporate independence. The article raises concerns about forced compliance and its implications for digital rights.",
    "content": "Article URL: https://www.eff.org/deeplinks/2026/02/tech-companies-shouldnt-be-bullied-doing-surveillance\nComments URL: https://news.ycombinator.com/item?id=47160226\nPoints: 300\n# Comments: 100",
    "category": "github",
    "translations": {
      "zh": {
        "title": "科技公司不应该被强制进行监控",
        "summary": "电子前沿基金会反对政府对科技公司施压实施监控功能，警告强制威胁用户隐私和企业独立性。该文章对强制合规及其对数字权利的影响表示关切。"
      },
      "fr": {
        "title": "Les entreprises technologiques ne devraient pas être forcées à faire de la surveillance",
        "summary": "L'EFF s'oppose à la pression gouvernementale sur les entreprises technologiques pour mettre en œuvre des capacités de surveillance, avertissant que la coercition menace la vie privée des utilisateurs et l'indépendance des entreprises. L'article soulève des préoccupations concernant la conformité forcée et ses implications pour les droits numériques."
      },
      "de": {
        "title": "Technologieunternehmen sollten nicht zu Überwachung gezwungen werden",
        "summary": "Die EFF spricht sich gegen Druck der Regierung auf Technologieunternehmen aus, um Überwachungsfunktionen zu implementieren, und warnt davor, dass Zwang die Benutzerprivatsphäre und die Unternehmensunabhängigkeit gefährdet. Der Artikel äußert Bedenken über erzwungene Compliance und ihre Auswirkungen auf digitale Rechte."
      },
      "es": {
        "title": "Las empresas tecnológicas no deberían ser obligadas a realizar vigilancia",
        "summary": "La EFF se opone a la presión gubernamental sobre las empresas tecnológicas para implementar capacidades de vigilancia, advirtiendo que la coerción amenaza la privacidad del usuario y la independencia corporativa. El artículo plantea preocupaciones sobre el cumplimiento forzado y sus implicaciones para los derechos digitales."
      }
    }
  },
  {
    "title": "Next.js 앱을 하루만에 6개국어로 만든 방법",
    "slug": "nextjs-app-six-languages-one-day",
    "url": "https://dev.to/ji_ai/nextjs-aebeul-harumane-6gaegugeoro-mandeun-bangbeob-pi5",
    "source": "DEV Community",
    "date": "2026-02-26T00:05:57.000Z",
    "summary": "This article details implementing internationalization in Next.js 15 using next-intl, covering locale-based routing architecture, translation file management, and region-specific pricing strategies to support rapid deployment across six countries and diverse markets.",
    "content": "사주 앱을 6개국에 내놓기로 했다. 한국, 미국, 일본, 중국, 베트남, 인도.\n사주가 동아시아 문화권 밖에서 먹힐까? 모르겠다. 근데 타로와 점성술이 전세계에서 먹히는 걸 보면, \"AI가 당신의 운명을 분석합니다\"는 어디서든 클릭을 부를 것 같았다.\n문제는 하드코딩된 한국어가 모든 페이지에 박혀 있다는 거다.\nNext.js 15 App Router에서 i18n 옵션은 몇 가지 있다. next-intl을 고른 이유는 단순하다 — App Router 네이티브 지원이 가장 깔끔하다. [locale] 동적 세그먼트에 미들웨어로 자동 리디렉트. Server Component에서도 Client Component에서도 같은 useTranslations() 훅.\napps/web/\n├── app/\n│   ├── [locale]/          ← 모든 페이지가 여기 안으로\n│   │   ├── page.tsx\n│   │   ├── result/page.tsx\n│   │   └── layout.tsx     ← html lang={locale} 여기서\n│   └── layout.tsx          ← 빈 껍데기\n├── i18n/\n│   ├── config.ts           ← locales, defaultLocale\n│   ├── routing.ts          ← localePrefix: \"as-needed\"\n│   └── navigation.ts       ← i18n Link, useRouter\n├── messages/\n│   ├── ko.json\n│   ├── en.json\n│   ├── ja.json\n│   ├── zh.json\n│   ├── vi.json\n│   └── hi.json\n└── middleware.ts            ← Accept-Language 감지\n\nlocalePrefix: \"as-needed\"가 핵심이다. 한국어가 디폴트니까 /로 접속하면 한국어, /en/으로 가면 영어. 한국 사용자는 URL에 /ko/가 안 붙는다.\nNext.js App Router에서 root layout은 반드시 <html>과 <body>를 렌더링해야 한다고 알고 있었다. 그래서 root layout에도 넣고, [locale]/layout.tsx에도 <html lang={locale}>을 넣었다.\n결과: html 안에 html. 브라우저는 조용히 무시하지만 완전히 잘못된 구조다.\n// app/layout.tsx — 이게 정답\nexport default function RootLayout({ children }) {\n  return children;  // html/body 없이 그냥 패스스루\n}\n\n// app/[locale]/layout.tsx — 여기서 html/body 관리\nexport default function LocaleLayout({ children, params }) {\n  return (\n    <html lang={locale}>\n      <body>{children}</body>\n    </html>\n  );\n}\n\nroot layout이 그냥 children만 리턴해도 Next.js 15에서는 에러가 안 난다. [locale] layout이 html/body를 제공하니까.\n같은 서비스라도 인도에서 $9.90을 받으면 아무도 안 산다. 각 나라 구매력에 맞춰 가격을 잡았다.\n// ko.json\n\"price\": \"₩12,900\"\n\n// en.json\n\"price\": \"$9.90\"\n\n// ja.json\n\"price\": \"¥1,490\"\n\n// zh.json\n\"price\": \"¥68\"\n\n// vi.json\n\"price\": \"199.000₫\"\n\n// hi.json\n\"price\": \"₹799\"\n\n번역 파일에 가격을 하드코딩한 거다. 나중에 결제 연동하면 서버에서 내려주겠지만, MVP 단계에서는 이게 가장 빠르다. placeholder 이름도 로컬라이즈했다 — 한국은 \"홍길동\", 일본은 \"山田太郎\", 인도는 \"राहुल शर्मा\".\napp/page.tsx를 app/[locale]/page.tsx로 옮",
    "category": "github",
    "translations": {
      "zh": {
        "title": "如何在一天内用6种语言构建Next.js应用",
        "summary": "本文详细介绍了使用next-intl在Next.js 15中实现国际化的方法，涵盖基于地区的路由架构、翻译文件管理和地域特定的定价策略，以支持在六个国家和不同市场中的快速部署。"
      },
      "fr": {
        "title": "Comment créer une application Next.js en six langues en une journée",
        "summary": "Cet article détaille l'implémentation de l'internationalisation dans Next.js 15 en utilisant next-intl, couvrant l'architecture de routage basée sur les paramètres régionaux, la gestion des fichiers de traduction et les stratégies de tarification spécifiques aux régions pour soutenir le déploiement rapide sur six pays et différents marchés."
      },
      "de": {
        "title": "Wie man eine Next.js-App in sechs Sprachen an einem Tag erstellt",
        "summary": "Dieser Artikel beschreibt die Implementierung der Internationalisierung in Next.js 15 mit next-intl und behandelt die Routing-Architektur nach Gebietsschema, die Verwaltung von Übersetzungsdateien und regionsspezifische Preisstrategien, um die schnelle Bereitstellung in sechs Ländern und verschiedenen Märkten zu unterstützen."
      },
      "es": {
        "title": "Cómo crear una aplicación Next.js en seis idiomas en un día",
        "summary": "Este artículo detalla la implementación de la internacionalización en Next.js 15 usando next-intl, cubriendo la arquitectura de enrutamiento basada en configuración regional, la gestión de archivos de traducción y estrategias de precios específicas por región para respaldar la implementación rápida en seis países y diversos mercados."
      }
    }
  },
  {
    "title": "Claude 하나로 1인 SaaS 전체를 설계한 기록",
    "slug": "claude-designed-saas-entire-stack-one-session",
    "url": "https://dev.to/ji_ai/claude-hanaro-1in-saas-jeoncereul-seolgyehan-girog-44h5",
    "source": "DEV Community",
    "date": "2026-02-26T00:01:21.000Z",
    "summary": "This article documents using Claude AI to comprehensively design a Korean fortune-telling SaaS, demonstrating how iterative conversations can simulate expert panels, identify business strategy gaps, and generate production-ready plans—replacing what would cost thousands in consulting.",
    "content": "\"로그인 벽부터 제거하세요. magic link 인증이 최대 이탈 원인입니다.\"\n이 말을 한 건 사람이 아니다. Claude가 시뮬레이션한 \"PM 역할의 가상 전문가\"다.\n하루 동안 Claude랑 9개 세션을 했다. 나온 산출물이 20개다. 사업 전략서, 랜딩 디자인, 전문가 패널 회의록, LLM 비용 분석서, 글로벌 확장 전략서, Claude Code 실행용 태스크 파일들.\n이걸 컨설팅 회사에 맡겼으면 몇 주에 몇 천만원이다.\n혼자, 하루, $0.\n처음부터 전부를 시킨 게 아니다. 대화가 깊어지면서 구체화됐다.\n1턴: \"사주 앱 사업성 어때?\" (추상적)\n2턴: \"무료/유료 나눠서 수익 모델 짜줘\" (구체적)\n3턴: \"무료 티어 API 비용을 토큰 단위로 계산해줘\" (매우 구체적)\n4턴: \"Prompt Caching 적용 시 시나리오 A/B/C 비교해줘\" (초구체적)\n\n한 번에 \"사업계획서 써줘\"라고 하면 일반적인 답변이 나온다.\n점진적으로 깊이를 올리면, 각 단계에서 AI가 이전 맥락을 다 갖고 있으니까 결과물이 훨씬 정밀해진다.\n가장 효과가 좋았던 건 \"전문가 패널 시뮬레이션\"이다.\n나: \"전문가 6명을 구성해줘.\n    PM 1명, 사업개발 1명, 로컬라이제이션 1명,\n    미국 시장 전문가 1명, 풀스택 개발자 1명, UI/UX 디자이너 1명.\n    각자 이름이랑 관점을 정해줘.\n    현재 상태(STATUS.md)를 리뷰하고 회의해줘.\"\n\nClaude가 6명의 캐릭터를 만들어서 각자의 관점으로 토론한다. PM이 우선순위를 짜고, 사업 담당이 시장성을 따지고, 개발자가 기술 난이도를 짚고, 디자이너가 UX 이슈를 제기한다.\n혼자 사업하면 \"내 관점\"밖에 없다. 이걸 쓰면 6개 관점이 동시에 나온다.\n물론 진짜 전문가 6명과는 다르다. 하지만 1인 개발자가 놓치기 쉬운 사각지대를 잡아내는 데는 충분하다.\n로그인 벽 제거가 첫 번째였다. magic link 인증이 최대 이탈 원인이라는 걸 PM이 지적했다. 무료 분석은 완전 비로그인으로 전환.\n무료 티어 비용도 94% 잘라냈다. LLM 풀 호출 대신 알고리즘 포맷팅에 AI 1줄 요약만 붙이는 구조로 건당 $0.085 → $0.005.\n사업자등록은 유저 반응 기다리지 말고 결제 연동을 위해 즉시 시작하기로 했다.\nGA4와 Rate Limiting은 필수라는 데 전원 동의했다. 분석 없이 개선은 눈감고 운전이고, 보호 없는 무료 API는 비용 폭탄이다.\n카카오 공유와 OG 이미지를 우선 구현하기로 했다. 사주 서비스의 유일한 무료 마케팅은 바이럴이다.\n글로벌 확장은 보류. 한국에서 유료 전환율 3% 넘기 전까지 영문화에 리소스 안 쓴다.\n이 결정들을 혼자 앉아서 다 생각해냈을까? 솔직히, 사업자등록 즉시 진행이랑 Rate Limiting은 나중에야 생각했을 거다.\n다른 프로젝트에도 그대로 적용할 수 있다. 비전 공유로 큰 그림을 그리고, 전략 수립에서 선택지를 결정하고, 전문가 패널로 검증하고, 디자인을 실물 HTML로 뽑고, 비용을 토큰 단위로 분석하고, 확장 아키텍처를 설계하고, 마지막에 Claude Code에서 실행 가능한 태스크 파일로 변환한다.\nAI를 도구로 쓰는 것과 AI와 사고하는 것은 다르다.\n도구로 쓰면 \"코드 써줘.\" 사고하면 \"이 구조가 맞아? 빠진 거 없어? 내가 놓치는 관점이 뭐야?\"\n후자가 1인 개발자한테는 훨씬 가치가 크다.\n한 가지 솔직한 고백. 이 패널에서 나온 숫자들 — \"30% 전환율\", \"₹99가 최적가\" 같은 것 — 에는 근거가 없다. Claude가 만든 가설이지, 데이터에서 나온 결론이 아니다.\n가설은 가설로만 취급하고, 검증은 런칭 후 실제 데이터로 한다.\n\"AI와 사고하면 6개 관점이 동시에 나온다. 진짜 전문가 6명은 아니지만, 혼자보다는 훨씬 낫다.\"",
    "category": "github",
    "translations": {
      "zh": {
        "title": "仅用Claude设计整个独立SaaS的记录",
        "summary": "本文记录了使用Claude AI全面设计一个韩国算命SaaS的过程，展示了迭代对话如何能够模拟专家小组、识别商业战略漏洞和生成生产就绪的计划——替代花费数千元咨询的方案。"
      },
      "fr": {
        "title": "Enregistrement de la conception d'une SaaS entière en solo avec Claude",
        "summary": "Cet article documente l'utilisation de Claude AI pour concevoir de manière complète une SaaS coréenne de prédiction de fortune, démontrant comment les conversations itératives peuvent simuler des panels d'experts, identifier les lacunes des stratégies commerciales et générer des plans prêts pour la production—remplaçant ce qui coûterait des milliers en consulting."
      },
      "de": {
        "title": "Aufzeichnung der Gestaltung eines gesamten Solo-SaaS mit Claude",
        "summary": "Dieser Artikel dokumentiert die Verwendung von Claude AI, um ein koreanisches Wahrsage-SaaS umfassend zu gestalten, und zeigt, wie iterative Gespräche Expert-Panels simulieren, Lücken in der Geschäftsstrategie identifizieren und produktionsbereite Pläne erstellen können—was Tausende an Beratungskosten ersetzt."
      },
      "es": {
        "title": "Registro del diseño de un SaaS completo en solitario con Claude",
        "summary": "Este artículo documenta el uso de Claude AI para diseñar de manera integral un SaaS coreano de adivinación de fortuna, demostrando cómo las conversaciones iterativas pueden simular paneles de expertos, identificar brechas en la estrategia comercial y generar planes listos para producción—reemplazando lo que costaría miles en consultoría."
      }
    }
  },
  {
    "title": "Understanding IP Management in Oracle Cloud Infrastructure (OCI)",
    "slug": "oracle-cloud-infrastructure-ip-management-guide",
    "url": "https://dev.to/hiltonj/understanding-ip-management-in-oracle-cloud-infrastructure-oci-1ili",
    "source": "DEV Community",
    "date": "2026-02-26T00:01:10.000Z",
    "summary": "A comprehensive guide to OCI's IP address management system covering private IPs for internal communication, public IPs for internet access, and advanced features like reserved IPs and bring-your-own-IP options, essential for building secure cloud infrastructure.",
    "content": "Navigating the complexities of cloud networking is crucial for building robust and scalable applications. In Oracle Cloud Infrastructure (OCI), effective IP address management forms the backbone of your network architecture. This guide will demystify OCI's IP address categories, explore their use cases, and introduce advanced concepts like Reserved Public IPs, Bring Your Own IP (BYOIP), and Public IP Pools. \nOCI categorizes IP addresses into two primary types, each serving distinct communication needs. \nThese are used for internal communication within your OCI network and with connected on-premises environments. \nInternal Communication: Instances within the same Virtual Cloud Network (VCN) communicate seamlessly using private IPs.\nVCN Peering: Connecting multiple VCNs, whether in the same or different regions, relies on private IP routing.\nOn-premises Connectivity: Secure connections to your data centers via the Dynamic Routing Gateway (DRG).\nInstance Allocation: Each instance receives at least one primary private IP.\nVNIC Capacity: Every Virtual Network Interface Card (VNIC) includes one primary private IP address and supports up to 32 secondary private IP addresses, totaling 33 private IPs per VNIC.\nThese are designed for internet accessibility, allowing your resources to communicate with the outside world. \nInternet Reachability: Public IPs are reachable from the internet, assigned to a private IP object on your OCI resource. \nPrerequisites: For a public IP to function, your VCN requires an Internet Gateway, and the associated public subnet must have correctly configured Route Tables and Security Lists. \nFlexibility: Resources can be assigned multiple public IPs across single or multiple VNICs. \nOCI offers two types of public IP addresses to cater to different operational requirements.\n\nReserved Public IP Addresses in Detail\nCreation: You create them individually. \nLimits: Up to 50 Reserved Public IPs are allowed per region. \nAssignment: Assigned to resources aft",
    "category": "github",
    "translations": {
      "zh": {
        "title": "了解Oracle云基础设施(OCI)中的IP管理",
        "summary": "这是一份全面的OCI IP地址管理系统指南，涵盖用于内部通信的私有IP、用于互联网访问的公共IP以及高级功能（如保留IP和自带IP选项），这些对于构建安全的云基础设施至关重要。"
      },
      "fr": {
        "title": "Comprendre la gestion des adresses IP dans l'infrastructure cloud Oracle (OCI)",
        "summary": "Un guide complet du système de gestion des adresses IP d'OCI couvrant les adresses IP privées pour la communication interne, les adresses IP publiques pour l'accès Internet et les fonctionnalités avancées comme les adresses IP réservées et les options bring-your-own-IP, essentielles pour construire une infrastructure cloud sécurisée."
      },
      "de": {
        "title": "Verständnis der IP-Verwaltung in der Oracle-Cloud-Infrastruktur (OCI)",
        "summary": "Ein umfassender Leitfaden zum IP-Adressenmanagementsystem von OCI, der private IP-Adressen für interne Kommunikation, öffentliche IP-Adressen für Internetzugriff und erweiterte Funktionen wie reservierte IP-Adressen und Bring-Your-Own-IP-Optionen abdeckt, die für den Aufbau einer sicheren Cloud-Infrastruktur unerlässlich sind."
      },
      "es": {
        "title": "Entender la gestión de direcciones IP en la infraestructura en la nube de Oracle (OCI)",
        "summary": "Una guía completa del sistema de gestión de direcciones IP de OCI que cubre direcciones IP privadas para comunicación interna, direcciones IP públicas para acceso a Internet y funciones avanzadas como direcciones IP reservadas y opciones bring-your-own-IP, esenciales para construir una infraestructura en la nube segura."
      }
    }
  },
  {
    "title": "Why You Shouldn't Let AI Do Your Fortune Telling — And How to Do It Right",
    "slug": "ai-fortune-telling-deterministic-algorithm-interpretation",
    "url": "https://dev.to/ji_ai/why-you-shouldnt-let-ai-do-your-fortune-telling-and-how-to-do-it-right-2h3l",
    "source": "DEV Community",
    "date": "2026-02-26T00:00:40.000Z",
    "summary": "This article demonstrates why LLMs cannot reliably perform precise astronomical calendar calculations for fortune-telling and advocates combining deterministic algorithms for computation with AI for human-readable interpretation of results.",
    "content": "The first lesson I learned building a saju (Korean four-pillar fortune telling) app: it's not about what you ask AI to do — it's about what you don't.\nRevenue is still $0. This isn't a success story — it's a debugging diary.\n\"Tell me the fortune for someone born March 15, 1990.\"\nI threw this straight at an LLM. The response looked great. Smooth sentences, Five Elements this, Wood-Fire-Earth-Metal-Water that.\nBut there was a problem. The base calculations were wrong.\nMe: \"Analyze the Four Pillars for March 15, 1990, 6 AM\"\nClaude: \"The year pillar is Geng-Wu, month is Ji-Mao...\"\nMe: \"...Ji-Mao is wrong.\"\n\nLLMs can't do manseryeok (traditional Korean astronomical calendar) calculations. More precisely, they appear to get it right probabilistically, but they don't actually compute anything.\nThe Heavenly Stems and Earthly Branches follow a 60-cycle system. Month pillars shift at solar term boundaries. Hour pillars depend on the day's Heavenly Stem. This isn't reasoning — it's arithmetic.\nWhen you ask a language model to do arithmetic, it gets things wrong.\nAnd when the base stems are wrong, everything downstream is garbage. Wrong Five Elements. Wrong Ten Gods. Wrong structure analysis.\nA beautifully written paragraph with incorrect data isn't fortune analysis — it's fiction.\nOnce I realized this, I rebuilt the whole thing.\n[Birth date + time] → [Calendar Engine] → [Accurate JSON] → [LLM] → [Interpretation]\n\nI built the calendar engine in code. It's based on the lunar-typescript library, with solar term correction, leap month handling, and midnight boundary logic — all deterministic algorithms.\nThe output is JSON. Heavenly Stems, Earthly Branches, Five Element distribution, Ten Gods relationships, structure type, favorable elements. All precise.\nThe LLM gets this JSON. \"Read this data and interpret it.\" That's the entire prompt strategy.\nCode handles the calendar calculations — the part that must be 100% accurate. AI handles turning that data into readable, insightful lan",
    "category": "github",
    "translations": {
      "zh": {
        "title": "为什么不应该让AI做占卜——以及如何正确地做",
        "summary": "本文演示了为什么大语言模型无法可靠地执行用于占卜的精确天文日历计算，并倡导将确定性算法用于计算，将AI用于人类可读的结果解释。"
      },
      "fr": {
        "title": "Pourquoi vous ne devriez pas laisser l'IA faire votre voyance — Et comment le faire correctement",
        "summary": "Cet article démontre pourquoi les LLM ne peuvent pas effectuer de manière fiable des calculs précis de calendrier astronomique pour la voyance et préconise de combiner des algorithmes déterministes pour le calcul avec l'IA pour l'interprétation lisible par l'homme des résultats."
      },
      "de": {
        "title": "Warum Sie KI nicht für Ihre Wahrsagung einsetzen sollten — Und wie man es richtig macht",
        "summary": "Dieser Artikel zeigt, warum LLMs keine zuverlässigen astronomischen Kalenderberechnungen für Wahrsagungen durchführen können, und befürwortet die Kombination deterministischer Algorithmen für Berechnungen mit KI für menschenlesbare Interpretationen der Ergebnisse."
      },
      "es": {
        "title": "Por qué no deberías dejar que la IA haga tu lectura del futuro — Y cómo hacerlo correctamente",
        "summary": "Este artículo demuestra por qué los LLM no pueden realizar de manera confiable cálculos precisos de calendarios astronómicos para la lectura del futuro e aboga por combinar algoritmos deterministas para cálculos con IA para la interpretación legible por humanos de los resultados."
      }
    }
  }
]